<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="fr" xml:lang="fr"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.43">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Apprentissage automatique interprétable – learned-features</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../10-neuralnet/10.2-pixel-attribution.html" rel="next">
<link href="../10-neuralnet/index.html" rel="prev">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "Pas de résultats",
    "search-matching-documents-text": "documents trouvés",
    "search-copy-link-title": "Copier le lien vers la recherche",
    "search-hide-matches-text": "Cacher les correspondances additionnelles",
    "search-more-match-text": "correspondance de plus dans ce document",
    "search-more-matches-text": "correspondances de plus dans ce document",
    "search-clear-button-title": "Effacer",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Annuler",
    "search-submit-button-title": "Envoyer",
    "search-label": "Recherche"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../styles.css">
</head>

<body class="nav-sidebar docked">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Basculer la barre latérale" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../10-neuralnet/index.html">10 - Interprétation d’un réseau de neurone</a></li><li class="breadcrumb-item"><a href="../10-neuralnet/10.1-learned-features.html">10.1 - Caractéristiques apprises</a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Basculer la barre latérale" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Recherche" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation docked overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="../">Apprentissage automatique interprétable</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Recherche"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../00-summary/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Résumé</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../01-preface/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">1 - Préface de l’auteur</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../02-introduction/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">2 - Introduction</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Basculer la section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../02-introduction/02.1-short_stories.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">2.1 - Quelques histoires</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../02-introduction/02.2-ml_definitions.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">2.2 - Qu’est-ce que l’apprentissage automatique ?</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../02-introduction/02.3-terminology.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">2.3 - Terminologie</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../03-interpretability/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">3 - Interprétabilité</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Basculer la section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../03-interpretability/03.1-importance_of_interpretability.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">3.1 - Importance de l’Interprétabilité</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../03-interpretability/03.2-taxonomy_of_interpretability_methods.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">3.2 - Taxonomie des Méthodes d’Interprétabilité</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../03-interpretability/03.3-scope_of_interpretability.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">3.3 - Portée de l’Interprétabilité</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../03-interpretability/03.4-evaluation_of_interpretability.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">3.4 - Evaluation de l’interprétabilité</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../03-interpretability/03.5-properties_of_explanations.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">3.5 - Propriétés des Explications</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../03-interpretability/03.6-human_friendly_explanations.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">3.6 - Explications conviviales pour l’être humain</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../04-datasets/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">4 - Jeux de données</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true" aria-label="Basculer la section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../04-datasets/04.1-datasets.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">4.1 - Location de vélo (Régression)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../04-datasets/04.2-datasets.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">4.2 - Commentaires indésirables sur YouTube (Classification de Texte)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../04-datasets/04.3-datasets.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">4.3 - Facteurs de Risque du Cancer du Col de l’Uterus (Classification)</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../05-interpretable_models/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">5 - Modèles interprétables</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true" aria-label="Basculer la section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../05-interpretable_models/05.1-linear-regression.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">5.1 - Régéression linéaire</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../05-interpretable_models/05.2-logistic-regression.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">5.2 - Régéression logistique</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../05-interpretable_models/05.3-glm-gam-more.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">5.3 - GLM, GAM et plus</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../05-interpretable_models/05.4-decision-tree.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">5.4 - Arbre de décision</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../05-interpretable_models/05.5-decision-rules.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">5.5 - Règles de décision</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../05-interpretable_models/05.6-rulefit.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">5.6 - Ajustement des règles</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../05-interpretable_models/05.7-other-interpretable-models.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">5.7 - Autres modèles interprétables</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../06-model_agnostic_methods/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">6 - Méthodes indépendantes du modèle</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../07-example/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">7 - Explications basées sur des exemples</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../08-global_model_agnostic_methods/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">8 - Méthodes globales indépendantes du modèle</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="true" aria-label="Basculer la section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../08-global_model_agnostic_methods/08.1-pdp.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">8.1 - Diagramme de dépendance partielle (PDP)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../08-global_model_agnostic_methods/08.2-ale.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">8.2 - Graphique des effets locaux accumulés (ALE)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../08-global_model_agnostic_methods/08.3-feature-interaction.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">8.3 - Interactions avec les fonctionnalités</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../08-global_model_agnostic_methods/08.4-functional-decomposition.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">8.4 - Functional Decomposition</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../08-global_model_agnostic_methods/08.5-permutation-feature-importance.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">8.5 - Décomposition fonctionnelle</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../08-global_model_agnostic_methods/08.6-global-surrogate.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">8.6 - Substitut global</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../08-global_model_agnostic_methods/08.7-prototype-criticisms.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">8.7 - Prototypes et critiques</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../09-local_model_agnostic_methods/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">9 - Méthodes locales indépendantes du modèle</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" role="navigation" aria-expanded="true" aria-label="Basculer la section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-6" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../09-local_model_agnostic_methods/09.1-ice.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">9.1 - Attente Conditionnelle Individuelle (<em>Individual Conditional Expectation - ICE</em>)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../09-local_model_agnostic_methods/09.2-lime.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">9.2 - Substitut local (LIME)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../09-local_model_agnostic_methods/09.3-counterfactual.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">9.3 - Explications contrefactuelles</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../09-local_model_agnostic_methods/09.4-anchors.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">9.4 - Règles de portée (ancres)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../09-local_model_agnostic_methods/09.5-shapley.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">9.5 - Valeurs de Shapley</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../09-local_model_agnostic_methods/09.6-shap.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">9.6 - SHAP (SHapley Additive exPlanations)</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../10-neuralnet/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">10 - Interprétation d'un réseau de neurone</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" role="navigation" aria-expanded="true" aria-label="Basculer la section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-7" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../10-neuralnet/10.1-learned-features.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text">10.1 - Caractéristiques apprises</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../10-neuralnet/10.2-pixel-attribution.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">10.2 - Attribution de pixel</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../10-neuralnet/10.3-concepts.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">10.3 - Détecter les concepts</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../10-neuralnet/10.4-adversarial-examples.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">10.4 - Exemples adverses</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../10-neuralnet/10.5-influential-instances.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">10.5 - Instances Influentes</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../11-future/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">11 - Un regard dans une boule de cristal</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-8" role="navigation" aria-expanded="true" aria-label="Basculer la section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-8" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../11-future/11.1-future-ml.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">11.1 - L’avenir de l’apprentissage automatique</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../11-future/11.2-future-interpretability.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">11.2 - L’avenir de l’interprétabilité</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../12-contribute/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">12 - Contribuer à ce livre</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../13-citation/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">13 - Citer ce livre</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../14-translations/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">14 - Traductions</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../15-acknowledgements/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">15 - Remerciements</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../Formulaire/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Des remarques ?</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../References/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Références</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active" data-toc-expanded="4">
    <h2 id="toc-title">Dans cette page</h2>
   
  <ul>
  <li><a href="#caractéristiques-apprises" id="toc-caractéristiques-apprises" class="nav-link active" data-scroll-target="#caractéristiques-apprises">10.1 - Caractéristiques apprises</a>
  <ul>
  <li><a href="#sec-feature_visualization" id="toc-sec-feature_visualization" class="nav-link" data-scroll-target="#sec-feature_visualization">10.1.1 - Visualisation des caractéristiques</a>
  <ul class="collapse">
  <li><a href="#visualisation-des-caractéristiques-par-optimisation" id="toc-visualisation-des-caractéristiques-par-optimisation" class="nav-link" data-scroll-target="#visualisation-des-caractéristiques-par-optimisation">10.1.1.1 Visualisation des caractéristiques par optimisation</a></li>
  <li><a href="#lien-avec-les-exemples-adverses" id="toc-lien-avec-les-exemples-adverses" class="nav-link" data-scroll-target="#lien-avec-les-exemples-adverses">10.1.1.2 Lien avec les exemples adverses</a></li>
  <li><a href="#données-textuelles-et-tabulaires" id="toc-données-textuelles-et-tabulaires" class="nav-link" data-scroll-target="#données-textuelles-et-tabulaires">10.1.1.3 Données textuelles et tabulaires</a></li>
  </ul></li>
  <li><a href="#sec-network_dissection" id="toc-sec-network_dissection" class="nav-link" data-scroll-target="#sec-network_dissection">10.1.2 Dissection d’un réseau</a>
  <ul class="collapse">
  <li><a href="#algorithme-de-dissection-de-réseau" id="toc-algorithme-de-dissection-de-réseau" class="nav-link" data-scroll-target="#algorithme-de-dissection-de-réseau">10.1.2.1 - Algorithme de dissection de réseau</a></li>
  <li><a href="#expériences" id="toc-expériences" class="nav-link" data-scroll-target="#expériences">10.1.2.2 - Expériences</a></li>
  </ul></li>
  <li><a href="#avantages" id="toc-avantages" class="nav-link" data-scroll-target="#avantages">10.1.3 Avantages</a></li>
  <li><a href="#inconvénients" id="toc-inconvénients" class="nav-link" data-scroll-target="#inconvénients">10.1.4 Inconvénients</a></li>
  <li><a href="#logiciels-et-matériel-complémentaire" id="toc-logiciels-et-matériel-complémentaire" class="nav-link" data-scroll-target="#logiciels-et-matériel-complémentaire">10.1.5 Logiciels et matériel complémentaire</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../10-neuralnet/index.html">10 - Interprétation d’un réseau de neurone</a></li><li class="breadcrumb-item"><a href="../10-neuralnet/10.1-learned-features.html">10.1 - Caractéristiques apprises</a></li></ol></nav>
<div class="quarto-title">
</div>



<div class="quarto-title-meta">

    
  
    <div>
    <div class="quarto-title-meta-heading">Modifié</div>
    <div class="quarto-title-meta-contents">
      <p class="date-modified">19 février 2025</p>
    </div>
  </div>
    
  </div>
  


</header>


<div class="callout callout-style-default callout-warning callout-titled" title="Avertissement">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Avertissement
</div>
</div>
<div class="callout-body-container callout-body">
<p>En cours de traduction.</p>
</div>
</div>
<section id="caractéristiques-apprises" class="level2">
<h2 class="anchored" data-anchor-id="caractéristiques-apprises">10.1 - Caractéristiques apprises</h2>
<!-- HTML only - Not in EN book-->
<p>Les réseaux neuronaux convolutionnels apprennent des caractéristiques et des concepts abstraits à partir de pixels d’images bruts. La <a href="#sec-feature_visualization">visualisation de caractéristiques</a> visualise les caractéristiques apprises par maximisation de l’activation. La <a href="#sec-network_dissection">dissection de réseau</a> étiquette les unités de réseau neuronal (p.&nbsp;ex. les canaux) avec des concepts humains.</p>
<p>Les réseaux neuronaux profonds apprennent des caractéristiques de haut niveau dans les couches cachées. C’est l’une de leurs plus grandes forces et réduit le besoin d’ingénierie des caractéristiques. Supposons que vous voulez construire un classificateur d’images avec une machine à vecteurs de support. Les matrices de pixels bruts ne constituent pas la meilleure entrée pour entraîner votre SVM, donc vous créez de nouvelles caractéristiques basées sur la couleur, le domaine fréquentiel, les détecteurs de bords, etc. Avec les réseaux neuronaux convolutionnels, l’image est introduite dans le réseau sous sa forme brute (i.e.&nbsp;les pixels). Le réseau transforme l’image à de nombreuses reprises. D’abord, l’image passe par de plusieurs couches convolutionnelles. Dans ces couches convolutionnelles, le réseau y apprend de nouvelles caractéristiques de plus en plus complexes. Ensuite, l’information de l’image transformée passe à travers les couches entièrement connectées et se transforme en une classification ou une prédiction.</p>
<!--
![Architecture of Inception V1 neural network. Each enumerated unit (3a to 5b) represents a layer with differently sized convolutions and pooling. Figure from Olah, et al. (2019, CC-BY 4.0) [https://distill.pub/2017/activation-atlas/](https://distill.pub/2017/activation-atlas/).](../images/inceptionv1.svg){align=center}
-->
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../images/cnn-features.png" class="img-fluid figure-img" data-align="center"></p>
<figcaption>Features learned by a convolutional neural network (Inception V1) trained on the ImageNet data. The features range from simple features in the lower convolutional layers (left) to more abstract features in the higher convolutional layers (right). Figure from Olah, et al.&nbsp;(2017, CC-BY 4.0) <a href="https://distill.pub/2017/feature-visualization/appendix/">https://distill.pub/2017/feature-visualization/appendix/</a>.</figcaption>
</figure>
</div>
<ul>
<li>La ou les premières couches convolutionnelles apprennent des caractéristiques telles que les bords et les textures simples.</li>
<li>Les couches convolutionnelles suivantes apprennent des caractéristiques telles que des textures et des motifs plus complexes.</li>
<li>Les dernières couches convolutionnelles apprennent des caractéristiques telles que des objets ou des parties d’objets.</li>
<li>Les couches entièrement connectées apprennent à relier les activations des caractéristiques de haut niveau aux classes individuelles à prédire.</li>
</ul>
<p>Génial. Mais comment obtenons-nous réellement ces images hallucinatoires ?</p>
<section id="sec-feature_visualization" class="level3">
<h3 class="anchored" data-anchor-id="sec-feature_visualization">10.1.1 - Visualisation des caractéristiques</h3>
<p>L’approche consistant à rendre explicites les caractéristiques apprises est appelée <strong>visualisation de caractéristiques</strong>. La visualisation de caractéristiques pour une unité d’un réseau neuronal se fait en trouvant l’entrée qui maximise l’activation de cette “unité”.</p>
<p>“Unité” fait référence soit à des neurones individuels, des canaux (également appelés cartes de caractéristiques), des couches entières ou à la probabilité de classe finale en classification (ou le neurone pré-softmax correspondant, ce qui est recommandé). Les neurones individuels sont des unités atomiques du réseau, donc nous obtiendrions le plus d’informations en créant des visualisations de caractéristiques pour chaque neurone. Mais il y a un problème : Les réseaux neuronaux contiennent souvent des millions de neurones. Examiner la visualisation de caractéristiques de chaque neurone prendrait trop de temps. Les canaux (parfois appelés cartes d’activation) en tant qu’unités sont un bon choix pour la visualisation de caractéristiques. Nous pouvons aller plus loin et visualiser une couche convolutionnelle entière. Les couches en tant qu’unité sont utilisées pour le DeepDream de Google, qui ajoute de manière répétée les caractéristiques visualisées d’une couche à l’image originale, résultant en une version onirique de l’entrée.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../images/units.jpg" class="img-fluid figure-img" data-align="center"></p>
<figcaption>Feature visualization can be done for different units. A) Convolution neuron, B) Convolution channel, C) Convolution layer, D) Neuron, E) Hidden layer, F) Class probability neuron (or corresponding pre-softmax neuron)</figcaption>
</figure>
</div>
<!--
![Optimized images for Inception V1 (channels mixed3a, mixed4c, mixed4d and mixed5a). Images are maximized for a random direction of the activations. Figure from Olah, et al. 2017 (CC-BY 4.0) [https://distill.pub/2017/feature-visualization/](https://distill.pub/2017/feature-visualization/).](../images/trippy.png){align=center fig-alt="Optimized images for Inception V1 (channels mixed3a, mixed4c, mixed4d and mixed5a). Images are maximized for a random direction of the activations. Figure from Olah, et al. 2017 (CC-BY 4.0) https://distill.pub/2017/feature-visualization/."}
-->
<section id="visualisation-des-caractéristiques-par-optimisation" class="level4">
<h4 class="anchored" data-anchor-id="visualisation-des-caractéristiques-par-optimisation">10.1.1.1 Visualisation des caractéristiques par optimisation</h4>
<p>En termes mathématiques, la visualisation de caractéristiques est un problème d’optimisation. Nous supposons que les poids du réseau neuronal sont fixes, ce qui signifie que le réseau est entraîné. Nous cherchons une nouvelle image qui maximise l’activation (moyenne) d’une unité, ici un seul neurone :</p>
<p><span class="math display">\[img^*=\arg\max_{img}h_{n,x,y,z}(img)\]</span></p>
<p>La fonction <span class="math inline">\(h\)</span> correspond à l’activation du neurone, <span class="math inline">\(img\)</span> les données d’entrée (une image) du réseau, <span class="math inline">\(x\)</span> et <span class="math inline">\(y\)</span> décrive la position spatiale du neurone, <span class="math inline">\(n\)</span> identifie la couche et <span class="math inline">\(z\)</span> l’indice du canal.</p>
<p><span class="math display">\[img^*=\arg\max_{img}\sum_{x,y}h_{n,x,y,z}(img)\]</span></p>
<p>Dans cette formule, tous les neurones dans le canal <span class="math inline">\(z\)</span> sont pondérés de manière égale. Alternativement, vous pouvez également maximiser des directions aléatoires, ce qui signifie que les neurones seraient multipliés par différents paramètres, y compris des directions négatives. De cette façon, nous étudions comment les neurones interagissent au sein du canal. Au lieu de maximiser l’activation, vous pouvez également la minimiser (ce qui correspond à maximiser la direction négative). De manière intéressante, lorsque vous maximisez la direction négative, vous obtenez des caractéristiques très différentes pour la même unité :</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../images/a484.png" class="img-fluid figure-img" data-align="center"></p>
<figcaption>Positive (left) and negative (right) activation of Inception V1 neuron 484 from layer mixed4d pre relu. While the neuron is maximally activated by wheels, something which seems to have eyes yields a negative activation.</figcaption>
</figure>
</div>
<p>Nous pouvons aborder ce problème d’optimisation de différentes manières. Par exemple, au lieu de générer de nouvelles images, nous pourrions parcourir nos images d’entraînement et sélectionner celles qui maximisent l’activation. C’est une approche valable, mais utiliser des données d’entraînement présente le problème que les éléments sur les images peuvent être corrélés et nous ne pouvons pas voir ce que le réseau neuronal recherche réellement. Si les images qui génèrent une haute activation d’un certain canal montrent un chien et une balle de tennis, nous ne savons pas si le réseau neuronal se concentre sur le chien, la balle de tennis ou peut-être sur les deux.</p>
<p>Une autre approche consiste à générer de nouvelles images, en partant de bruit aléatoire. Pour obtenir des visualisations significatives, il y a généralement des contraintes sur l’image, par exemple que seuls de petits changements sont autorisés. Pour réduire le bruit dans la visualisation des caractéristiques, vous pouvez appliquer du jittering, de la rotation ou du redimensionnement à l’image avant l’étape d’optimisation. D’autres options de régularisation incluent la pénalisation de fréquence (par exemple, réduire la variance des pixels voisins) ou générer des images avec des prioris appris, par exemple avec des réseaux adverses génératifs (GANs)<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a> ou des autoencodeurs débruitants<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a>.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../images/activation-optim.png" class="img-fluid figure-img" data-align="center"></p>
<figcaption>Iterative optimization from random image to maximizing activation. Olah, et al.&nbsp;2017 (CC-BY 4.0), <a href="https://distill.pub/2017/feature-visualization/">https://distill.pub/2017/feature-visualization/</a>.</figcaption>
</figure>
</div>
<p>Si vous désirez approdonfir le sujet de visualisation des caractéristiques, jetez une oeil au journal en ligne <a href="https://distill.pub/">distill.pub</a>, particulièrement l’article posté par Olah et al (2017)<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a>, dont j’ai utilisé de nombreuses images. Je recommande également l’article sur la construction de blocs d’interprétation<a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a>.</p>
</section>
<section id="lien-avec-les-exemples-adverses" class="level4">
<h4 class="anchored" data-anchor-id="lien-avec-les-exemples-adverses">10.1.1.2 Lien avec les exemples adverses</h4>
<p>Il existe un lien entre la visualisation de caractéristiques et les <a href="../10-neuralnet/10.4-adversarial-examples.html">exemples adverses</a> : les deux techniques maximisent l’activation d’une unité de réseau neuronal. Pour les exemples adverses, nous recherchons l’activation maximale du neurone pour la classe adverse (= incorrecte). Une différence réside dans l’image avec laquelle nous commençons : pour les exemples adverses, il s’agit de l’image pour laquelle nous voulons générer l’image adverse. Pour la visualisation de caractéristiques, c’est, selon l’approche, du bruit aléatoire.</p>
</section>
<section id="données-textuelles-et-tabulaires" class="level4">
<h4 class="anchored" data-anchor-id="données-textuelles-et-tabulaires">10.1.1.3 Données textuelles et tabulaires</h4>
<p>La littérature se concentre sur la visualisation des caractéristiques pour les réseaux neuronaux convolutionnels destinés à la reconnaissance d’images. Techniquement, rien ne vous empêche de trouver l’entrée qui active au maximum un neurone d’un réseau neuronal entièrement connecté pour des données tabulaires ou d’un réseau neuronal récurrent pour des données textuelles. Vous pourriez ne plus appeler cela “visualisation de caractéristiques”, puisque le “caractéristique” serait une entrée de données tabulaires ou textuelle. Pour la prédiction de défaut de crédit, les entrées pourraient être le nombre de crédits antérieurs, le nombre de contrats mobiles, l’adresse et des dizaines d’autres caractéristiques. La caractéristique apprise d’un neurone serait alors une certaine combinaison des dizaines de caractéristiques. Pour les réseaux neuronaux récurrents, il est un peu plus agréable de visualiser ce que le réseau a appris : Karpathy et al.&nbsp;(2015)<a href="#fn5" class="footnote-ref" id="fnref5" role="doc-noteref"><sup>5</sup></a> ont montré que les réseaux neuronaux récurrents ont effectivement des neurones qui apprennent des caractéristiques interprétables. Ils ont entraîné un modèle au niveau des caractères, qui prédit le prochain caractère dans la séquence à partir des caractères précédents. Une fois qu’une parenthèse ouvrante “(” survenait, l’un des neurones était fortement activé, et se désactivait lorsque la parenthèse fermante correspondante “)” survenait. D’autres neurones se déclenchaient à la fin d’une ligne. Certains neurones se déclenchaient dans les URL. La différence avec la visualisation des caractéristiques pour les CNN est que les exemples n’ont pas été trouvés par optimisation, mais en étudiant les activations des neurones dans les données d’entraînement.</p>
<p>Certaines des images semblent montrer des concepts bien connus comme des museaux de chiens ou des bâtiments. Mais comment pouvons-nous en être sûrs ? La méthode de dissection de réseau relie les concepts humains avec des unités individuelles de réseau neuronal. Alerte de divulgâchage : la dissection de réseau nécessite des jeux de données supplémentaires que quelqu’un a étiquetés avec des concepts humains.</p>
</section>
</section>
<section id="sec-network_dissection" class="level3">
<h3 class="anchored" data-anchor-id="sec-network_dissection">10.1.2 Dissection d’un réseau</h3>
<p>L’approche de Dissection de Réseau par Bau &amp; Zhou et al.&nbsp;(2017)<a href="#fn6" class="footnote-ref" id="fnref6" role="doc-noteref"><sup>6</sup></a> quantifie l’interprétabilité d’une unité d’un réseau neuronal convolutionnel. Elle relie les zones fortement activées des canaux de CNN avec des concepts humains (objets, parties, textures, couleurs, …).</p>
<p>Les canaux d’un réseau neuronal convolutionnel apprennent de nouvelles caractéristiques, comme nous l’avons vu dans le chapitre consacré à la <a href="#sec-feature_visualization">visualisation de caractéristiques</a>. Mais ces visualisations ne prouvent pas qu’une unité a appris un certain concept. Nous n’avons pas non plus de mesure pour évaluer à quel point une unité détecte, par exemple, les gratte-ciels. Avant d’entrer dans les détails de la Dissection de Réseau, nous devons parler de la grande hypothèse qui est derrière cette ligne de recherche. L’hypothèse est la suivante : Les unités d’un réseau neuronal (comme les canaux convolutionnels) apprennent des concepts désenchevêtrés.</p>
<p><strong>La Question des caractéristiques désenchevêtrées</strong></p>
<p>Les réseaux neuronaux (convolutionnels) apprennent-ils des caractéristiques désenchevêtrées ? Les caractéristiques désenchevêtrées signifient que des unités individuelles du réseau détectent des concepts spécifiques du monde réel. Le canal convolutionnel 394 pourrait détecter des gratte-ciels, le canal 121 des museaux de chiens, le canal 12 des rayures à un angle de 30 degrés… L’opposé d’un réseau désenchevêtré est un réseau complètement enchevêtré. Dans un réseau complètement enchevêtré, par exemple, il n’y aurait pas d’unité individuelle pour les museaux de chiens. Tous les canaux contribueraient à la reconnaissance des museaux de chiens.</p>
<p>Les caractéristiques désenchevêtrées impliquent que le réseau est hautement interprétable. Supposons que nous ayons un réseau avec des unités complètement désenchevêtrées qui sont étiquetées avec des concepts connus. Cela ouvrirait la possibilité de suivre le processus de prise de décision du réseau. Par exemple, nous pourrions analyser comment le réseau classe les loups par rapport aux huskies. D’abord, nous identifions l’unité “husky”. Nous pouvons vérifier si cette unité dépend des unités “museau de chien”, “fourrure duveteuse” et “neige” de la couche précédente. Si c’est le cas, nous savons qu’elle classera par erreur une image d’un husky avec un arrière-plan enneigé comme un loup. Dans un réseau désenchevêtré, nous pourrions identifier des corrélations non causales problématiques. Nous pourrions automatiquement lister toutes les unités fortement activées et leurs concepts pour expliquer une prédiction individuelle. Nous pourrions facilement détecter les biais dans le réseau neuronal. Par exemple, le réseau a-t-il appris une caractéristique de “peau blanche” pour prédire un salaire ?</p>
<p>Alerte de divulgâchage : les réseaux neuronaux convolutionnels ne sont pas parfaitement désenchevêtrés. Nous allons maintenant examiner de plus près la Dissection de Réseau pour découvrir à quel point les réseaux neuronaux sont interprétables.</p>
<section id="algorithme-de-dissection-de-réseau" class="level4">
<h4 class="anchored" data-anchor-id="algorithme-de-dissection-de-réseau">10.1.2.1 - Algorithme de dissection de réseau</h4>
<p>La Dissection de Réseau comporte trois étapes :</p>
<ol type="1">
<li>Obtenir des images avec des concepts visuels étiquetés par des humains, allant des rayures aux gratte-ciels.</li>
<li>Mesurer les activations des canaux CNN pour ces images.</li>
<li>Quantifier l’alignement des activations et des concepts étiquetés.</li>
</ol>
<p>La figure suivante visualise comment une image est transmise à un canal et associée aux concepts étiquetés.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../images/dissection-network.png" class="img-fluid figure-img" data-align="center&quot;"></p>
<figcaption>For a given input image and a trained network (fixed weights), we propagate the image forward to the target layer, upscale the activations to match the original image size and compare the maximum activations with the ground truth pixel-wise segmentation. Figure originally from http://netdissect.csail.mit.edu/.</figcaption>
</figure>
</div>
<p><strong>Étape 1 : Le jeu de données Broden</strong></p>
<p>La première étape difficile mais cruciale est la collecte de données. La dissection de réseau nécessite des images étiquetées pixel par pixel avec des concepts de différents niveaux d’abstraction (des couleurs aux scènes de rue). Bau &amp; Zhou et al.&nbsp;ont combiné plusieurs jeux de données avec des concepts pixel par pixel. Ils ont nommé ce nouveau jeu de données <code>Broden</code>, qui signifie que les données sont largement et densément étiquetées. Le jeu de données Broden est segmenté principalement au niveau du pixel, pour certains jeux de données, toute l’image est étiquetée. Broden contient 60 000 images avec plus de 1 000 concepts visuels à différents niveaux d’abstraction : 468 scènes, 585 objets, 234 parties, 32 matériaux, 47 textures et 11 couleurs.</p>
<p><strong>Étape 2 : Récupération des activations du réseau</strong></p>
<p>Ensuite, nous créons les masques des zones les plus activées par canal et par image. À ce stade, les étiquettes de concept ne sont pas encore impliquées.</p>
<ul>
<li>Pour chaque canal convolutionnel <span class="math inline">\(k\)</span> :
<ul>
<li>Pour chaque image <span class="math inline">\(x\)</span> dans le jeu de données Broden :
<ul>
<li>Propager l’image <span class="math inline">\(x\)</span> vers la couche cible contenant le canal <span class="math inline">\(k\)</span>.</li>
<li>Extraire les activations de pixels du canal convolutionnel <span class="math inline">\(k\)</span> : <span class="math inline">\(A_k(x)\)</span>.</li>
</ul></li>
<li>Calculer la distribution des activations de pixels <span class="math inline">\(\alpha_k\)</span> sur toutes les images.</li>
<li>Déterminer le niveau du quantile à <span class="math inline">\(0,995\)</span> de activations <span class="math inline">\(\alpha_k\)</span>. Cela signifie que <span class="math inline">\(0,5%\)</span> de toutes les activations du canal <span class="math inline">\(k\)</span> pour l’image <span class="math inline">\(x\)</span> sont supérieures à <span class="math inline">\(T_k\)</span>.</li>
<li>Pour chaque image x dans le jeu de données Broden :
<ul>
<li>Mettre à l’échelle la carte d’activation (éventuellement) de résolution inférieure <span class="math inline">\(A_k(x)\)</span> à la résolution de l’image <span class="math inline">\(x\)</span>. Nous appelons le résultat <span class="math inline">\(S_k(x)\)</span>.</li>
<li>Binariser la carte d’activation : Un pixel est soit activé soit désactivé, selon qu’il dépasse ou non le seuil d’activation <span class="math inline">\(T_k\)</span>. Le nouveau masque est <span class="math inline">\(M_k(x) = S_k(x) \geq T_k(x)\)</span>.</li>
</ul></li>
</ul></li>
</ul>
<p><strong>Étape 3 : Alignement activation-concept</strong></p>
<p>Après l’étape 2, nous avons un masque d’activation par canal et par image. Ces masques d’activation marquent les zones fortement activées. Pour chaque canal, nous voulons trouver le concept humain qui active ce canal. Nous trouvons le concept en comparant les masques d’activation avec tous les concepts étiquetés. Nous quantifions l’alignement entre le masque d’activation k et le masque de concept c avec le score d’Intersection sur Union (IoU) :</p>
<p><span class="math display">\[IoU_{k,c} = \frac{\sum |M_k(x) \bigcap L_c(x)|}{\sum |M_k(x) \bigcup L_c(x)|}\]</span></p>
<p>où <span class="math inline">\(|\cdot|\)</span> est la cardinalité d’un ensemble. L’intersection sur union compare l’alignement entre deux zones. <span class="math inline">\(IoU_{k,c}\)</span> peut être interprété comme la précision avec laquelle l’unité k détecte le concept <span class="math inline">\(c\)</span>. Nous appelons l’unité <span class="math inline">\(k\)</span> un détecteur du concept c lorsque <span class="math inline">\(IoU_{k,c} &gt; 0,04\)</span>. Ce seuil a été choisi par Bau &amp; Zhou et al (2017).</p>
<p>La figure suivante illustre l’intersection et l’union du masque d’activation et du masque de concept pour une seule image :</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../images/dissection-dog-exemplary.jpg" class="img-fluid figure-img" data-align="center"></p>
<figcaption>The Intersection over Union (IoU) is computed by comparing the human ground truth annotation and the top activated pixels.</figcaption>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../images/dissection-dogs.jpeg" class="img-fluid figure-img" data-align="center"></p>
<figcaption>Activation mask for inception - 4e channel 750 which detects dogs with <span class="math inline">\(IoU=0.203\)</span>. Figure originally from <a href="http://netdissect.csail.mit.edu/">http://netdissect.csail.mit.edu/</a></figcaption>
</figure>
</div>
</section>
<section id="expériences" class="level4">
<h4 class="anchored" data-anchor-id="expériences">10.1.2.2 - Expériences</h4>
<p>Les auteurs de la dissection de réseau ont entraîné différentes architectures de réseaux (AlexNet, VGG, GoogleNet, ResNet) à partir de zéro sur différents jeux de données (ImageNet, Places205, Places365). ImageNet contient 1,6 million d’images de 1000 classes axées sur les objets. Places205 et Places365 contiennent respectivement 2,4 millions / 1,6 million d’images de 205 / 365 scènes différentes. Les auteurs ont également entraîné AlexNet sur des tâches d’apprentissage auto-supervisées telles que la prédiction de l’ordre des images vidéo ou la colorisation des images. Pour bon nombre de ces différents paramètres, ils ont compté le nombre de détecteurs de concepts uniques comme mesure de l’interprétabilité. Voici quelques-unes des découvertes :</p>
<ul>
<li>Les réseaux détectent des concepts de niveau inférieur (couleurs, textures) dans les couches inférieures et des concepts de niveau supérieur (parties, objets) dans les couches supérieures. Nous avons déjà vu cela dans les <a href="#sec-feature_visualization">Visualisations de Caractéristiques</a>.</li>
<li>La normalisation par lots réduit le nombre de détecteurs de concepts uniques.</li>
<li>De nombreuses unités détectent le même concept. Par exemple, il y a 95 (!) canaux de chien dans VGG entraîné sur ImageNet en utilisant <span class="math inline">\(IoU \geq 0,04\)</span> comme seuil de détection (4 dans conv4_3, 91 dans conv5_3, voir <a href="http://netdissect.csail.mit.edu/dissect/vgg16_imagenet/">site web du projet</a>).</li>
<li>Augmenter le nombre de canaux dans une couche augmente le nombre d’unités interprétables.</li>
<li>Les initialisations aléatoires (entraînement avec différentes graines aléatoires) entraînent des nombres légèrement différents d’unités interprétables.</li>
<li>ResNet est l’architecture de réseau avec le plus grand nombre de détecteurs uniques, suivi de VGG, GoogleNet et AlexNet en dernier.</li>
<li>Le plus grand nombre de détecteurs de concepts uniques est appris pour Places356, suivi de Places205 et ImageNet en dernier.</li>
<li>Le nombre de détecteurs de concepts uniques augmente avec le nombre d’itérations d’entraînement.</li>
<li>Les réseaux formés sur des tâches auto-supervisées ont moins de détecteurs uniques par rapport aux réseaux formés sur des tâches supervisées.</li>
<li>En apprentissage par transfert, le concept d’un canal peut changer. Par exemple, un détecteur de chien est devenu un détecteur de cascade. Cela s’est produit dans un modèle initialement formé pour classifier des objets et ensuite affiné pour classifier des scènes.</li>
<li>Dans l’une des expériences, les auteurs ont projeté les canaux sur une nouvelle base tournée. Cela a été fait pour le réseau VGG formé sur ImageNet. “Tourné” ne signifie pas que l’image a été tournée. “Tourné” signifie que nous prenons les 256 canaux de la couche conv5 et calculons 256 nouveaux canaux comme combinaisons linéaires des canaux originaux. Dans le processus, les canaux deviennent enchevêtrés. La rotation réduit l’interprétabilité, c’est-à-dire que le nombre de canaux alignés sur un concept diminue. La rotation a été conçue pour maintenir la performance du modèle identique. La première conclusion : l’interprétabilité des CNN est dépendante de l’axe. Cela signifie que des combinaisons aléatoires de canaux sont moins susceptibles de détecter des concepts uniques. La deuxième conclusion : l’interprétabilité est indépendante du pouvoir discriminant. Les canaux peuvent être transformés avec des transformations orthogonales tandis que le pouvoir discriminant reste le même, mais l’interprétabilité diminue.</li>
</ul>
<p>Les auteurs ont également utilisé la dissection de réseau pour les réseaux adverses génératifs (<em>Generative Adversarial Networks</em> : GANs). Vous pouvez trouver la dissection de réseau pour les GANs sur <a href="https://gandissect.csail.mit.edu/">le site web du projet</a>.</p>
</section>
</section>
<section id="avantages" class="level3">
<h3 class="anchored" data-anchor-id="avantages">10.1.3 Avantages</h3>
<p>Les visualisations de caractéristiques offrent <strong>un aperçu unique du fonctionnement des réseaux neuronaux</strong>, en particulier pour la reconnaissance d’images. Étant donné la complexité et l’opacité des réseaux neuronaux, la visualisation des caractéristiques est une étape importante dans l’analyse et la description des réseaux neuronaux. Grâce à la visualisation des caractéristiques, nous avons appris que les réseaux neuronaux apprennent d’abord des détecteurs de bords et de textures simples et des détecteurs de parties et d’objets plus abstraits dans les couches supérieures. La dissection de réseau élargit ces perspectives et rend mesurable l’interprétabilité des unités de réseau.</p>
<p>La dissection de réseau nous permet de <strong>lier automatiquement des unités à des concepts</strong>, ce qui est très pratique.</p>
<p>La visualisation de caractéristiques est un excellent outil pour <strong>communiquer de manière non technique le fonctionnement des réseaux neuronaux</strong>.</p>
<p>Avec la dissection de réseau, nous pouvons également <strong>détecter des concepts au-delà des classes dans la tâche de classification</strong>. Mais nous avons besoin de jeux de données contenant des images avec des concepts étiquetés pixel par pixel.</p>
<p>La visualisation des caractéristiques peut être <strong>combinée avec des <a href="../10-neuralnet/10.2-pixel-attribution.html">méthodes d’attribution de caractéristiques</a></strong>, qui expliquent quels pixels étaient importants pour la classification. La combinaison des deux méthodes permet d’expliquer une classification individuelle ainsi que la visualisation locale des caractéristiques apprises qui ont été impliquées dans la classification. Voir <a href="https://distill.pub/2018/building-blocks/">Les Blocs de Construction de l’Interprétabilité de distill.pub</a>.</p>
<p>Enfin, les visualisations de caractéristiques forment de <strong>magnifiques fonds d’écran et impressions sur T-shirt</strong>.</p>
</section>
<section id="inconvénients" class="level3">
<h3 class="anchored" data-anchor-id="inconvénients">10.1.4 Inconvénients</h3>
<p><strong>De nombreuses images de visualisation de caractéristiques ne sont pas interprétables</strong> du tout, mais contiennent certaines caractéristiques abstraites pour lesquelles nous n’avons ni mots ni concept mental. L’affichage des visualisations de caractéristiques avec des données d’entraînement peut aider. Les images pourraient toujours ne pas révéler ce à quoi le réseau neuronal a réagi et indiquer seulement quelque chose comme “peut-être doit-il y avoir du jaune dans les images”. Même avec la dissection de réseau, certains canaux ne sont pas liés à un concept humain. Par exemple, la couche conv5_3 de VGG formée sur ImageNet a 193 canaux (sur 512) qui n’ont pas pu être associés à un concept humain.</p>
<p>Il y a <strong>trop d’unités à examiner</strong>, même en “visualisant seulement” les activations de canal. Pour l’architecture Inception V1, par exemple, il y a déjà plus de 5000 canaux issus de neuf couches convolutionnelles. Si vous voulez également montrer les activations négatives plus quelques images des données d’entraînement qui activent maximale ou minimalement le canal (disons quatre images positives, quatre images négatives), alors vous devez déjà afficher plus de 50 000 images. Au moins, nous savons – grâce à la dissection de réseau – que nous n’avons pas besoin d’étudier les directions aléatoires.</p>
<p><strong>Illusion d’interprétabilité ?</strong> Les visualisations de caractéristiques peuvent donner l’illusion que nous comprenons ce que fait le réseau neuronal. Mais comprenons-nous vraiment ce qui se passe dans le réseau neuronal ? Même si nous examinons des centaines ou des milliers de visualisations de caractéristiques, nous ne pouvons pas comprendre le réseau neuronal. Les canaux interagissent de manière complexe, les activations positives et négatives ne sont pas liées, plusieurs neurones peuvent apprendre des caractéristiques très similaires et pour beaucoup de ces caractéristiques, nous n’avons pas de concepts humains équivalents. Nous ne devons pas tomber dans le piège de croire que nous comprenons complètement les réseaux neuronaux simplement parce que nous croyons avoir vu que le neurone 349 de la couche 7 est activé par des marguerites. La Dissection de Réseau a montré que des architectures comme ResNet ou Inception ont des unités qui réagissent à certains concepts. Mais l’IoU n’est pas si élevé et souvent de nombreuses unités répondent au même concept et certaines à aucun concept du tout. Les canaux ne sont pas complètement désenchevêtrés et nous ne pouvons pas les interpréter isolément.</p>
<p>Pour la dissection de réseau, <strong>vous avez besoin de jeux de données étiquetés au niveau du pixel</strong> avec les concepts. Ces jeux de données nécessitent beaucoup d’efforts pour être collectés, car chaque pixel doit être étiqueté, ce qui se fait généralement en dessinant des segments autour des objets sur l’image.</p>
<p>La dissection de réseau n’aligne que les concepts humains avec les activations positives mais pas avec les activations négatives des canaux. Comme l’ont montré les visualisations de caractéristiques, les activations négatives semblent être liées à des concepts. Cela pourrait être corrigé en examinant également le quantile inférieur des activations.</p>
</section>
<section id="logiciels-et-matériel-complémentaire" class="level3">
<h3 class="anchored" data-anchor-id="logiciels-et-matériel-complémentaire">10.1.5 Logiciels et matériel complémentaire</h3>
<p>Il existe une implémentation open-source de la visualisation de caractéristiques appelée <a href="https://github.com/tensorflow/lucid">Lucid</a>. Vous pouvez l’essayer facilement dans votre navigateur en utilisant les liens vers les notebooks fournis sur la page GitHub de Lucid. Aucun logiciel supplémentaire n’est nécessaire. D’autres implémentations incluent <a href="https://github.com/InFoCusp/tf_cnnvis">tf_cnnvis</a> pour TensorFlow, <a href="https://github.com/jacobgil/keras-filter-visualization">Keras Filters</a> pour Keras et <a href="https://github.com/yosinski/deep-visualization-toolbox">DeepVis</a> pour Caffe.</p>
<p>La dissection de réseau dispose d’un excellent <a href="http://netdissect.csail.mit.edu/">site web associé à ce projet</a>. En plus de la publication, il héberge du matériel supplémentaire tel que du code, des données et des visualisations de masques d’activation.</p>
<!-- REFERENCES -->
<!-- 02 -->
<!-- 02.3 -->
<!-- 03 -->
<!-- 03.1 -->
<!--
[^Miller2017]: Miller, Tim. "Explanation in artificial intelligence: Insights from the social sciences." arXiv Preprint arXiv:1706.07269. (2017).
-->
<!-- 03.3 -->
<!-- 03.4 -->
<!--
[^Doshi2017]: Doshi-Velez, Finale, and Been Kim. "Towards a rigorous science of interpretable machine learning," no. Ml: 1–13. https://arxiv.org/abs/1702.08608 (2017).
-->
<!-- 03.5 -->
<!-- 03.6 -->
<!--
[^Miller2017]: Miller, Tim. "Explanation in artificial intelligence: Insights from the social sciences." arXiv Preprint arXiv:1706.07269. (2017).
-->
<!-- 04.1 -->
<!-- 04.2 -->
<!-- 04.3 -->
<!-- 05.1 -->
<!-- 05.4 -->
<!--
[^Hastie]: Friedman, Jerome, Trevor Hastie, and Robert Tibshirani. "The elements of statistical learning". hastie.su.domains/ElemStatLearn (2009).
-->
<!-- 05.5 -->
<!-- 05.6 -->
<!-- 06.0 -->
<!--
[^Ribeiro2016]: Ribeiro, Marco Tulio, Sameer Singh, and Carlos Guestrin. "Model-agnostic interpretability of machine learning." ICML Workshop on Human Interpretability in Machine Learning. (2016).
-->
<!-- 07.0 -->
<!-- 08.1 -->
<!-- 08.2 -->
<!-- 08.3 -->
<!--
[^Friedman2008]: Friedman, Jerome H, and Bogdan E Popescu. "Predictive learning via rule ensembles." The Annals of Applied Statistics. JSTOR, 916–54. (2008).
-->
<!--
[^pdp-importance]: Greenwell, Brandon M., Bradley C. Boehmke, and Andrew J. McCarthy. "A simple and effective model-based variable importance measure." arXiv preprint arXiv:1805.04755 (2018).
-->
<!-- 08.4 -->
<!--
[^fanova]: Hooker, Giles. "Discovering additive structure in black box functions." Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining. (2004).
-->
<!--
[^ale]: Apley, Daniel W., and Jingyu Zhu. "Visualizing the effects of predictor variables in black box supervised learning models." Journal of the Royal Statistical Society: Series B (Statistical Methodology) 82.4 (2020): 1059-1086.
-->
<!-- 08.5 -->
<!-- 08.7 -->
<!--
[^critique]: Kim, Been, Rajiv Khanna, and Oluwasanmi O. Koyejo. "Examples are not enough, learn to criticize! Criticism for interpretability." Advances in Neural Information Processing Systems (2016).
-->
<!-- 09.1 -->
<!-- 09.2 -->
<!-- 09.3 -->
<!-- 09.4 -->
<!-- 09.5 -->
<!-- 09.6 -->
<!--
[^lundberg2017]: Lundberg, Scott M., and Su-In Lee. "A unified approach to interpreting model predictions." Advances in Neural Information Processing Systems (2017).
-->
<!--
[^cond1]: Sundararajan, Mukund, and Amir Najmi. "The many Shapley values for model explanation." arXiv preprint arXiv:1908.08474 (2019).
-->
<!--
[^cond2]: Janzing, Dominik, Lenon Minorics, and Patrick Blöbaum. "Feature relevance quantification in explainable AI: A causal problem." International Conference on Artificial Intelligence and Statistics. PMLR (2020).
-->
<!--
[^fool]: Slack, Dylan, Sophie Hilgard, Emily Jia, Sameer Singh, and Himabindu Lakkaraju. "Fooling lime and shap: Adversarial attacks on post hoc explanation methods." In Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society, pp. 180-186 (2020).
-->
<!-- 10.0 -->
<!-- 10.1 -->
<!-- 10.2 -->
<!--
[^integrated-gradients]: Sundararajan, Mukund, Ankur Taly, and Qiqi Yan. "Axiomatic attribution for deep networks." Proceedings of the 34th International Conference on Machine Learning-Volume 70. JMLR. org, 2017.
-->
<!--
[^grad-cam]: Selvaraju, Ramprasaath R., et al. "Grad-cam: Visual explanations from deep networks via gradient-based localization." Proceedings of the IEEE international conference on computer vision. (2017).
-->
<!--
[^guided-backpropagation]: Springenberg, Jost Tobias, et al. "Striving for simplicity: The all convolutional net." arXiv preprint arXiv:1412.6806 (2014).
-->
<!--
[^lrp]: Bach, Sebastian, et al. "On pixel-wise explanations for non-linear classifier decisions by layer-wise relevance propagation." PloS one 10.7 (2015).
-->
<!-- References about problems -->
<!--
[^better-understanding]: Ancona, Marco, et al. "Towards better understanding of gradient-based attribution methods for deep neural networks." arXiv preprint arXiv:1711.06104 (2017).
-->
<!--
[^perplexing-behavior]: Nie, Weili, Yang Zhang, and Ankit Patel. "A theoretical explanation for perplexing behaviors of backpropagation-based visualizations." arXiv preprint arXiv:1805.07039 (2018).
-->
<!-- Toolboxes -->
<!--
[^innvestigate]: Alber, Maximilian, Sebastian Lapuschkin, Philipp Seegerer, Miriam Hägele, Kristof T. Schütt, Grégoire Montavon, Wojciech Samek, Klaus-Robert Müller, Sven Dähne, and Pieter-Jan Kindermans. "iNNvestigate neural networks!." J. Mach. Learn. Res. 20, no. 93 (2019): 1-8.
-->
<!--
[^human-visuals]: Linsley, Drew, et al. "What are the visual features underlying human versus machine vision?." Proceedings of the IEEE International Conference on Computer Vision Workshops. 2017.
-->
<!-- 10.3 -->
<!-- 10.4 -->
<!-- 10.5 -->


</section>
</section>


<a onclick="window.scrollTo(0, 0); return false;" role="button" id="quarto-back-to-top"><i class="bi bi-arrow-up"></i> Retour au sommet</a><div id="quarto-appendix" class="default"><section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes"><h2 class="anchored quarto-appendix-heading">Notes de bas de page</h2>

<ol>
<li id="fn1"><p>Nguyen, Anh, Alexey Dosovitskiy, Jason Yosinski, Thomas Brox, and Jeff Clune. “Synthesizing the preferred inputs for neurons in neural networks via deep generator networks.” Advances in neural information processing systems 29 (2016): 3387-3395.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p>Nguyen, Anh, Jeff Clune, Yoshua Bengio, Alexey Dosovitskiy, and Jason Yosinski. “Plug &amp; play generative networks: Conditional iterative generation of images in latent space.” In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp.&nbsp;4467-4477. 2017.<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3"><p>Olah, Chris, Alexander Mordvintsev, and Ludwig Schubert. “Feature visualization.” Distill 2, no. 11 (2017): e7.<a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn4"><p>Olah, Chris, Arvind Satyanarayan, Ian Johnson, Shan Carter, Ludwig Schubert, Katherine Ye, and Alexander Mordvintsev. “The building blocks of interpretability.” Distill 3, no. 3 (2018): e10.<a href="#fnref4" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn5"><p>Karpathy, Andrej, Justin Johnson, and Li Fei-Fei. “Visualizing and understanding recurrent networks.” arXiv preprint arXiv:1506.02078 (2015).<a href="#fnref5" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn6"><p>Bau, David, Bolei Zhou, Aditya Khosla, Aude Oliva, and Antonio Torralba. “Network dissection: Quantifying interpretability of deep visual representations.” In Proceedings of the IEEE conference on computer vision and pattern recognition, pp.&nbsp;6541-6549 (2017).<a href="#fnref6" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copié");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copié");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
          // target, if specified
          link.setAttribute("target", "_blank");
          if (link.getAttribute("rel") === null) {
            link.setAttribute("rel", "noopener");
          }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../10-neuralnet/index.html" class="pagination-link" aria-label="10 - Interprétation d'un réseau de neurone">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text">10 - Interprétation d’un réseau de neurone</span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../10-neuralnet/10.2-pixel-attribution.html" class="pagination-link" aria-label="10.2 - Attribution de pixel">
        <span class="nav-page-text">10.2 - Attribution de pixel</span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
      &nbsp;
    </div>   
    <div class="nav-footer-center">
<p>Copyright 2018-2025, Christoph Molnar <br> Traduction 2024-2025 : Nicolas Guillard</p>
</div>
    <div class="nav-footer-right">
      &nbsp;
    </div>
  </div>
</footer>




</body></html>