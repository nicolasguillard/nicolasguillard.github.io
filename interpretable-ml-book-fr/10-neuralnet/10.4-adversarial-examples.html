<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="fr" xml:lang="fr"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.43">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Apprentissage automatique interprétable – adversarial-examples</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../10-neuralnet/10.5-influential-instances.html" rel="next">
<link href="../10-neuralnet/10.3-concepts.html" rel="prev">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "Pas de résultats",
    "search-matching-documents-text": "documents trouvés",
    "search-copy-link-title": "Copier le lien vers la recherche",
    "search-hide-matches-text": "Cacher les correspondances additionnelles",
    "search-more-match-text": "correspondance de plus dans ce document",
    "search-more-matches-text": "correspondances de plus dans ce document",
    "search-clear-button-title": "Effacer",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Annuler",
    "search-submit-button-title": "Envoyer",
    "search-label": "Recherche"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../styles.css">
</head>

<body class="nav-sidebar docked">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Basculer la barre latérale" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../10-neuralnet/index.html">10 - Interprétation d’un réseau de neurone</a></li><li class="breadcrumb-item"><a href="../10-neuralnet/10.4-adversarial-examples.html">10.4 - Exemples adverses</a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Basculer la barre latérale" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Recherche" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation docked overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="../">Apprentissage automatique interprétable</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Recherche"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../00-summary/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Résumé</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../01-preface/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">1 - Préface de l’auteur</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../02-introduction/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">2 - Introduction</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Basculer la section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../02-introduction/02.1-short_stories.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">2.1 - Quelques histoires</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../02-introduction/02.2-ml_definitions.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">2.2 - Qu’est-ce que l’apprentissage automatique ?</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../02-introduction/02.3-terminology.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">2.3 - Terminologie</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../03-interpretability/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">3 - Interprétabilité</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Basculer la section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../03-interpretability/03.1-importance_of_interpretability.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">3.1 - Importance de l’Interprétabilité</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../03-interpretability/03.2-taxonomy_of_interpretability_methods.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">3.2 - Taxonomie des Méthodes d’Interprétabilité</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../03-interpretability/03.3-scope_of_interpretability.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">3.3 - Portée de l’Interprétabilité</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../03-interpretability/03.4-evaluation_of_interpretability.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">3.4 - Evaluation de l’interprétabilité</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../03-interpretability/03.5-properties_of_explanations.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">3.5 - Propriétés des Explications</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../03-interpretability/03.6-human_friendly_explanations.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">3.6 - Explications conviviales pour l’être humain</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../04-datasets/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">4 - Jeux de données</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true" aria-label="Basculer la section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../04-datasets/04.1-datasets.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">4.1 - Location de vélo (Régression)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../04-datasets/04.2-datasets.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">4.2 - Commentaires indésirables sur YouTube (Classification de Texte)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../04-datasets/04.3-datasets.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">4.3 - Facteurs de Risque du Cancer du Col de l’Uterus (Classification)</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../05-interpretable_models/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">5 - Modèles interprétables</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true" aria-label="Basculer la section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../05-interpretable_models/05.1-linear-regression.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">5.1 - Régéression linéaire</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../05-interpretable_models/05.2-logistic-regression.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">5.2 - Régéression logistique</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../05-interpretable_models/05.3-glm-gam-more.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">5.3 - GLM, GAM et plus</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../05-interpretable_models/05.4-decision-tree.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">5.4 - Arbre de décision</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../05-interpretable_models/05.5-decision-rules.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">5.5 - Règles de décision</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../05-interpretable_models/05.6-rulefit.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">5.6 - Ajustement des règles</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../05-interpretable_models/05.7-other-interpretable-models.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">5.7 - Autres modèles interprétables</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../06-model_agnostic_methods/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">6 - Méthodes indépendantes du modèle</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../07-example/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">7 - Explications basées sur des exemples</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../08-global_model_agnostic_methods/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">8 - Méthodes globales indépendantes du modèle</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="true" aria-label="Basculer la section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../08-global_model_agnostic_methods/08.1-pdp.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">8.1 - Diagramme de dépendance partielle (PDP)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../08-global_model_agnostic_methods/08.2-ale.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">8.2 - Graphique des effets locaux accumulés (ALE)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../08-global_model_agnostic_methods/08.3-feature-interaction.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">8.3 - Interactions avec les fonctionnalités</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../08-global_model_agnostic_methods/08.4-functional-decomposition.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">8.4 - Functional Decomposition</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../08-global_model_agnostic_methods/08.5-permutation-feature-importance.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">8.5 - Décomposition fonctionnelle</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../08-global_model_agnostic_methods/08.6-global-surrogate.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">8.6 - Substitut global</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../08-global_model_agnostic_methods/08.7-prototype-criticisms.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">8.7 - Prototypes et critiques</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../09-local_model_agnostic_methods/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">9 - Méthodes locales indépendantes du modèle</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" role="navigation" aria-expanded="true" aria-label="Basculer la section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-6" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../09-local_model_agnostic_methods/09.1-ice.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">9.1 - Attente Conditionnelle Individuelle (<em>Individual Conditional Expectation - ICE</em>)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../09-local_model_agnostic_methods/09.2-lime.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">9.2 - Substitut local (LIME)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../09-local_model_agnostic_methods/09.3-counterfactual.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">9.3 - Explications contrefactuelles</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../09-local_model_agnostic_methods/09.4-anchors.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">9.4 - Règles de portée (ancres)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../09-local_model_agnostic_methods/09.5-shapley.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">9.5 - Valeurs de Shapley</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../09-local_model_agnostic_methods/09.6-shap.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">9.6 - SHAP (SHapley Additive exPlanations)</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../10-neuralnet/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">10 - Interprétation d'un réseau de neurone</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" role="navigation" aria-expanded="true" aria-label="Basculer la section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-7" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../10-neuralnet/10.1-learned-features.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">10.1 - Caractéristiques apprises</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../10-neuralnet/10.2-pixel-attribution.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">10.2 - Attribution de pixel</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../10-neuralnet/10.3-concepts.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">10.3 - Détecter les concepts</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../10-neuralnet/10.4-adversarial-examples.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text">10.4 - Exemples adverses</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../10-neuralnet/10.5-influential-instances.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">10.5 - Instances Influentes</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../11-future/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">11 - Un regard dans une boule de cristal</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-8" role="navigation" aria-expanded="true" aria-label="Basculer la section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-8" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../11-future/11.1-future-ml.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">11.1 - L’avenir de l’apprentissage automatique</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../11-future/11.2-future-interpretability.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">11.2 - L’avenir de l’interprétabilité</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../12-contribute/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">12 - Contribuer à ce livre</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../13-citation/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">13 - Citer ce livre</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../14-translations/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">14 - Traductions</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../15-acknowledgements/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">15 - Remerciements</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../Formulaire/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Des remarques ?</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../References/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Références</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active" data-toc-expanded="4">
    <h2 id="toc-title">Dans cette page</h2>
   
  <ul>
  <li><a href="#exemples-adverses-sec-adversarial_examples" id="toc-exemples-adverses-sec-adversarial_examples" class="nav-link active" data-scroll-target="#exemples-adverses-sec-adversarial_examples">10.4 - Exemples adverses {sec-adversarial_examples}</a>
  <ul>
  <li><a href="#méthodes-et-exemples" id="toc-méthodes-et-exemples" class="nav-link" data-scroll-target="#méthodes-et-exemples">10.4.1 Méthodes et exemples</a>
  <ul class="collapse">
  <li><a href="#quelque-chose-ne-va-pas-avec-mon-chien" id="toc-quelque-chose-ne-va-pas-avec-mon-chien" class="nav-link" data-scroll-target="#quelque-chose-ne-va-pas-avec-mon-chien">10.4.1.1 - Quelque chose ne va pas avec mon chien</a></li>
  <li><a href="#panda-perturbé-méthode-du-signe-de-gradient-rapide" id="toc-panda-perturbé-méthode-du-signe-de-gradient-rapide" class="nav-link" data-scroll-target="#panda-perturbé-méthode-du-signe-de-gradient-rapide">10.4.1.2 - Panda perturbé : méthode du signe de gradient rapide</a></li>
  <li><a href="#une-méduse-non-attendez.-une-baignoire-attaques-à-un-pixel" id="toc-une-méduse-non-attendez.-une-baignoire-attaques-à-un-pixel" class="nav-link" data-scroll-target="#une-méduse-non-attendez.-une-baignoire-attaques-à-un-pixel">10.4.1.3 - Une méduse… Non, attendez. Une baignoire : Attaques à un pixel</a></li>
  <li><a href="#tout-est-un-grille-pain-patch-adversaire" id="toc-tout-est-un-grille-pain-patch-adversaire" class="nav-link" data-scroll-target="#tout-est-un-grille-pain-patch-adversaire">10.4.1.4 - Tout est un grille-pain : patch adversaire</a></li>
  <li><a href="#napportez-jamais-une-tortue-imprimée-en-3d-à-un-combat-de-pistolets-même-si-votre-ordinateur-pense-que-cest-une-bonne-idée-exemples-adverses-robustes" id="toc-napportez-jamais-une-tortue-imprimée-en-3d-à-un-combat-de-pistolets-même-si-votre-ordinateur-pense-que-cest-une-bonne-idée-exemples-adverses-robustes" class="nav-link" data-scroll-target="#napportez-jamais-une-tortue-imprimée-en-3d-à-un-combat-de-pistolets-même-si-votre-ordinateur-pense-que-cest-une-bonne-idée-exemples-adverses-robustes">10.4.1.5 - N’apportez jamais une tortue imprimée en 3D à un combat de pistolets – même si votre ordinateur pense que c’est une bonne idée : exemples adverses robustes</a></li>
  <li><a href="#ladversaire-aux-yeux-bandés-attaque-de-la-boîte-noire" id="toc-ladversaire-aux-yeux-bandés-attaque-de-la-boîte-noire" class="nav-link" data-scroll-target="#ladversaire-aux-yeux-bandés-attaque-de-la-boîte-noire">10.4.1.6 - L’adversaire aux yeux bandés : attaque de la boîte noire</a></li>
  </ul></li>
  <li><a href="#la-perspective-de-la-cybersécurité" id="toc-la-perspective-de-la-cybersécurité" class="nav-link" data-scroll-target="#la-perspective-de-la-cybersécurité">10.4.2 La perspective de la cybersécurité</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../10-neuralnet/index.html">10 - Interprétation d’un réseau de neurone</a></li><li class="breadcrumb-item"><a href="../10-neuralnet/10.4-adversarial-examples.html">10.4 - Exemples adverses</a></li></ol></nav>
<div class="quarto-title">
</div>



<div class="quarto-title-meta">

    
  
    <div>
    <div class="quarto-title-meta-heading">Modifié</div>
    <div class="quarto-title-meta-contents">
      <p class="date-modified">19 février 2025</p>
    </div>
  </div>
    
  </div>
  


</header>


<div class="callout callout-style-default callout-warning callout-titled" title="Avertissement">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Avertissement
</div>
</div>
<div class="callout-body-container callout-body">
<p>En cours de traduction.</p>
</div>
</div>
<section id="exemples-adverses-sec-adversarial_examples" class="level2">
<h2 class="anchored" data-anchor-id="exemples-adverses-sec-adversarial_examples">10.4 - Exemples adverses {sec-adversarial_examples}</h2>
<!-- HTML only - Not in EN book-->
<p>Un exemple adversaire est une instance avec de petites perturbations intentionnelles de caractéristiques qui amènent un modèle d’apprentissage automatique à faire une fausse prédiction. Je recommande de lire d’abord le chapitre consacré aux <a href="../09-local_model_agnostic_methods/09.3-counterfactual.html">explications contrefactuelles</a>, car les concepts sont très similaires. Les exemples adverses sont des exemples contrefactuels visant à tromper le modèle, et non à l’interpréter.</p>
<p>Pourquoi sommes-nous intéressés par les exemples adverses ? Ne sont-ils pas juste des sous-produits curieux des modèles d’apprentissage automatique sans pertinence pratique ? La réponse est clairement “non”. Les exemples adverses rendent les modèles d’apprentissage automatique vulnérables aux attaques, comme dans les scénarios suivants.</p>
<p>Une voiture autonome entre en collision avec une autre voiture parce qu’elle ignore un panneau stop. Quelqu’un avait placé une image sur le panneau, qui ressemble à un panneau stop avec un peu de saleté pour les humains, mais était conçu pour ressembler à un panneau d’interdiction de stationner pour le logiciel de reconnaissance de panneaux de la voiture.</p>
<p>Un détecteur de spam échoue à classer un courriel comme spam. Le mail spam a été conçu pour ressembler à un courriel normal, mais avec l’intention de tromper le destinataire.</p>
<p>Un scanner propulsé par l’apprentissage automatique analyse les valises à la recherche d’armes à l’aéroport. Un couteau a été développé pour éviter la détection en faisant croire au système qu’il s’agit d’un parapluie.</p>
<p>Jetons un coup d’œil à certaines manières de créer des exemples adverses.</p>
<section id="méthodes-et-exemples" class="level3">
<h3 class="anchored" data-anchor-id="méthodes-et-exemples">10.4.1 Méthodes et exemples</h3>
<p>Il existe de nombreuses techniques pour créer des exemples adverses. La plupart des approches suggèrent de minimiser la distance entre l’exemple adverse et l’instance à manipuler, tout en déplaçant la prédiction vers le résultat souhaité (adverse). Certaines méthodes nécessitent un accès aux gradients du modèle, ce qui ne fonctionne bien sûr qu’avec des modèles basés sur des gradients tels que les réseaux neuronaux, d’autres méthodes ne nécessitent qu’un accès à la fonction de prédiction, ce qui rend ces méthodes agnostiques au modèle. Les méthodes de cette section se concentrent sur les classificateurs d’images avec des réseaux neuronaux profonds, car beaucoup de recherches sont effectuées dans ce domaine et la visualisation des images adverses est très éducative. Les exemples adverses pour les images sont des images avec des pixels intentionnellement perturbés dans le but de tromper le modèle lors du temps d’application. Les exemples démontrent de manière impressionnante à quel point les réseaux neuronaux profonds pour la reconnaissance d’objets peuvent être facilement trompés par des images qui semblent inoffensives pour les humains. Si vous n’avez pas encore vu ces exemples, vous pourriez être surpris, car les changements dans les prédictions sont incompréhensibles pour un observateur humain. Les exemples adverses sont comme des illusions d’optique, mais pour les machines.</p>
<section id="quelque-chose-ne-va-pas-avec-mon-chien" class="level4">
<h4 class="anchored" data-anchor-id="quelque-chose-ne-va-pas-avec-mon-chien">10.4.1.1 - Quelque chose ne va pas avec mon chien</h4>
<p>Szegedy et al.&nbsp;(2013)<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a> ont utilisé une approche d’optimisation basée sur le gradient dans leur travail “Propriétés Intrigantes des Réseaux Neuronaux” pour trouver des exemples adverses pour les réseaux neuronaux profonds.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../images/adversarial-ostrich.jpg" class="img-fluid figure-img" data-align="center"></p>
<figcaption>Adversarial examples for AlexNet by Szegedy et al.&nbsp;(2013). All images in the left column are correctly classified. The middle column shows the (magnified) error added to the images to produce the images in the right column all categorized (incorrectly) as “Ostrich”. “Intriguing properties of neural networks”, Figure 5 by Szegedy et al.&nbsp;CC-BY 3.0.</figcaption>
</figure>
</div>
<p>Ces exemples adverses ont été générés en minimisant la fonction suivante par rapport à <span class="math inline">\(r\)</span> :</p>
<p><span class="math display">\[loss(\hat{f}(x+r),l)+c \cdot |r|\]</span></p>
<p>Dans cette formule, <span class="math inline">\(x\)</span> est une image (représentée comme un vecteur de pixels), <span class="math inline">\(r\)</span> est les changements apportés aux pixels pour créer une image adverse (<span class="math inline">\(x+r\)</span> produit une nouvelle image), <span class="math inline">\(l\)</span> est la classe de résultat souhaitée, et le paramètre <span class="math inline">\(c\)</span> est utilisé pour équilibrer la distance entre les images et la distance entre les prédictions. Le premier terme est la distance entre le résultat prédit de l’exemple adverse et la classe désirée <span class="math inline">\(l\)</span>, le deuxième terme mesure la distance entre l’exemple adverse et l’image originale. Cette formulation est presque identique à la fonction de perte pour générer des <a href="../09-local_model_agnostic_methods/09.3-counterfactual.html">explications contrefactuelles</a>. Il y a des contraintes supplémentaires pour <span class="math inline">\(r\)</span> afin que les valeurs des pixels restent entre <span class="math inline">\(0\)</span> et <span class="math inline">\(1\)</span>. Les auteurs suggèrent de résoudre ce problème d’optimisation avec un L-BFGS à contraintes de boîte, un algorithme d’optimisation qui fonctionne avec des gradients.</p>
</section>
<section id="panda-perturbé-méthode-du-signe-de-gradient-rapide" class="level4">
<h4 class="anchored" data-anchor-id="panda-perturbé-méthode-du-signe-de-gradient-rapide">10.4.1.2 - Panda perturbé : méthode du signe de gradient rapide</h4>
<p>Goodfellow et al.&nbsp;(2014)<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a> ont inventé la méthode du signe de gradient rapide pour générer des images adverses. La méthode du signe de gradient utilise le gradient du modèle sous-jacent pour trouver des exemples adverses. L’image originale x est manipulée en ajoutant ou en soustrayant une petite erreur <span class="math inline">\(\epsilon\)</span> à chaque pixel. Le fait d’ajouter ou de soustraire <span class="math inline">\(\epsilon\)</span> dépend de si le signe du gradient pour un pixel est positif ou négatif. Ajouter des erreurs dans la direction du gradient signifie que l’image est intentionnellement altérée de sorte que la classification du modèle échoue.</p>
<p>La formule suivante décrit le cœur de la méthode du signe de gradient rapide :</p>
<p><span class="math display">\[x^\prime=x+\epsilon\cdot{}sign(\bigtriangledown_x{}J(\theta,x,y))\]</span></p>
<p>où <span class="math inline">\(\bigtriangledown_x{}J\)</span> est le gradient de la fonction de perte du modèle par rapport au vecteur de pixels d’entrée original <span class="math inline">\(x\)</span>, <span class="math inline">\(y\)</span> est le vecteur d’étiquettes réelles pour <span class="math inline">\(x\)</span> et <span class="math inline">\(\theta\)</span> est le vecteur de paramètres du modèle. Du vecteur de gradient (qui est aussi long que le vecteur des pixels d’entrée), nous n’avons besoin que du signe : Le signe du gradient est positif (<span class="math inline">\(+1\)</span>) si une augmentation de l’intensité des pixels augmente la perte (l’erreur que fait le modèle) et négatif (<span class="math inline">\(-1\)</span>) si une diminution de l’intensité des pixels augmente la perte. Cette vulnérabilité se produit lorsque un réseau neuronal traite une relation entre l’intensité d’un pixel d’entrée et le score de classe de manière linéaire. En particulier, les architectures de réseau neuronal qui favorisent la linéarité, telles que les LSTMs, les réseaux maxout, les réseaux avec des unités d’activation ReLU ou d’autres algorithmes d’apprentissage automatique linéaires tels que la régression logistique, sont vulnérables à la méthode du signe de gradient. L’attaque est réalisée par extrapolation. La linéarité entre l’intensité des pixels d’entrée et les scores de classe conduit à une vulnérabilité aux valeurs aberrantes, c’est-à-dire que le modèle peut être trompé en déplaçant les valeurs des pixels dans des zones en dehors de la distribution des données. Je m’attendais à ce que ces exemples adverses soient assez spécifiques à une architecture de réseau neuronal donnée. Mais il s’avère que vous pouvez réutiliser des exemples adverses pour tromper des réseaux avec une architecture différente formés sur la même tâche.</p>
<p>Goodfellow et al.&nbsp;(2014) ont suggéré d’ajouter des exemples adverses aux données d’entraînement pour apprendre des modèles robustes.</p>
</section>
<section id="une-méduse-non-attendez.-une-baignoire-attaques-à-un-pixel" class="level4">
<h4 class="anchored" data-anchor-id="une-méduse-non-attendez.-une-baignoire-attaques-à-un-pixel">10.4.1.3 - Une méduse… Non, attendez. Une baignoire : Attaques à un pixel</h4>
<p>L’approche présentée par Goodfellow et ses collègues (2014) nécessite de changer de nombreux pixels, même si ce n’est que de peu. Mais que se passerait-il si vous ne pouviez changer qu’un seul pixel ? Pourriez-vous tromper un modèle d’apprentissage automatique ? Su et al.&nbsp;(2019)<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a> ont montré qu’il est effectivement possible de tromper des classificateurs d’images en changeant un seul pixel.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../images/adversarial-1pixel.png" class="img-fluid figure-img" data-align="center"></p>
<figcaption>By intentionally changing a single pixel a neural network trained on ImageNet can be deceived to predict the wrong class instead of the original class.</figcaption>
</figure>
</div>
<p>Semblable aux contrefactuels, l’attaque à un pixel cherche un exemple modifié <span class="math inline">\(x^\prime\)</span> qui se rapproche de l’image originale <span class="math inline">\(x\)</span>, mais change la prédiction pour un résultat adverse. Cependant, la définition de la proximité diffère : seul un seul pixel peut changer. L’attaque à un pixel utilise l’évolution différentielle pour déterminer quel pixel doit être changé et comment. L’évolution différentielle s’inspire librement de l’évolution biologique des espèces. Une population d’individus appelés solutions candidates se recombinent génération après génération jusqu’à ce qu’une solution soit trouvée. Chaque solution candidate encode une modification de pixel et est représentée par un vecteur de cinq éléments : les coordonnées <span class="math inline">\(x\)</span> et <span class="math inline">\(y\)</span> et les valeurs des composantes rouge, vert et bleu (RVB). La recherche commence avec, par exemple, 400 solutions candidates (i.e suggestions de modification de pixel) et crée une nouvelle génération de solutions candidates (enfants) à partir de la génération parente en utilisant la formule suivante :</p>
<p><span class="math display">\[x_{i}(g+1)=x_{r1}(g)+F\cdot(x_{r2}(g)-x_{r3}(g))\]</span></p>
<p>où chaque <span class="math inline">\(x_i\)</span> est un élément d’une solution candidate (soit coordonnée <span class="math inline">\(x\)</span>, coordonnée <span class="math inline">\(y\)</span>, valeurs des compodantes rouge, vert ou bleu), <span class="math inline">\(g\)</span> est la génération actuelle, <span class="math inline">\(F\)</span> est un paramètre d’échelle (fixé à <span class="math inline">\(0,5\)</span>) et <span class="math inline">\(r1\)</span>, <span class="math inline">\(r2\)</span> et <span class="math inline">\(r3\)</span> sont des nombres aléatoires différents. Chaque nouvelle solution candidate enfant est à son tour un pixel avec les cinq attributs pour l’emplacement et la couleur et chacun de ces attributs est un mélange de trois pixels parents aléatoires.</p>
<div class="callout callout-style-default callout-important callout-titled" title="Note du traducteur">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note du traducteur
</div>
</div>
<div class="callout-body-container callout-body">
<p>Possible problème de traduction du sens dans le paragraphe ci-sessus : <em>Each new child candidate solution is in turn a pixel with the five attributes for location and color and each of those attributes is a mixture of three random parent pixels.</em></p>
</div>
</div>
<p>La création de solutions candidates est arrêtée si l’une des solutions candidates est un exemple adverse, c’est-à-dire qu’elle est classée dans une classe incorrecte, ou si le nombre maximum d’itérations spécifié par l’utilisateur est atteint.</p>
</section>
<section id="tout-est-un-grille-pain-patch-adversaire" class="level4">
<h4 class="anchored" data-anchor-id="tout-est-un-grille-pain-patch-adversaire">10.4.1.4 - Tout est un grille-pain : patch adversaire</h4>
<p>Une de mes méthodes préférées amène les exemples adverses dans la réalité physique. Brown et al.&nbsp;(2017)<a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a> ont conçu une étiquette imprimable qui peut être collée à côté d’objets pour les faire ressembler à des grille-pains pour un classificateur d’images. Un travail brillant !</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../images/adversarial-toaster.png" class="img-fluid figure-img" data-align="center"></p>
<figcaption>A sticker that makes a VGG16 classifier trained on ImageNet categorize an image of a banana as a toaster. Work by Brown et al.&nbsp;(2017).</figcaption>
</figure>
</div>
<p>Cette méthode diffère des méthodes présentées jusqu’à présent pour les exemples adverses, puisque la restriction impliquant que l’image adverse doit être très proche de l’image originale est supprimée. Au lieu de cela, la méthode remplace complètement une partie de l’image par un patch qui peut prendre n’importe quelle forme. L’image du patch est optimisée sur différentes images de fond, avec différentes positions du patch sur les images, parfois déplacé, parfois agrandi ou réduit et tourné, de sorte que le patch fonctionne dans de nombreuses situations. À la fin, cette image optimisée peut être imprimée et utilisée pour tromper les classificateurs d’images en situation réelle.</p>
</section>
<section id="napportez-jamais-une-tortue-imprimée-en-3d-à-un-combat-de-pistolets-même-si-votre-ordinateur-pense-que-cest-une-bonne-idée-exemples-adverses-robustes" class="level4">
<h4 class="anchored" data-anchor-id="napportez-jamais-une-tortue-imprimée-en-3d-à-un-combat-de-pistolets-même-si-votre-ordinateur-pense-que-cest-une-bonne-idée-exemples-adverses-robustes">10.4.1.5 - N’apportez jamais une tortue imprimée en 3D à un combat de pistolets – même si votre ordinateur pense que c’est une bonne idée : exemples adverses robustes</h4>
<p>La méthode suivante ajoute littéralement une autre dimension au grille-pain : Athalye et al.&nbsp;(2017)<a href="#fn5" class="footnote-ref" id="fnref5" role="doc-noteref"><sup>5</sup></a> ont imprimé en 3D une tortue conçue pour ressembler à un fusil pour un réseau neuronal profond sous presque tous les angles possibles. Oui, vous avez bien lu. Un objet physique qui ressemble à une tortue pour les humains ressemble à un fusil pour l’ordinateur !</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../images/adversarial-turtle.png" class="img-fluid figure-img" data-align="center"></p>
<figcaption>Athalye et al.&nbsp;(2017) created a 3D-printed that is recognized as a rifle by TensorFlow’s standard pre-trained InceptionV3 classifier.</figcaption>
</figure>
</div>
<p>Les auteurs ont trouvé un moyen de créer un exemple adverse en 3D pour un classificateur 2D qui est adverse sur des transformations, telles que toutes les possibilités de faire tourner la tortue, de zoomer, etc. D’autres approches telles que la méthode du gradient rapide ne fonctionnent plus lorsque l’image est tournée ou que l’angle de vue change. Athalye et al.&nbsp;(2017)^turtle] proposent l’algorithme Expectation Over Transformation (EOT), qui est une méthode pour générer des exemples adverses qui fonctionnent même lorsque l’image est transformée. L’idée principale derrière EOT est d’optimiser des exemples adverses sur de nombreuses transformations possibles. Au lieu de minimiser la distance entre l’exemple adverse et l’image originale, EOT maintient la distance attendue entre les deux en dessous d’un certain seuil, étant donné une distribution sélectionnée de transformations possibles. La distance attendue sous transformation peut s’écrire comme :</p>
<p><span class="math display">\[\mathbb{E}_{t\sim{}T}[d(t(x^\prime),t(x))]\]</span></p>
<p>où <span class="math inline">\(x\)</span> est l’image originale, <span class="math inline">\(t(x)\)</span> l’image transformée (par exemple, tournée), <span class="math inline">\(x'\)</span> l’exemple adverse et <span class="math inline">\(t(x^\prime)\)</span> sa version transformée. Outre le travail avec une distribution de transformations, la méthode EOT suit le schéma familier de cadrer la recherche d’exemples adverses comme un problème d’optimisation. Nous essayons de trouver un exemple adverse <span class="math inline">\(x^\prime\)</span> qui maximise la probabilité pour la classe sélectionnée <span class="math inline">\(y_t\)</span> (par exemple, “fusil”) à travers la distribution de transformations possibles <span class="math inline">\(T\)</span> :</p>
<p><span class="math display">\[\arg\max_{x^\prime}\mathbb{E}_{t\sim{}T}[log{}P(y_t|t(x^\prime))]\]</span></p>
<p>Avec la contrainte que la distance attendue sur toutes les transformations possibles entre l’exemple adverse <span class="math inline">\(x^\prime\)</span> et l’image originale <span class="math inline">\(x\)</span> reste en dessous d’un certain seuil :</p>
<p><span class="math display">\[\mathbb{E}_{t\sim{}T}[d(t(x^\prime),t(x))]&lt;\epsilon\quad\text{and}\quad{}x\in[0,1]^d\]</span></p>
<p>Je pense que nous devrions être préoccupés par les possibilités que cette méthode permet. Les autres méthodes sont basées sur la manipulation d’images numériques. Cependant, ces exemples adverses robustes imprimés en 3D peuvent être insérés dans n’importe quelle scène réelle et tromper un ordinateur pour mal classer un objet. Retournons la situation : et si quelqu’un crée un fusil qui ressemble à une tortue ?</p>
</section>
<section id="ladversaire-aux-yeux-bandés-attaque-de-la-boîte-noire" class="level4">
<h4 class="anchored" data-anchor-id="ladversaire-aux-yeux-bandés-attaque-de-la-boîte-noire">10.4.1.6 - L’adversaire aux yeux bandés : attaque de la boîte noire</h4>
<p>Imaginez le scénario suivant : Je vous donne accès à mon excellent classificateur d’images via une API Web. Vous pouvez obtenir des prédictions du modèle, mais vous n’avez pas accès aux paramètres du modèle. Depuis le confort de votre canapé, vous pouvez envoyer des données et mon service répond avec les classifications correspondantes. La plupart des attaques adverses ne sont pas conçues pour fonctionner dans ce scénario car elles nécessitent un accès au gradient du réseau neuronal profond sous-jacent pour trouver des exemples adverses. Papernot et ses collègues (2017)<a href="#fn6" class="footnote-ref" id="fnref6" role="doc-noteref"><sup>6</sup></a> ont montré qu’il est possible de créer des exemples adverses sans informations internes sur le modèle et sans accès aux données d’entraînement. Ce type d’attaque (presque) sans connaissance préalable est appelé attaque de boîte noire.</p>
<p>Comment ça fonctionne :</p>
<ol type="1">
<li>Commencez avec quelques images qui proviennent du même domaine que les données d’entraînement, par exemple, si le classificateur à attaquer est un classificateur de chiffres, utilisez des images de chiffres. La connaissance du domaine est requise, mais pas l’accès aux données d’entraînement.</li>
<li>Obtenez des prédictions pour l’ensemble actuel d’images de la boîte noire.</li>
<li>Entraînez un modèle substitut sur l’ensemble actuel d’images (par exemple, un réseau neuronal).</li>
<li>Créez un nouvel ensemble d’images synthétiques en utilisant une heuristique qui examine, pour l’ensemble actuel d’images, dans quelle direction manipuler les pixels pour que la sortie du modèle ait plus de variance.</li>
<li>Répétez les étapes 2 à 4 pour un nombre prédéfini d’époques.</li>
<li>Créez des exemples adverses pour le modèle substitut en utilisant la méthode du gradient rapide (ou similaire).</li>
<li>Attaquez le modèle original avec des exemples adverses.</li>
</ol>
<p>Le but du modèle substitut est d’approximer les frontières de décision du modèle de la boîte noire, mais pas nécessairement d’atteindre la même précision.</p>
<p>Les auteurs ont testé cette approche en attaquant des classificateurs d’images formés sur divers services en ligne d’apprentissage automatique. Ces services forment des classificateurs d’images sur des images et des étiquettes téléchargées par les utilisateurs. Le logiciel entraine automatiquement le modèle – parfois avec un algorithme inconnu de l’utilisateur – et le déploie. Le classificateur donne alors des prédictions pour les images téléchargées, mais le modèle lui-même ne peut être inspecté ou téléchargé. Les auteurs ont pu trouver des exemples adverses pour divers fournisseurs, avec jusqu’à 84% des exemples adverses mal classés.</p>
<p>La méthode fonctionne même si le modèle “boîte noire” à tromper n’est pas un réseau neuronal. Cela inclut des modèles d’apprentissage automatique sans gradient tels que des arbres de décision.</p>
</section>
</section>
<section id="la-perspective-de-la-cybersécurité" class="level3">
<h3 class="anchored" data-anchor-id="la-perspective-de-la-cybersécurité">10.4.2 La perspective de la cybersécurité</h3>
<p>L’apprentissage automatique traite des inconnus connus : la prédiction de points de données inconnus à partir d’une distribution connue. La défense contre les attaques traite des inconnus inconnus : prédire de manière robuste des points de données inconnus à partir d’une distribution inconnue d’entrées adverses. À mesure que l’apprentissage automatique est intégré dans de plus en plus de systèmes, tels que les véhicules autonomes ou les dispositifs médicaux, ils deviennent également des points d’entrée pour les attaques. Même si les prédictions d’un modèle d’apprentissage automatique sur un ensemble de test sont correctes à 100 %, des exemples adverses peuvent être trouvés pour tromper le modèle. La défense des modèles d’apprentissage automatique contre les cyberattaques est une nouvelle partie du domaine de la cybersécurité.</p>
<p>Biggio et al.&nbsp;(2018)<a href="#fn7" class="footnote-ref" id="fnref7" role="doc-noteref"><sup>7</sup></a> donnent un bon aperçu de dix années de recherche sur l’apprentissage automatique adversaire, sur lesquelles cette section est basée. La cybersécurité est une course aux armements dans laquelle les attaquants et les défenseurs se surpassent à maintes reprises.</p>
<p><strong>Il existe trois règles d’or en cybersécurité : 1) connaître son adversaire, 2) être proactif et 3) se protéger.</strong></p>
<p>Différentes applications ont différents adversaires. Les personnes qui tentent de frauder d’autres personnes par courrier électronique pour voler leur argent sont des agents adversaires des utilisateurs et des fournisseurs de services de messagerie. Les fournisseurs veulent protéger leurs utilisateurs, afin qu’ils puissent continuer à utiliser leur programme de messagerie, les attaquants veulent amener les gens à leur donner de l’argent. Connaître ses adversaires signifie connaître leurs objectifs. En supposant que vous ne savez pas que ces spammeurs existent et que le seul abus du service de messagerie est l’envoi de copies piratées de musique, alors la défense serait différente (par exemple, analyser les pièces jointes pour du matériel protégé par le droit d’auteur au lieu d’analyser le texte pour des indicateurs de spam).</p>
<p>Être proactif signifie tester activement et identifier les points faibles du système. Vous êtes proactif lorsque vous essayez activement de tromper le modèle avec des exemples adverses, puis vous défendez contre eux. Utiliser des méthodes d’interprétation pour comprendre quelles caractéristiques sont importantes et comment les caractéristiques affectent la prédiction est également une étape proactive pour comprendre les faiblesses d’un modèle d’apprentissage automatique. En tant que data scientist, faites-vous confiance à votre modèle dans ce monde dangereux sans jamais avoir regardé au-delà de la puissance prédictive sur un ensemble de test ? Avez-vous analysé comment le modèle se comporte dans différents scénarios, identifié les entrées les plus importantes, vérifié les explications des prédictions pour certains exemples ? Avez-vous essayé de trouver des entrées adverses ? L’interprétabilité des modèles d’apprentissage automatique joue un rôle majeur en cybersécurité. Être réactif, l’opposé de proactif, signifie attendre que le système ait été attaqué et seulement alors comprendre le problème et installer des mesures de défense.</p>
<p>Comment pouvons-nous protéger nos systèmes d’apprentissage automatique contre les exemples adverses ? Une approche proactive est la reformation itérative du classificateur avec des exemples adverses, également appelée entraînement adverse. D’autres approches sont basées sur la théorie des jeux, telles que l’apprentissage de transformations invariantes des caractéristiques ou l’optimisation robuste (régularisation). Une autre méthode proposée est d’utiliser plusieurs classificateurs au lieu d’un seul et de les faire voter pour la prédiction (ensemble), mais cela n’a aucune garantie de fonctionner, car ils pourraient tous souffrir de similaires exemples adverses. Une autre approche qui ne fonctionne pas bien non plus est le masquage du gradient, qui construit un modèle sans gradients utiles en utilisant un classificateur du plus proche voisin au lieu du modèle original.</p>
<p>Nous pouvons distinguer les types d’attaques en fonction de la connaissance qu’a l’attaquant du système.</p>
<ul>
<li>Les attaquants peuvent avoir une connaissance parfaite (attaque de boîte blanche), ce qui signifie qu’ils savent tout sur le modèle, comme le type de modèle, les paramètres et les données d’entraînement ;</li>
<li>Les attaquants peuvent avoir une connaissance partielle (attaque de boîte grise), ce qui signifie qu’ils pourraient seulement connaître la représentation des caractéristiques et le type de modèle utilisé, mais n’ont pas accès aux données d’entraînement ou aux paramètres ;</li>
<li>Les attaquants peuvent n’avoir aucune connaissance (attaque de boîte noire), ce qui signifie qu’ils ne peuvent interroger le modèle que de manière boîte noire, et n’ont pas accès aux données d’entraînement ni aux informations sur les paramètres du modèle. Selon le niveau d’information, les attaquants peuvent utiliser différentes techniques pour attaquer le modèle.</li>
</ul>
<p>Comme nous l’avons vu dans les exemples, même dans le cas de la boîte noire, des exemples adverses peuvent être créés, de sorte que cacher des informations sur les données et le modèle n’est pas suffisant pour se protéger contre les attaques.</p>
<p>Étant donné la nature du jeu du chat et de la souris entre les attaquants et les défenseurs, nous assisterons à beaucoup de développements et d’innovations dans ce domaine. Pensez juste aux nombreux types différents de courriels indésirables qui évoluent constamment. De nouvelles méthodes d’attaques contre les modèles d’apprentissage automatique sont inventées et de nouvelles mesures défensives sont proposées contre ces nouvelles attaques. Des attaques plus puissantes sont développées pour éviter les dernières défenses et ainsi de suite, ad infinitum. Avec ce chapitre, j’espère vous avoir sensibilisé au problème des exemples adverses et que seulement en étudiant de manière proactive les modèles d’apprentissage automatique nous serons capables de découvrir et de remédier à leurs faiblesses.</p>
<!-- REFERENCES -->
<!-- 02 -->
<!-- 02.3 -->
<!-- 03 -->
<!-- 03.1 -->
<!--
[^Miller2017]: Miller, Tim. "Explanation in artificial intelligence: Insights from the social sciences." arXiv Preprint arXiv:1706.07269. (2017).
-->
<!-- 03.3 -->
<!-- 03.4 -->
<!--
[^Doshi2017]: Doshi-Velez, Finale, and Been Kim. "Towards a rigorous science of interpretable machine learning," no. Ml: 1–13. https://arxiv.org/abs/1702.08608 (2017).
-->
<!-- 03.5 -->
<!-- 03.6 -->
<!--
[^Miller2017]: Miller, Tim. "Explanation in artificial intelligence: Insights from the social sciences." arXiv Preprint arXiv:1706.07269. (2017).
-->
<!-- 04.1 -->
<!-- 04.2 -->
<!-- 04.3 -->
<!-- 05.1 -->
<!-- 05.4 -->
<!--
[^Hastie]: Friedman, Jerome, Trevor Hastie, and Robert Tibshirani. "The elements of statistical learning". hastie.su.domains/ElemStatLearn (2009).
-->
<!-- 05.5 -->
<!-- 05.6 -->
<!-- 06.0 -->
<!--
[^Ribeiro2016]: Ribeiro, Marco Tulio, Sameer Singh, and Carlos Guestrin. "Model-agnostic interpretability of machine learning." ICML Workshop on Human Interpretability in Machine Learning. (2016).
-->
<!-- 07.0 -->
<!-- 08.1 -->
<!-- 08.2 -->
<!-- 08.3 -->
<!--
[^Friedman2008]: Friedman, Jerome H, and Bogdan E Popescu. "Predictive learning via rule ensembles." The Annals of Applied Statistics. JSTOR, 916–54. (2008).
-->
<!--
[^pdp-importance]: Greenwell, Brandon M., Bradley C. Boehmke, and Andrew J. McCarthy. "A simple and effective model-based variable importance measure." arXiv preprint arXiv:1805.04755 (2018).
-->
<!-- 08.4 -->
<!--
[^fanova]: Hooker, Giles. "Discovering additive structure in black box functions." Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining. (2004).
-->
<!--
[^ale]: Apley, Daniel W., and Jingyu Zhu. "Visualizing the effects of predictor variables in black box supervised learning models." Journal of the Royal Statistical Society: Series B (Statistical Methodology) 82.4 (2020): 1059-1086.
-->
<!-- 08.5 -->
<!-- 08.7 -->
<!--
[^critique]: Kim, Been, Rajiv Khanna, and Oluwasanmi O. Koyejo. "Examples are not enough, learn to criticize! Criticism for interpretability." Advances in Neural Information Processing Systems (2016).
-->
<!-- 09.1 -->
<!-- 09.2 -->
<!-- 09.3 -->
<!-- 09.4 -->
<!-- 09.5 -->
<!-- 09.6 -->
<!--
[^lundberg2017]: Lundberg, Scott M., and Su-In Lee. "A unified approach to interpreting model predictions." Advances in Neural Information Processing Systems (2017).
-->
<!--
[^cond1]: Sundararajan, Mukund, and Amir Najmi. "The many Shapley values for model explanation." arXiv preprint arXiv:1908.08474 (2019).
-->
<!--
[^cond2]: Janzing, Dominik, Lenon Minorics, and Patrick Blöbaum. "Feature relevance quantification in explainable AI: A causal problem." International Conference on Artificial Intelligence and Statistics. PMLR (2020).
-->
<!--
[^fool]: Slack, Dylan, Sophie Hilgard, Emily Jia, Sameer Singh, and Himabindu Lakkaraju. "Fooling lime and shap: Adversarial attacks on post hoc explanation methods." In Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society, pp. 180-186 (2020).
-->
<!-- 10.0 -->
<!-- 10.1 -->
<!-- 10.2 -->
<!--
[^integrated-gradients]: Sundararajan, Mukund, Ankur Taly, and Qiqi Yan. "Axiomatic attribution for deep networks." Proceedings of the 34th International Conference on Machine Learning-Volume 70. JMLR. org, 2017.
-->
<!--
[^grad-cam]: Selvaraju, Ramprasaath R., et al. "Grad-cam: Visual explanations from deep networks via gradient-based localization." Proceedings of the IEEE international conference on computer vision. (2017).
-->
<!--
[^guided-backpropagation]: Springenberg, Jost Tobias, et al. "Striving for simplicity: The all convolutional net." arXiv preprint arXiv:1412.6806 (2014).
-->
<!--
[^lrp]: Bach, Sebastian, et al. "On pixel-wise explanations for non-linear classifier decisions by layer-wise relevance propagation." PloS one 10.7 (2015).
-->
<!-- References about problems -->
<!--
[^better-understanding]: Ancona, Marco, et al. "Towards better understanding of gradient-based attribution methods for deep neural networks." arXiv preprint arXiv:1711.06104 (2017).
-->
<!--
[^perplexing-behavior]: Nie, Weili, Yang Zhang, and Ankit Patel. "A theoretical explanation for perplexing behaviors of backpropagation-based visualizations." arXiv preprint arXiv:1805.07039 (2018).
-->
<!-- Toolboxes -->
<!--
[^innvestigate]: Alber, Maximilian, Sebastian Lapuschkin, Philipp Seegerer, Miriam Hägele, Kristof T. Schütt, Grégoire Montavon, Wojciech Samek, Klaus-Robert Müller, Sven Dähne, and Pieter-Jan Kindermans. "iNNvestigate neural networks!." J. Mach. Learn. Res. 20, no. 93 (2019): 1-8.
-->
<!--
[^human-visuals]: Linsley, Drew, et al. "What are the visual features underlying human versus machine vision?." Proceedings of the IEEE International Conference on Computer Vision Workshops. 2017.
-->
<!-- 10.3 -->
<!-- 10.4 -->
<!-- 10.5 -->


</section>
</section>


<a onclick="window.scrollTo(0, 0); return false;" role="button" id="quarto-back-to-top"><i class="bi bi-arrow-up"></i> Retour au sommet</a><div id="quarto-appendix" class="default"><section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes"><h2 class="anchored quarto-appendix-heading">Notes de bas de page</h2>

<ol>
<li id="fn1"><p>Szegedy, Christian, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow, and Rob Fergus. “Intriguing properties of neural networks.” arXiv preprint arXiv:1312.6199 (2013).<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p>Goodfellow, Ian J., Jonathon Shlens, and Christian Szegedy. “Explaining and harnessing adversarial examples.” arXiv preprint arXiv:1412.6572 (2014).<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3"><p>Su, Jiawei, Danilo Vasconcellos Vargas, and Kouichi Sakurai. “One pixel attack for fooling deep neural networks.” IEEE Transactions on Evolutionary Computation (2019).<a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn4"><p>Brown, Tom B., Dandelion Mané, Aurko Roy, Martín Abadi, and Justin Gilmer. “Adversarial patch.” arXiv preprint arXiv:1712.09665 (2017).<a href="#fnref4" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn5"><p>Athalye, Anish, and Ilya Sutskever. “Synthesizing robust adversarial examples.” arXiv preprint arXiv:1707.07397 (2017).<a href="#fnref5" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn6"><p>Papernot, Nicolas, et al.&nbsp;“Practical black-box attacks against machine learning.” Proceedings of the 2017 ACM on Asia Conference on Computer and Communications Security. ACM (2017).<a href="#fnref6" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn7"><p>Biggio, Battista, and Fabio Roli. “Wild Patterns: Ten years after the rise of adversarial machine learning.” Pattern Recognition 84 (2018): 317-331.<a href="#fnref7" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copié");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copié");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
          // target, if specified
          link.setAttribute("target", "_blank");
          if (link.getAttribute("rel") === null) {
            link.setAttribute("rel", "noopener");
          }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../10-neuralnet/10.3-concepts.html" class="pagination-link" aria-label="10.3 - Détecter les concepts">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text">10.3 - Détecter les concepts</span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../10-neuralnet/10.5-influential-instances.html" class="pagination-link" aria-label="10.5 - Instances Influentes">
        <span class="nav-page-text">10.5 - Instances Influentes</span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
      &nbsp;
    </div>   
    <div class="nav-footer-center">
<p>Copyright 2018-2025, Christoph Molnar <br> Traduction 2024-2025 : Nicolas Guillard</p>
</div>
    <div class="nav-footer-right">
      &nbsp;
    </div>
  </div>
</footer>




</body></html>