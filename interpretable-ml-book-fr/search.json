[
  {
    "objectID": "10-neuralnet/index.html",
    "href": "10-neuralnet/index.html",
    "title": "10 - Interprétation d’un réseau de neurone",
    "section": "",
    "text": "Avertissement\n\n\n\nEn cours de traduction.",
    "crumbs": [
      "10 - Interprétation d'un réseau de neurone"
    ]
  },
  {
    "objectID": "10-neuralnet/index.html#footnotes",
    "href": "10-neuralnet/index.html#footnotes",
    "title": "10 - Interprétation d’un réseau de neurone",
    "section": "Notes de bas de page",
    "text": "Notes de bas de page\n\n\nOlga Russakovsky and Jia Deng (equal contribution), Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg and Li Fei-Fei. “ImageNet large scale visual recognition challenge”. IJCV (2015).↩︎",
    "crumbs": [
      "10 - Interprétation d'un réseau de neurone"
    ]
  },
  {
    "objectID": "10-neuralnet/10.4-adversarial-examples.html",
    "href": "10-neuralnet/10.4-adversarial-examples.html",
    "title": "Apprentissage automatique interprétable",
    "section": "",
    "text": "Avertissement\n\n\n\nEn cours de traduction.",
    "crumbs": [
      "10 - Interprétation d'un réseau de neurone",
      "10.4 - Exemples adverses"
    ]
  },
  {
    "objectID": "10-neuralnet/10.4-adversarial-examples.html#exemples-adverses-sec-adversarial_examples",
    "href": "10-neuralnet/10.4-adversarial-examples.html#exemples-adverses-sec-adversarial_examples",
    "title": "Apprentissage automatique interprétable",
    "section": "10.4 - Exemples adverses {sec-adversarial_examples}",
    "text": "10.4 - Exemples adverses {sec-adversarial_examples}\n\nUn exemple adversaire est une instance avec de petites perturbations intentionnelles de caractéristiques qui amènent un modèle d’apprentissage automatique à faire une fausse prédiction. Je recommande de lire d’abord le chapitre consacré aux explications contrefactuelles, car les concepts sont très similaires. Les exemples adverses sont des exemples contrefactuels visant à tromper le modèle, et non à l’interpréter.\nPourquoi sommes-nous intéressés par les exemples adverses ? Ne sont-ils pas juste des sous-produits curieux des modèles d’apprentissage automatique sans pertinence pratique ? La réponse est clairement “non”. Les exemples adverses rendent les modèles d’apprentissage automatique vulnérables aux attaques, comme dans les scénarios suivants.\nUne voiture autonome entre en collision avec une autre voiture parce qu’elle ignore un panneau stop. Quelqu’un avait placé une image sur le panneau, qui ressemble à un panneau stop avec un peu de saleté pour les humains, mais était conçu pour ressembler à un panneau d’interdiction de stationner pour le logiciel de reconnaissance de panneaux de la voiture.\nUn détecteur de spam échoue à classer un courriel comme spam. Le mail spam a été conçu pour ressembler à un courriel normal, mais avec l’intention de tromper le destinataire.\nUn scanner propulsé par l’apprentissage automatique analyse les valises à la recherche d’armes à l’aéroport. Un couteau a été développé pour éviter la détection en faisant croire au système qu’il s’agit d’un parapluie.\nJetons un coup d’œil à certaines manières de créer des exemples adverses.\n\n10.4.1 Méthodes et exemples\nIl existe de nombreuses techniques pour créer des exemples adverses. La plupart des approches suggèrent de minimiser la distance entre l’exemple adverse et l’instance à manipuler, tout en déplaçant la prédiction vers le résultat souhaité (adverse). Certaines méthodes nécessitent un accès aux gradients du modèle, ce qui ne fonctionne bien sûr qu’avec des modèles basés sur des gradients tels que les réseaux neuronaux, d’autres méthodes ne nécessitent qu’un accès à la fonction de prédiction, ce qui rend ces méthodes agnostiques au modèle. Les méthodes de cette section se concentrent sur les classificateurs d’images avec des réseaux neuronaux profonds, car beaucoup de recherches sont effectuées dans ce domaine et la visualisation des images adverses est très éducative. Les exemples adverses pour les images sont des images avec des pixels intentionnellement perturbés dans le but de tromper le modèle lors du temps d’application. Les exemples démontrent de manière impressionnante à quel point les réseaux neuronaux profonds pour la reconnaissance d’objets peuvent être facilement trompés par des images qui semblent inoffensives pour les humains. Si vous n’avez pas encore vu ces exemples, vous pourriez être surpris, car les changements dans les prédictions sont incompréhensibles pour un observateur humain. Les exemples adverses sont comme des illusions d’optique, mais pour les machines.\n\n10.4.1.1 - Quelque chose ne va pas avec mon chien\nSzegedy et al. (2013)1 ont utilisé une approche d’optimisation basée sur le gradient dans leur travail “Propriétés Intrigantes des Réseaux Neuronaux” pour trouver des exemples adverses pour les réseaux neuronaux profonds.\n\n\n\nAdversarial examples for AlexNet by Szegedy et al. (2013). All images in the left column are correctly classified. The middle column shows the (magnified) error added to the images to produce the images in the right column all categorized (incorrectly) as “Ostrich”. “Intriguing properties of neural networks”, Figure 5 by Szegedy et al. CC-BY 3.0.\n\n\nCes exemples adverses ont été générés en minimisant la fonction suivante par rapport à \\(r\\) :\n\\[loss(\\hat{f}(x+r),l)+c \\cdot |r|\\]\nDans cette formule, \\(x\\) est une image (représentée comme un vecteur de pixels), \\(r\\) est les changements apportés aux pixels pour créer une image adverse (\\(x+r\\) produit une nouvelle image), \\(l\\) est la classe de résultat souhaitée, et le paramètre \\(c\\) est utilisé pour équilibrer la distance entre les images et la distance entre les prédictions. Le premier terme est la distance entre le résultat prédit de l’exemple adverse et la classe désirée \\(l\\), le deuxième terme mesure la distance entre l’exemple adverse et l’image originale. Cette formulation est presque identique à la fonction de perte pour générer des explications contrefactuelles. Il y a des contraintes supplémentaires pour \\(r\\) afin que les valeurs des pixels restent entre \\(0\\) et \\(1\\). Les auteurs suggèrent de résoudre ce problème d’optimisation avec un L-BFGS à contraintes de boîte, un algorithme d’optimisation qui fonctionne avec des gradients.\n\n\n10.4.1.2 - Panda perturbé : méthode du signe de gradient rapide\nGoodfellow et al. (2014)2 ont inventé la méthode du signe de gradient rapide pour générer des images adverses. La méthode du signe de gradient utilise le gradient du modèle sous-jacent pour trouver des exemples adverses. L’image originale x est manipulée en ajoutant ou en soustrayant une petite erreur \\(\\epsilon\\) à chaque pixel. Le fait d’ajouter ou de soustraire \\(\\epsilon\\) dépend de si le signe du gradient pour un pixel est positif ou négatif. Ajouter des erreurs dans la direction du gradient signifie que l’image est intentionnellement altérée de sorte que la classification du modèle échoue.\nLa formule suivante décrit le cœur de la méthode du signe de gradient rapide :\n\\[x^\\prime=x+\\epsilon\\cdot{}sign(\\bigtriangledown_x{}J(\\theta,x,y))\\]\noù \\(\\bigtriangledown_x{}J\\) est le gradient de la fonction de perte du modèle par rapport au vecteur de pixels d’entrée original \\(x\\), \\(y\\) est le vecteur d’étiquettes réelles pour \\(x\\) et \\(\\theta\\) est le vecteur de paramètres du modèle. Du vecteur de gradient (qui est aussi long que le vecteur des pixels d’entrée), nous n’avons besoin que du signe : Le signe du gradient est positif (\\(+1\\)) si une augmentation de l’intensité des pixels augmente la perte (l’erreur que fait le modèle) et négatif (\\(-1\\)) si une diminution de l’intensité des pixels augmente la perte. Cette vulnérabilité se produit lorsque un réseau neuronal traite une relation entre l’intensité d’un pixel d’entrée et le score de classe de manière linéaire. En particulier, les architectures de réseau neuronal qui favorisent la linéarité, telles que les LSTMs, les réseaux maxout, les réseaux avec des unités d’activation ReLU ou d’autres algorithmes d’apprentissage automatique linéaires tels que la régression logistique, sont vulnérables à la méthode du signe de gradient. L’attaque est réalisée par extrapolation. La linéarité entre l’intensité des pixels d’entrée et les scores de classe conduit à une vulnérabilité aux valeurs aberrantes, c’est-à-dire que le modèle peut être trompé en déplaçant les valeurs des pixels dans des zones en dehors de la distribution des données. Je m’attendais à ce que ces exemples adverses soient assez spécifiques à une architecture de réseau neuronal donnée. Mais il s’avère que vous pouvez réutiliser des exemples adverses pour tromper des réseaux avec une architecture différente formés sur la même tâche.\nGoodfellow et al. (2014) ont suggéré d’ajouter des exemples adverses aux données d’entraînement pour apprendre des modèles robustes.\n\n\n10.4.1.3 - Une méduse… Non, attendez. Une baignoire : Attaques à un pixel\nL’approche présentée par Goodfellow et ses collègues (2014) nécessite de changer de nombreux pixels, même si ce n’est que de peu. Mais que se passerait-il si vous ne pouviez changer qu’un seul pixel ? Pourriez-vous tromper un modèle d’apprentissage automatique ? Su et al. (2019)3 ont montré qu’il est effectivement possible de tromper des classificateurs d’images en changeant un seul pixel.\n\n\n\nBy intentionally changing a single pixel a neural network trained on ImageNet can be deceived to predict the wrong class instead of the original class.\n\n\nSemblable aux contrefactuels, l’attaque à un pixel cherche un exemple modifié \\(x^\\prime\\) qui se rapproche de l’image originale \\(x\\), mais change la prédiction pour un résultat adverse. Cependant, la définition de la proximité diffère : seul un seul pixel peut changer. L’attaque à un pixel utilise l’évolution différentielle pour déterminer quel pixel doit être changé et comment. L’évolution différentielle s’inspire librement de l’évolution biologique des espèces. Une population d’individus appelés solutions candidates se recombinent génération après génération jusqu’à ce qu’une solution soit trouvée. Chaque solution candidate encode une modification de pixel et est représentée par un vecteur de cinq éléments : les coordonnées \\(x\\) et \\(y\\) et les valeurs des composantes rouge, vert et bleu (RVB). La recherche commence avec, par exemple, 400 solutions candidates (i.e suggestions de modification de pixel) et crée une nouvelle génération de solutions candidates (enfants) à partir de la génération parente en utilisant la formule suivante :\n\\[x_{i}(g+1)=x_{r1}(g)+F\\cdot(x_{r2}(g)-x_{r3}(g))\\]\noù chaque \\(x_i\\) est un élément d’une solution candidate (soit coordonnée \\(x\\), coordonnée \\(y\\), valeurs des compodantes rouge, vert ou bleu), \\(g\\) est la génération actuelle, \\(F\\) est un paramètre d’échelle (fixé à \\(0,5\\)) et \\(r1\\), \\(r2\\) et \\(r3\\) sont des nombres aléatoires différents. Chaque nouvelle solution candidate enfant est à son tour un pixel avec les cinq attributs pour l’emplacement et la couleur et chacun de ces attributs est un mélange de trois pixels parents aléatoires.\n\n\n\n\n\n\nNote du traducteur\n\n\n\nPossible problème de traduction du sens dans le paragraphe ci-sessus : Each new child candidate solution is in turn a pixel with the five attributes for location and color and each of those attributes is a mixture of three random parent pixels.\n\n\nLa création de solutions candidates est arrêtée si l’une des solutions candidates est un exemple adverse, c’est-à-dire qu’elle est classée dans une classe incorrecte, ou si le nombre maximum d’itérations spécifié par l’utilisateur est atteint.\n\n\n10.4.1.4 - Tout est un grille-pain : patch adversaire\nUne de mes méthodes préférées amène les exemples adverses dans la réalité physique. Brown et al. (2017)4 ont conçu une étiquette imprimable qui peut être collée à côté d’objets pour les faire ressembler à des grille-pains pour un classificateur d’images. Un travail brillant !\n\n\n\nA sticker that makes a VGG16 classifier trained on ImageNet categorize an image of a banana as a toaster. Work by Brown et al. (2017).\n\n\nCette méthode diffère des méthodes présentées jusqu’à présent pour les exemples adverses, puisque la restriction impliquant que l’image adverse doit être très proche de l’image originale est supprimée. Au lieu de cela, la méthode remplace complètement une partie de l’image par un patch qui peut prendre n’importe quelle forme. L’image du patch est optimisée sur différentes images de fond, avec différentes positions du patch sur les images, parfois déplacé, parfois agrandi ou réduit et tourné, de sorte que le patch fonctionne dans de nombreuses situations. À la fin, cette image optimisée peut être imprimée et utilisée pour tromper les classificateurs d’images en situation réelle.\n\n\n10.4.1.5 - N’apportez jamais une tortue imprimée en 3D à un combat de pistolets – même si votre ordinateur pense que c’est une bonne idée : exemples adverses robustes\nLa méthode suivante ajoute littéralement une autre dimension au grille-pain : Athalye et al. (2017)5 ont imprimé en 3D une tortue conçue pour ressembler à un fusil pour un réseau neuronal profond sous presque tous les angles possibles. Oui, vous avez bien lu. Un objet physique qui ressemble à une tortue pour les humains ressemble à un fusil pour l’ordinateur !\n\n\n\nAthalye et al. (2017) created a 3D-printed that is recognized as a rifle by TensorFlow’s standard pre-trained InceptionV3 classifier.\n\n\nLes auteurs ont trouvé un moyen de créer un exemple adverse en 3D pour un classificateur 2D qui est adverse sur des transformations, telles que toutes les possibilités de faire tourner la tortue, de zoomer, etc. D’autres approches telles que la méthode du gradient rapide ne fonctionnent plus lorsque l’image est tournée ou que l’angle de vue change. Athalye et al. (2017)^turtle] proposent l’algorithme Expectation Over Transformation (EOT), qui est une méthode pour générer des exemples adverses qui fonctionnent même lorsque l’image est transformée. L’idée principale derrière EOT est d’optimiser des exemples adverses sur de nombreuses transformations possibles. Au lieu de minimiser la distance entre l’exemple adverse et l’image originale, EOT maintient la distance attendue entre les deux en dessous d’un certain seuil, étant donné une distribution sélectionnée de transformations possibles. La distance attendue sous transformation peut s’écrire comme :\n\\[\\mathbb{E}_{t\\sim{}T}[d(t(x^\\prime),t(x))]\\]\noù \\(x\\) est l’image originale, \\(t(x)\\) l’image transformée (par exemple, tournée), \\(x'\\) l’exemple adverse et \\(t(x^\\prime)\\) sa version transformée. Outre le travail avec une distribution de transformations, la méthode EOT suit le schéma familier de cadrer la recherche d’exemples adverses comme un problème d’optimisation. Nous essayons de trouver un exemple adverse \\(x^\\prime\\) qui maximise la probabilité pour la classe sélectionnée \\(y_t\\) (par exemple, “fusil”) à travers la distribution de transformations possibles \\(T\\) :\n\\[\\arg\\max_{x^\\prime}\\mathbb{E}_{t\\sim{}T}[log{}P(y_t|t(x^\\prime))]\\]\nAvec la contrainte que la distance attendue sur toutes les transformations possibles entre l’exemple adverse \\(x^\\prime\\) et l’image originale \\(x\\) reste en dessous d’un certain seuil :\n\\[\\mathbb{E}_{t\\sim{}T}[d(t(x^\\prime),t(x))]&lt;\\epsilon\\quad\\text{and}\\quad{}x\\in[0,1]^d\\]\nJe pense que nous devrions être préoccupés par les possibilités que cette méthode permet. Les autres méthodes sont basées sur la manipulation d’images numériques. Cependant, ces exemples adverses robustes imprimés en 3D peuvent être insérés dans n’importe quelle scène réelle et tromper un ordinateur pour mal classer un objet. Retournons la situation : et si quelqu’un crée un fusil qui ressemble à une tortue ?\n\n\n10.4.1.6 - L’adversaire aux yeux bandés : attaque de la boîte noire\nImaginez le scénario suivant : Je vous donne accès à mon excellent classificateur d’images via une API Web. Vous pouvez obtenir des prédictions du modèle, mais vous n’avez pas accès aux paramètres du modèle. Depuis le confort de votre canapé, vous pouvez envoyer des données et mon service répond avec les classifications correspondantes. La plupart des attaques adverses ne sont pas conçues pour fonctionner dans ce scénario car elles nécessitent un accès au gradient du réseau neuronal profond sous-jacent pour trouver des exemples adverses. Papernot et ses collègues (2017)6 ont montré qu’il est possible de créer des exemples adverses sans informations internes sur le modèle et sans accès aux données d’entraînement. Ce type d’attaque (presque) sans connaissance préalable est appelé attaque de boîte noire.\nComment ça fonctionne :\n\nCommencez avec quelques images qui proviennent du même domaine que les données d’entraînement, par exemple, si le classificateur à attaquer est un classificateur de chiffres, utilisez des images de chiffres. La connaissance du domaine est requise, mais pas l’accès aux données d’entraînement.\nObtenez des prédictions pour l’ensemble actuel d’images de la boîte noire.\nEntraînez un modèle substitut sur l’ensemble actuel d’images (par exemple, un réseau neuronal).\nCréez un nouvel ensemble d’images synthétiques en utilisant une heuristique qui examine, pour l’ensemble actuel d’images, dans quelle direction manipuler les pixels pour que la sortie du modèle ait plus de variance.\nRépétez les étapes 2 à 4 pour un nombre prédéfini d’époques.\nCréez des exemples adverses pour le modèle substitut en utilisant la méthode du gradient rapide (ou similaire).\nAttaquez le modèle original avec des exemples adverses.\n\nLe but du modèle substitut est d’approximer les frontières de décision du modèle de la boîte noire, mais pas nécessairement d’atteindre la même précision.\nLes auteurs ont testé cette approche en attaquant des classificateurs d’images formés sur divers services en ligne d’apprentissage automatique. Ces services forment des classificateurs d’images sur des images et des étiquettes téléchargées par les utilisateurs. Le logiciel entraine automatiquement le modèle – parfois avec un algorithme inconnu de l’utilisateur – et le déploie. Le classificateur donne alors des prédictions pour les images téléchargées, mais le modèle lui-même ne peut être inspecté ou téléchargé. Les auteurs ont pu trouver des exemples adverses pour divers fournisseurs, avec jusqu’à 84% des exemples adverses mal classés.\nLa méthode fonctionne même si le modèle “boîte noire” à tromper n’est pas un réseau neuronal. Cela inclut des modèles d’apprentissage automatique sans gradient tels que des arbres de décision.\n\n\n\n10.4.2 La perspective de la cybersécurité\nL’apprentissage automatique traite des inconnus connus : la prédiction de points de données inconnus à partir d’une distribution connue. La défense contre les attaques traite des inconnus inconnus : prédire de manière robuste des points de données inconnus à partir d’une distribution inconnue d’entrées adverses. À mesure que l’apprentissage automatique est intégré dans de plus en plus de systèmes, tels que les véhicules autonomes ou les dispositifs médicaux, ils deviennent également des points d’entrée pour les attaques. Même si les prédictions d’un modèle d’apprentissage automatique sur un ensemble de test sont correctes à 100 %, des exemples adverses peuvent être trouvés pour tromper le modèle. La défense des modèles d’apprentissage automatique contre les cyberattaques est une nouvelle partie du domaine de la cybersécurité.\nBiggio et al. (2018)7 donnent un bon aperçu de dix années de recherche sur l’apprentissage automatique adversaire, sur lesquelles cette section est basée. La cybersécurité est une course aux armements dans laquelle les attaquants et les défenseurs se surpassent à maintes reprises.\nIl existe trois règles d’or en cybersécurité : 1) connaître son adversaire, 2) être proactif et 3) se protéger.\nDifférentes applications ont différents adversaires. Les personnes qui tentent de frauder d’autres personnes par courrier électronique pour voler leur argent sont des agents adversaires des utilisateurs et des fournisseurs de services de messagerie. Les fournisseurs veulent protéger leurs utilisateurs, afin qu’ils puissent continuer à utiliser leur programme de messagerie, les attaquants veulent amener les gens à leur donner de l’argent. Connaître ses adversaires signifie connaître leurs objectifs. En supposant que vous ne savez pas que ces spammeurs existent et que le seul abus du service de messagerie est l’envoi de copies piratées de musique, alors la défense serait différente (par exemple, analyser les pièces jointes pour du matériel protégé par le droit d’auteur au lieu d’analyser le texte pour des indicateurs de spam).\nÊtre proactif signifie tester activement et identifier les points faibles du système. Vous êtes proactif lorsque vous essayez activement de tromper le modèle avec des exemples adverses, puis vous défendez contre eux. Utiliser des méthodes d’interprétation pour comprendre quelles caractéristiques sont importantes et comment les caractéristiques affectent la prédiction est également une étape proactive pour comprendre les faiblesses d’un modèle d’apprentissage automatique. En tant que data scientist, faites-vous confiance à votre modèle dans ce monde dangereux sans jamais avoir regardé au-delà de la puissance prédictive sur un ensemble de test ? Avez-vous analysé comment le modèle se comporte dans différents scénarios, identifié les entrées les plus importantes, vérifié les explications des prédictions pour certains exemples ? Avez-vous essayé de trouver des entrées adverses ? L’interprétabilité des modèles d’apprentissage automatique joue un rôle majeur en cybersécurité. Être réactif, l’opposé de proactif, signifie attendre que le système ait été attaqué et seulement alors comprendre le problème et installer des mesures de défense.\nComment pouvons-nous protéger nos systèmes d’apprentissage automatique contre les exemples adverses ? Une approche proactive est la reformation itérative du classificateur avec des exemples adverses, également appelée entraînement adverse. D’autres approches sont basées sur la théorie des jeux, telles que l’apprentissage de transformations invariantes des caractéristiques ou l’optimisation robuste (régularisation). Une autre méthode proposée est d’utiliser plusieurs classificateurs au lieu d’un seul et de les faire voter pour la prédiction (ensemble), mais cela n’a aucune garantie de fonctionner, car ils pourraient tous souffrir de similaires exemples adverses. Une autre approche qui ne fonctionne pas bien non plus est le masquage du gradient, qui construit un modèle sans gradients utiles en utilisant un classificateur du plus proche voisin au lieu du modèle original.\nNous pouvons distinguer les types d’attaques en fonction de la connaissance qu’a l’attaquant du système.\n\nLes attaquants peuvent avoir une connaissance parfaite (attaque de boîte blanche), ce qui signifie qu’ils savent tout sur le modèle, comme le type de modèle, les paramètres et les données d’entraînement ;\nLes attaquants peuvent avoir une connaissance partielle (attaque de boîte grise), ce qui signifie qu’ils pourraient seulement connaître la représentation des caractéristiques et le type de modèle utilisé, mais n’ont pas accès aux données d’entraînement ou aux paramètres ;\nLes attaquants peuvent n’avoir aucune connaissance (attaque de boîte noire), ce qui signifie qu’ils ne peuvent interroger le modèle que de manière boîte noire, et n’ont pas accès aux données d’entraînement ni aux informations sur les paramètres du modèle. Selon le niveau d’information, les attaquants peuvent utiliser différentes techniques pour attaquer le modèle.\n\nComme nous l’avons vu dans les exemples, même dans le cas de la boîte noire, des exemples adverses peuvent être créés, de sorte que cacher des informations sur les données et le modèle n’est pas suffisant pour se protéger contre les attaques.\nÉtant donné la nature du jeu du chat et de la souris entre les attaquants et les défenseurs, nous assisterons à beaucoup de développements et d’innovations dans ce domaine. Pensez juste aux nombreux types différents de courriels indésirables qui évoluent constamment. De nouvelles méthodes d’attaques contre les modèles d’apprentissage automatique sont inventées et de nouvelles mesures défensives sont proposées contre ces nouvelles attaques. Des attaques plus puissantes sont développées pour éviter les dernières défenses et ainsi de suite, ad infinitum. Avec ce chapitre, j’espère vous avoir sensibilisé au problème des exemples adverses et que seulement en étudiant de manière proactive les modèles d’apprentissage automatique nous serons capables de découvrir et de remédier à leurs faiblesses.",
    "crumbs": [
      "10 - Interprétation d'un réseau de neurone",
      "10.4 - Exemples adverses"
    ]
  },
  {
    "objectID": "10-neuralnet/10.4-adversarial-examples.html#footnotes",
    "href": "10-neuralnet/10.4-adversarial-examples.html#footnotes",
    "title": "Apprentissage automatique interprétable",
    "section": "Notes de bas de page",
    "text": "Notes de bas de page\n\n\nSzegedy, Christian, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow, and Rob Fergus. “Intriguing properties of neural networks.” arXiv preprint arXiv:1312.6199 (2013).↩︎\nGoodfellow, Ian J., Jonathon Shlens, and Christian Szegedy. “Explaining and harnessing adversarial examples.” arXiv preprint arXiv:1412.6572 (2014).↩︎\nSu, Jiawei, Danilo Vasconcellos Vargas, and Kouichi Sakurai. “One pixel attack for fooling deep neural networks.” IEEE Transactions on Evolutionary Computation (2019).↩︎\nBrown, Tom B., Dandelion Mané, Aurko Roy, Martín Abadi, and Justin Gilmer. “Adversarial patch.” arXiv preprint arXiv:1712.09665 (2017).↩︎\nAthalye, Anish, and Ilya Sutskever. “Synthesizing robust adversarial examples.” arXiv preprint arXiv:1707.07397 (2017).↩︎\nPapernot, Nicolas, et al. “Practical black-box attacks against machine learning.” Proceedings of the 2017 ACM on Asia Conference on Computer and Communications Security. ACM (2017).↩︎\nBiggio, Battista, and Fabio Roli. “Wild Patterns: Ten years after the rise of adversarial machine learning.” Pattern Recognition 84 (2018): 317-331.↩︎",
    "crumbs": [
      "10 - Interprétation d'un réseau de neurone",
      "10.4 - Exemples adverses"
    ]
  },
  {
    "objectID": "10-neuralnet/10.2-pixel-attribution.html",
    "href": "10-neuralnet/10.2-pixel-attribution.html",
    "title": "Apprentissage automatique interprétable",
    "section": "",
    "text": "Avertissement\n\n\n\nEn cours de traduction.",
    "crumbs": [
      "10 - Interprétation d'un réseau de neurone",
      "10.2 - Attribution de pixel"
    ]
  },
  {
    "objectID": "10-neuralnet/10.2-pixel-attribution.html#attribution-de-pixel",
    "href": "10-neuralnet/10.2-pixel-attribution.html#attribution-de-pixel",
    "title": "Apprentissage automatique interprétable",
    "section": "10.2 - Attribution de pixel",
    "text": "10.2 - Attribution de pixel\n\nLes méthodes d’attribution de pixels mettent en évidence les pixels qui étaient pertinents pour une certaine classification d’image par un réseau neuronal. L’image suivante est un exemple d’explication :\n\n\n\nA saliency map in which pixels are colored by their contribution to the classification.\n\n\nLes méthodes d’attribution de pixels mettent en lumière les pixels qui ont été pertinents pour une certaine classification d’image effectuée par un réseau neuronal. Vous verrez plus tard dans ce chapitre ce qui se passe dans cette image particulière. Les méthodes d’attribution de pixels peuvent être trouvées sous divers noms : carte de sensibilité, carte de saillance, carte d’attribution de pixels, méthodes d’attribution basées sur les gradients, pertinence des caractéristiques, attribution des caractéristiques et contribution des caractéristiques.\nL’attribution de pixels est un cas particulier de l’attribution de caractéristiques, mais pour les images. L’attribution de caractéristiques explique les prédictions individuelles en attribuant chaque caractéristique d’entrée selon la mesure dans laquelle elle a changé la prédiction (négativement ou positivement). Les caractéristiques peuvent être des pixels d’entrée, des données tabulaires ou des mots. SHAP, les valeurs de Shapley et LIME sont des exemples de méthodes d’attribution de caractéristiques générales.\nNous considérons les réseaux neuronaux qui produisent en sortie un vecteur de longueur \\(C\\), ce qui inclut la régression où \\(C=1\\). La sortie du réseau neuronal pour l’image \\(I\\) est appelée \\(S(I)=[S_1(I),\\ldots,S_C(I)]\\). Toutes ces méthodes prennent en entrée \\(x\\in\\mathbb{R}^p\\) (peuvent être des pixels d’image, des données tabulaires, des mots, …) avec \\(p\\) caractéristiques et produisent en sortie un score de pertinence pour chacune des \\(p\\) caractéristiques d’entrée : \\(R^c=[R_1^c,\\ldots,R_p^c]\\). Le \\(c\\) indique la pertinence pour la \\(c^{ième}\\) sortie \\(S_C(I)\\).\nIl existe une quantité déroutante d’approches d’attribution de pixels. Il est utile de comprendre qu’il existe deux types différents de méthodes d’attribution :\nBasées sur l’occlusion ou la perturbation : Des méthodes comme SHAP et LIME manipulent des parties de l’image pour générer des explications (agnostiques au modèle).\nBasées sur le gradient : De nombreuses méthodes calculent le gradient de la prédiction (ou du score de classification) par rapport aux caractéristiques d’entrée. Les méthodes basées sur le gradient (dont il en existe beaucoup) diffèrent principalement dans la manière dont le gradient est calculé.\nLes deux approches ont en commun que l’explication a la même taille que l’image d’entrée (ou peut au moins être projetée de manière significative sur celle-ci) et elles attribuent à chaque pixel une valeur qui peut être interprétée comme la pertinence du pixel pour la prédiction ou la classification de cette image.\nUne autre catégorisation utile pour les méthodes d’attribution de pixels est la question de base :\nLes méthodes basées uniquement sur le gradient nous indiquent si un changement dans un pixel changerait la prédiction. Des exemples sont le Gradient Vanille et Grad-CAM. L’interprétation de l’attribution basée uniquement sur le gradient est : si j’augmentais les valeurs de couleur du pixel, la probabilité de la classe prédite augmenterait (pour un gradient positif) ou diminuerait (pour un gradient négatif). Plus la valeur absolue du gradient est grande, plus l’effet d’un changement de ce pixel est fort.\nLes méthodes d’attribution de chemin comparent l’image actuelle à une image de référence, qui peut être une image “zéro” artificielle comme une image complètement grise. La différence entre la prédiction actuelle et la prédiction de base est répartie parmi les pixels. L’image de référence peut également être multiple : une distribution d’images. Cette catégorie comprend des méthodes basées sur le gradient spécifiques au modèle comme Deep Taylor et Integrated Gradients, ainsi que des méthodes agnostiques au modèle telles que LIME et SHAP. Certaines méthodes d’attribution de chemin sont “complètes”, ce qui signifie que la somme des scores de pertinence pour toutes les caractéristiques d’entrée est la différence entre la prédiction de l’image et la prédiction d’une image de référence. Des exemples sont SHAP et Integrated Gradients. Pour les méthodes d’attribution de chemin, l’interprétation se fait toujours par rapport à la référence : la différence entre les scores de classification de l’image actuelle et de l’image de référence est attribuée aux pixels. Le choix de l’image de référence (distribution) a un grand effet sur l’explication. L’hypothèse habituelle est d’utiliser une image (distribution) “neutre”. Bien sûr, il est parfaitement possible d’utiliser votre selfie préféré, mais vous devriez vous demander si cela a du sens dans une application. Cela affirmerait certainement la domination parmi les autres membres du projet.\nÀ ce stade, je donnerais normalement une explication intuitive de la manière dont ces méthodes fonctionnent, mais je pense qu’il est préférable de commencer directement par la méthode du Gradient Vanille, car elle montre très bien la recette générale que de nombreuses autres méthodes suivent.\n\n10.2.1 - Gradient Vanille (Cartes de saillance)\nL’idée du Gradient Vanille, introduite par Simonyan et al. (2013)1 en tant que l’une des premières approches d’attribution de pixels, est assez simple si vous connaissez déjà la rétropropagation. (Ils ont appelé leur approche “Saliency Image-Specific Class”, mais je préfère Gradient Vanille). Nous calculons le gradient de la fonction de perte pour la classe qui nous intéresse par rapport aux pixels d’entrée. Cela nous donne une carte de la taille des caractéristiques d’entrée avec des valeurs négatives à positives.\nLa recette pour cette approche est :\n\nEffectuer un passage en avant de l’image d’intérêt.\nCalculer le gradient du score de classe d’intérêt par rapport aux pixels d’entrée : \\[E_{grad}(I_0)=\\frac{\\delta{}S_c}{\\delta{}I}|_{I=I_0}\\] Ici, nous fixons toutes les autres classes à zéro.\nVisualiser les gradients. Vous pouvez soit montrer les valeurs absolues, soit mettre en évidence séparément les contributions négatives et positives.\n\nPlus formellement, nous avons une image I et le réseau neuronal convolutionnel lui donne un score \\(S_c(I)\\) pour la classe c. Le score est une fonction hautement non linéaire de notre image. L’idée derrière l’utilisation du gradient est que nous pouvons approximer ce score en appliquant une expansion de Taylor du premier ordre\n\\[S_c(I)\\approx{}w^T{}I+b\\]\noù w est la dérivée de notre score :\n\\[w = \\frac{\\delta S_C}{\\delta I}|_{I_0}\\]\nMaintenant, il y a une certaine ambiguïté sur la manière d’effectuer un passage en arrière des gradients, car des unités non linéaires telles que ReLU (Rectifying Linear Unit) “suppriment” le signe. Donc, lorsque nous faisons un passage en arrière, nous ne savons pas s’il faut attribuer une activation positive ou négative. Utilisant mes incroyables compétences en art ASCII, la fonction ReLU ressemble à ceci : _/ et est définie comme \\(X_{n+1}(x)=max(0,X_n)\\) de la couche \\(X_n\\) à la couche \\(X_{n-1}\\). Cela signifie que lorsque l’activation d’un neurone est nulle, nous ne savons pas quelle valeur rétropropager. Dans le cas du Gradient Vanille, l’ambiguïté est résolue comme suit :\n\\[\\frac{\\delta f}{\\delta X_n} = \\frac{\\delta f}{\\delta X_{n+1}} \\cdot \\mathbf{I}(X_n &gt; 0)\\]\nIci, \\(\\mathbf{I}\\) est la fonction indicatrice élément par élément, qui est zéro là où l’activation à la couche inférieure était négative, et un où elle est positive ou nulle. Le Gradient Vanille prend le gradient que nous avons rétropropagé jusqu’à présent jusqu’à la couche \\(n+1\\), puis fixe simplement les gradients à zéro où l’activation à la couche en dessous est négative.\nRegardons un exemple où nous avons les couches \\(X_n\\) et \\(X_{n+1}=\\text{ReLU}(X_{n+1})\\). Notre activation fictive à \\(X_n\\) est :\n\\[\n\\begin{pmatrix}\n1 & 0 \\\\\n-1 & -10 \\\\\n\\end{pmatrix}\n\\]\nEt voici nos gradients à \\(X_{(n+1)}\\) :\n\\[\n\\begin{pmatrix}\n0,4 & 1,1 \\\\\n-0,5 & -0,1  \\\\\n\\end{pmatrix}\n\\]\nAlors nos gradients à \\(X_n\\) sont :\n\\[\n\\begin{pmatrix}\n0,4 & 0 \\\\\n0 & 0  \\\\\n\\end{pmatrix}\n\\]\n\n10.2.1.1 - Les problèmes avec Vanilla Gradient\nLe Gradient Vanille a un problème de saturation, comme expliqué dans Avanti et al. (2017)2. Lorsque ReLU est utilisé, et lorsque l’activation passe en dessous de zéro, alors l’activation est plafonnée à zéro et ne change plus. L’activation est saturée. Par exemple : l’entrée de la couche est deux neurones avec des poids \\(-1\\) et \\(-1\\) et un biais de \\(1\\). Lors du passage à travers la couche ReLU, l’activation sera \\(neuron_1\\) + \\(neuron_2\\) si la somme des deux neurones est inférieure strictement à \\(1\\).Si la somme des deux est supérieure à \\(1\\), l’activation restera saturée à une activation de \\(1\\). Aussi, le gradient à ce point sera zéro, et le Gradient Vanille dira que ce neurone n’est pas important.\n\n\n\n\n\n\nNote du traducteur\n\n\n\nVérifier et illustrer l’exemple précédent\n\n\nEt maintenant, mes chers lecteurs, apprenez une autre méthode, plus ou moins gratuitement : DeconvNet.\n\n\n\n10.2.2 - DeconvNet\nDeconvNet par Zeiler et Fergus (2014)3 est presque identique au Gradient Vanille. L’objectif de DeconvNet est d’inverser un réseau neuronal et l’article propose des opérations qui sont des inverses des couches de filtrage, de pooling et d’activation. Si vous regardez dans l’article, cela semble très différent du Gradient Vanille, mais à part l’inversion de la couche ReLU, DeconvNet est équivalent à l’approche du Gradient Vanille. Le Gradient Vanille peut être considéré comme une généralisation de DeconvNet. DeconvNet fait un choix différent pour rétropropager le gradient à travers ReLU :\n\\[R_n = R_{n+1}\\mathbb{I}(R_{n+1} &gt; 0)\\]\noù \\(R_n\\) et \\(R_{n+1}\\) sont les reconstructions des couches et \\(\\mathbb{I}\\) la fonction indicatrice. Lors de la rétropropagation de la couche \\(n\\) à la couche \\(n-1\\), DeconvNet “se souvient” des activations dans la couche \\(n\\) qui ont été mises à zéro lors du passage en avant et les met à zéro dans la couche \\(n-1\\). Les activations ayant une valeur négative dans la couche \\(n\\) sont mises à zéro dans la couche \\(n-1\\). Le gradient \\(X_n\\) pour l’exemple précédent devient :\n\n\n\n\n\n\nNote du traducteur\n\n\n\nVérifier et illustrer l’exemple précédent\n\n\n\\[\n\\begin{pmatrix}\n0,4 & 1,1 \\\\\n0 & 0  \\\\\n\\end{pmatrix}\n\\]\n\n\n10.2.3 - Grad-CAM\nGrad-CAM fournit des explications visuelles pour les décisions prises par les CNN (réseaux de neurones convolutionnels). Contrairement à d’autres méthodes, le gradient n’est pas rétropropagé jusqu’à l’image, mais (généralement) jusqu’à la dernière couche convolutionnelle pour produire une carte de localisation grossière qui met en évidence les régions importantes de l’image.\nGrad-CAM signifie Gradient-weighted Class Activation Map (Carte d’Activation de Classe Pondérée par Gradient). Et, comme son nom l’indique, il est basé sur le gradient des réseaux neuronaux. Grad-CAM, comme d’autres techniques, attribue à chaque neurone un score de pertinence pour la décision d’intérêt. Cette décision d’intérêt peut être la prédiction de classe (que nous trouvons dans la couche de sortie), mais peut théoriquement être n’importe quelle autre couche dans le réseau neuronal. Grad-CAM rétropropage cette information à la dernière couche convolutionnelle. Grad-CAM peut être utilisé avec différents CNNs : avec des couches entièrement connectées, pour des sorties structurées telles que le sous-titrage et dans des sorties multi-tâches, et pour l’apprentissage par renforcement.\nCommençons par une considération intuitive de Grad-CAM. L’objectif de Grad-CAM est de comprendre sur quelles parties d’une image une couche convolutionnelle “regarde” pour une certaine classification. Pour rappel, la première couche convolutionnelle d’un CNN prend en entrée les images et produit des cartes de caractéristiques qui encodent les caractéristiques apprises (voir la section sur les caractéristiques apprises). Les couches convolutionnelles de niveau supérieur font de même, mais prennent en entrée les cartes de caractéristiques des couches convolutionnelles précédentes. Pour comprendre comment le CNN prend des décisions, Grad-CAM analyse quelles régions sont activées dans les cartes de caractéristiques des dernières couches convolutionnelles. Il y a \\(k\\) cartes de caractéristiques dans la dernière couche convolutionnelle, et je vais les appeler \\(A_1, A_2, \\ldots, A_k\\). Comment pouvons-nous “voir” à partir des cartes de caractéristiques comment le réseau neuronal convolutionnel a effectué une certaine classification ? Dans la première approche, nous pourrions simplement visualiser les valeurs brutes de chaque carte de caractéristiques, faire une moyenne sur les cartes de caractéristiques et superposer cela sur notre image. Cela ne serait pas utile car les cartes de caractéristiques encodent des informations pour toutes les classes, mais nous sommes intéressés par une classe particulière. Grad-CAM doit décider de l’importance de chacune des \\(k\\) cartes de caractéristiques pour notre classe c qui nous intéresse. Nous devons pondérer chaque pixel de chaque carte de caractéristiques avec le gradient avant de faire une moyenne sur les cartes de caractéristiques. Cela nous donne une carte thermique qui met en évidence les régions qui affectent positivement ou négativement la classe d’intérêt. Cette carte thermique est passée à travers la fonction ReLU, ce qui est une façon élégante de dire que nous fixons toutes les valeurs négatives à zéro. Grad-CAM élimine toutes les valeurs négatives en utilisant une fonction ReLU, avec l’argument que nous sommes uniquement intéressés par les parties qui contribuent à la classe sélectionnée c et non à d’autres classes. Le mot pixel pourrait être trompeur ici car la carte de caractéristiques est plus petite que l’image (à cause des unités de pooling) mais est mappée sur l’image originale. Nous échelonnons ensuite la carte Grad-CAM à l’intervalle \\([0,1]\\) à des fins de visualisation et la superposons sur l’image originale.\nRegardons la recette pour Grad-CAM. Notre objectif est de trouver la carte de localisation, qui est définie comme suit :\n\\[L^c_{Grad-CAM} \\in \\mathbb{R}^{u\\times v} = \\underbrace{\\text{ReLU}}_{\\text{Choisir valeurs positives}}\\left(\\sum_{k} \\alpha_k^c A^k\\right)\\]\nIci, \\(u\\) est la largeur, \\(v\\) la hauteur de l’explication et \\(c\\) la classe d’intérêt.\n\nPropager en avant l’image d’entrée à travers le réseau neuronal convolutionnel.\nObtenir le score brut pour la classe d’intérêt, c’est-à-dire l’activation du neurone avant la couche softmax.\nDéfinir toutes les autres activations de classe à zéro.\nRétropropager le gradient de la classe d’intérêt vers la dernière couche convolutionnelle avant les couches entièrement connectées : ().\nPonderer chaque “pixel” de la carte de caractéristiques par le gradient pour la classe. Les indices i et j se réfèrent aux dimensions de largeur et de hauteur : \\[\\alpha_k^c = \\overbrace{\\frac{1}{Z}\\sum_{i}\\sum_{j}}^{\\text{pooling moyen global}} \\underbrace{\\frac{\\delta y^c}{\\delta A_{ij}^k}}_{\\text{gradients via rétropropagation}}\\] Cela signifie que les gradients sont mis en commun de manière globale.\nCalculer une moyenne des cartes de caractéristiques, pondérée par pixel par le gradient.\nAppliquer ReLU à la carte de caractéristiques moyennée.\nPour la visualisation : Échelonner les valeurs à l’intervalle entre \\(0\\) et \\(1\\). Agrandir l’image et la superposer sur l’image originale.\nÉtape supplémentaire pour Guided Grad-CAM : Multiplier la carte thermique avec la rétropropagation guidée.\n\n\n\n10.2.4 - Guided Grad-CAM\nD’après la description de Grad-CAM, vous pouvez deviner que la localisation est très grossière, car les dernières cartes de caractéristiques convolutionnelles ont une résolution beaucoup plus grossière par rapport à l’image d’entrée. En revanche, d’autres techniques d’attribution rétropropagent jusqu’aux pixels d’entrée. Elles sont donc beaucoup plus détaillées et peuvent vous montrer des bords ou des points individuels qui ont le plus contribué à une prédiction. Une fusion de ces deux méthodes est appelée Guided Grad-CAM. Et c’est super simple. Vous calculez pour une image à la fois l’explication Grad-CAM et l’explication d’une autre méthode d’attribution, telle que le Gradient Vanille. La sortie Grad-CAM est ensuite agrandie avec une interpolation bilinéaire, puis les deux cartes sont multipliées élément par élément. Grad-CAM fonctionne comme une lentille qui se concentre sur des parties spécifiques de la carte d’attribution pixel par pixel.\n\n\n10.2.5 - SmoothGrad\nL’idée de SmoothGrad par Smilkov et al. (2017)4 est de rendre les explications basées sur les gradients moins bruyantes en ajoutant du bruit et en moyennant ces gradients artificiellement bruyants. SmoothGrad n’est pas une méthode d’explication autonome, mais une extension à toute méthode d’explication basée sur les gradients.\nSmoothGrad fonctionne de la manière suivante :\n\nGénérer plusieurs versions de l’image d’intérêt en y ajoutant du bruit.\nCréer des cartes d’attribution de pixels pour toutes les images.\nMoyenner les cartes d’attribution de pixels.\n\nOui, c’est aussi simple que ça. Pourquoi cela devrait-il fonctionner ? La théorie est que la dérivée fluctue grandement à petite échelle. Les réseaux neuronaux n’ont aucune incitation, lors de l’entraînement, à maintenir les gradients lisses, leur objectif est de classer correctement les images. Moyenner plusieurs cartes “lisse” ces fluctuations :\n\\[R_{sg}(x) = \\frac{1}{N} \\sum_{i=1}^n R(x + g_i)\\]\nIci, \\(g_i \\sim N(0,\\sigma^2)\\) sont des vecteurs de bruit échantillonnés à partir de la distribution gaussienne. Le niveau de bruit “idéal” dépend de l’image d’entrée et du réseau. Les auteurs suggèrent un niveau de bruit de \\(10%\\) à \\(20%\\), ce qui signifie que \\(\\frac{\\sigma}{x_{max} - x_{min}}\\) devrait être entre \\(0,1\\) et \\(0,2\\). Les limites \\(x_{min}\\) et \\(x_{max}\\) se réfèrent aux valeurs de pixels minimales et maximales de l’image. L’autre paramètre est le nombre d’échantillons n, pour lequel il a été suggéré d’utiliser \\(n = 50\\), car il y a des rendements décroissants au-delà de cela.\n\n\n10.2.6 - Exemples\nVoyons quelques exemples de l’apparence de ces cartes et comment les méthodes se comparent qualitativement. Le réseau examiné est VGG-16 (Simonyan et al. 20145) qui a été entraîné sur ImageNet et peut donc distinguer plus de 20 000 classes. Pour les images suivantes, nous créerons des explications pour la classe ayant le score de classification le plus élevé.\nVoici les images et leur classification par le réseau neuronal :\n\n\n\nImages of a dog classified as greyhound, a ramen soup classified as soup bowl, and an octopus classified as eel.\n\n\nL’image à gauche avec le chien honorable gardant le livre “Interpretable Machine Learning” a été classifiée comme “Lévrier” avec une probabilité de \\(35%\\) (il semble que “Livre Interpretable Machine Learning” n’était pas l’une des \\(20 000\\) classes). L’image au milieu montre un bol de délicieuse soupe de ramen et est correctement classifiée comme “Bol de soupe” avec une probabilité de \\(50%\\). La troisième image montre un poulpe sur le fond de l’océan, qui est incorrectement classifié comme “Anguille” avec une forte probabilité de \\(70%\\).\nEt voici les attributions de pixels qui visent à expliquer la classification :\n\n\n\nPixel attributions or saliency maps for the Vanilla Gradient method, SmoothGrad and Grad-CAM.\n\n\nMalheureusement, c’est un peu confus. Mais examinons les explications individuelles, en commençant par le chien. Le Gradient Vanille et le Gradient Vanille + SmoothGrad mettent tous deux en évidence le chien, ce qui a du sens. Mais ils mettent aussi en évidence certaines zones autour du livre, ce qui est étrange. Grad-CAM met en évidence uniquement la zone du livre, ce qui n’a aucun sens. Et à partir de là, cela devient un peu plus compliqué. La méthode du Gradient Vanille semble échouer pour le bol de soupe et le poulpe (ou, comme le réseau le pense, anguille). Les deux images ressemblent à des images résiduelles de regarder le soleil trop longtemps. (S’il vous plaît, ne regardez pas directement le soleil). SmoothGrad aide beaucoup, au moins les zones sont plus définies. Dans l’exemple de la soupe, certains ingrédients sont mis en évidence, tels que les œufs et la viande, mais aussi la zone autour des baguettes. Dans l’image du poulpe, l’animal lui-même est principalement mis en évidence. Pour le bol de soupe, Grad-CAM met en évidence la partie œuf et, pour une raison quelconque, la partie supérieure du bol. Les explications du poulpe par Grad-CAM sont encore plus désordonnées.\nVous pouvez déjà voir ici les difficultés à évaluer si nous faisons confiance aux explications. Comme première étape, nous devons considérer quelles parties de l’image contiennent des informations pertinentes pour la classification de l’image. Mais ensuite, nous devons aussi réfléchir à ce que le réseau neuronal a pu utiliser pour la classification. Peut-être que le bol de soupe a été correctement classifié en fonction de la combinaison des œufs et des baguettes, comme le suggère SmoothGrad ? Ou peut-être que le réseau neuronal a reconnu la forme du bol plus quelques ingrédients, comme le suggère Grad-CAM ? Nous ne savons tout simplement pas.\nEt c’est là le grand problème avec toutes ces méthodes. Nous n’avons pas de vérité absolue pour les explications. Nous ne pouvons que, dans un premier temps, rejeter les explications qui n’ont manifestement aucun sens (et même dans cette étape, nous n’avons pas une grande confiance. Le processus de prédiction dans le réseau neuronal est très compliqué).\n\n\n10.2.7 - Avantages\nLes explications sont visuelles et nous sommes rapides à reconnaître les images. En particulier, lorsque les méthodes mettent en évidence uniquement les pixels importants, il est facile de reconnaître immédiatement les régions importantes de l’image.\nLes méthodes basées sur les gradients sont généralement plus rapides à calculer que les méthodes agnostiques au modèle. Par exemple, LIME et SHAP peuvent également être utilisés pour expliquer les classifications d’images, mais sont plus coûteux à calculer.\nIl existe de nombreuses méthodes parmi lesquelles choisir.\n\n\n10.2.8 - Inconvénients\nComme pour la plupart des méthodes d’interprétation, il est difficile de savoir si une explication est correcte, et une grande partie de l’évaluation est uniquement qualitative (« Ces explications semblent correctes, publions l’article tout de suite »).\nLes méthodes d’attribution de pixels peuvent être très fragiles. Ghorbani et al. (2019)6 ont montré qu’introduire de petites perturbations (adverses) dans une image, qui conduisent toujours à la même prédiction, peut entraîner la mise en évidence de pixels très différents comme explications.\nKindermans et al. (2019)7 ont également montré que ces méthodes d’attribution de pixels peuvent être hautement peu fiables. Ils ont ajouté un décalage constant aux données d’entrée, c’est-à-dire qu’ils ont ajouté les mêmes changements de pixels à toutes les images. Ils ont comparé deux réseaux, le réseau original et le réseau “décalé” où le biais de la première couche est modifié pour s’adapter au décalage constant des pixels. Les deux réseaux produisent les mêmes prédictions. De plus, le gradient est le même pour les deux. Mais les explications ont changé, ce qui est une propriété indésirable. Ils ont examiné DeepLift, Gradient Vanille et Gradients Intégrés.\nL’article “Sanity checks for saliency maps”8 a étudié si les méthodes de salience sont insensibles au modèle et aux données. L’insensibilité est hautement indésirable, car cela signifierait que l’“explication” est sans rapport avec le modèle et les données. Les méthodes insensibles au modèle et aux données d’entraînement sont similaires aux détecteurs de bords. Les détecteurs de bords mettent simplement en évidence des changements de couleur de pixels forts dans les images et sont sans rapport avec un modèle de prédiction ou des caractéristiques abstraites de l’image, et ne nécessitent aucune formation. Les méthodes testées étaient le Gradient Vanille, Gradient x Entrée, Gradients Intégrés, Rétropropagation Guidée, Guided Grad-CAM et SmoothGrad (avec Gradient Vanille). Le Gradient Vanille et Grad-CAM ont passé le test d’insensibilité, tandis que la Rétropropagation Guidée et Guided Grad-CAM ont échoué. Cependant, le papier de vérification de la santé lui-même a trouvé des critiques de Tomsett et al. (2020)9 avec un article appelé “Sanity checks for saliency metrics” (bien sûr). Ils ont trouvé un manque de cohérence pour les métriques d’évaluation (je sais, cela devient assez méta maintenant). Donc, nous revenons là où nous avons commencé… Il reste difficile d’évaluer les explications visuelles. Cela rend très difficile pour un praticien.\nDans l’ensemble, c’est un état de choses très insatisfaisant. Nous devons attendre un peu plus de recherche sur ce sujet. Et s’il vous plaît, pas plus d’invention de nouvelles méthodes de salience, mais plus de rigueur sur la façon de les évaluer.\n\n\n10.2.9 - Logiciels\nIl existe plusieurs implémentations logicielles de méthodes d’attribution de pixels. Pour l’exemple, j’ai utilisé tf-keras-vis. L’une des bibliothèques les plus complètes est iNNvestigate, qui implémente le Gradient Vanille, SmoothGrad, DeconvNet, la Rétropropagation Guidée, PatternNet, LRP et plus encore. Beaucoup de ces méthodes sont implémentées dans la Boîte à Outils DeepExplain.",
    "crumbs": [
      "10 - Interprétation d'un réseau de neurone",
      "10.2 - Attribution de pixel"
    ]
  },
  {
    "objectID": "10-neuralnet/10.2-pixel-attribution.html#footnotes",
    "href": "10-neuralnet/10.2-pixel-attribution.html#footnotes",
    "title": "Apprentissage automatique interprétable",
    "section": "Notes de bas de page",
    "text": "Notes de bas de page\n\n\nSimonyan, Karen, Andrea Vedaldi, and Andrew Zisserman. “Deep inside convolutional networks: Visualising image classification models and saliency maps.” arXiv preprint arXiv:1312.6034 (2013).↩︎\nShrikumar, Avanti, Peyton Greenside, and Anshul Kundaje. “Learning important features through propagating activation differences.” Proceedings of the 34th International Conference on Machine Learning-Volume 70. JMLR. org, (2017).↩︎\nZeiler, Matthew D., and Rob Fergus. “Visualizing and understanding convolutional networks.” European conference on computer vision. Springer, Cham (2014).↩︎\nSmilkov, Daniel, et al. “SmoothGrad: removing noise by adding noise.” arXiv preprint arXiv:1706.03825 (2017).↩︎\nSimonyan, Karen, and Andrew Zisserman. “Very deep convolutional networks for large-scale image recognition.” arXiv preprint arXiv:1409.1556 (2014).↩︎\nGhorbani, Amirata, Abubakar Abid, and James Zou. “Interpretation of neural networks is fragile.” Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 33. 2019.↩︎\nKindermans, Pieter-Jan, Sara Hooker, Julius Adebayo, Maximilian Alber, Kristof T. Schütt, Sven Dähne, Dumitru Erhan, and Been Kim. “The (un) reliability of saliency methods.” In Explainable AI: Interpreting, Explaining and Visualizing Deep Learning, pp. 267-280. Springer, Cham (2019).↩︎\nAdebayo, Julius, Justin Gilmer, Michael Muelly, Ian Goodfellow, Moritz Hardt, and Been Kim. “Sanity checks for saliency maps.” arXiv preprint arXiv:1810.03292 (2018).↩︎\nTomsett, Richard, Dan Harborne, Supriyo Chakraborty, Prudhvi Gurram, and Alun Preece. “Sanity checks for saliency metrics.” In Proceedings of the AAAI Conference on Artificial Intelligence, vol. 34, no. 04, pp. 6021-6029. 2020.↩︎",
    "crumbs": [
      "10 - Interprétation d'un réseau de neurone",
      "10.2 - Attribution de pixel"
    ]
  },
  {
    "objectID": "09-local_model_agnostic_methods/09.6-shap.html",
    "href": "09-local_model_agnostic_methods/09.6-shap.html",
    "title": "Apprentissage automatique interprétable",
    "section": "",
    "text": "Avertissement\n\n\n\nEn cours de traduction.",
    "crumbs": [
      "9 - Méthodes locales indépendantes du modèle",
      "9.6 - SHAP (SHapley Additive exPlanations)"
    ]
  },
  {
    "objectID": "09-local_model_agnostic_methods/09.6-shap.html#shap-shapley-additive-explanations",
    "href": "09-local_model_agnostic_methods/09.6-shap.html#shap-shapley-additive-explanations",
    "title": "Apprentissage automatique interprétable",
    "section": "9.6 - SHAP (SHapley Additive exPlanations)",
    "text": "9.6 - SHAP (SHapley Additive exPlanations)\nSHAP (SHapley Additive exPlanations) de Lundberg et Lee (2017)1 est une méthode pour expliquer les prédictions individuelles. SHAP est basé sur les valeurs de Shapley théoriquement optimales du jeu.\n\n\n\n\n\n\nNote\n\n\n\nVous recherchez un livre pratique et approfondi sur les valeurs SHAP et Shapley ? J’en ai trouvé un pour vous.\n\n\nIl y a deux raisons pour lesquelles SHAP a son propre chapitre et n’est pas un sous-chapitre des valeurs de Shapley. Premièrement, les auteurs de SHAP ont proposé KernelSHAP, une approche alternative d’estimation basée sur le noyau pour les valeurs de Shapley inspirée des modèles de substitution locaux. Et ils ont proposé TreeSHAP, une approche d’estimation efficace pour les modèles arborescents. Deuxièmement, SHAP est livré avec de nombreuses méthodes d’interprétation globale basées sur des agrégations de valeurs de Shapley. Ce chapitre explique à la fois les nouvelles approches d’estimation et les méthodes d’interprétation globale.\nJe recommande de lire d’abord les chapitres sur les valeurs de Shapley et les modèles locaux (LIME).\n\n9.6.1 - Définition\nLe but de SHAP est d’expliquer la prédiction d’une instance x en calculant la contribution de chaque fonctionnalité à la prédiction. La méthode d’explication SHAP calcule les valeurs de Shapley à partir de la théorie des jeux coalitionnels. Les valeurs caractéristiques d’une instance de données agissent en tant qu’acteurs dans une coalition. Les valeurs de Shapley nous indiquent comment répartir équitablement le « paiement » (= la prédiction) entre les fonctionnalités. Un joueur peut être une valeur de caractéristique individuelle, par exemple pour des données tabulaires. Un joueur peut également être un groupe de valeurs de fonctionnalités. Par exemple, pour expliquer une image, les pixels peuvent être regroupés en superpixels et la prédiction répartie entre eux. Une innovation apportée par SHAP est que l’explication de la valeur de Shapley est représentée comme une méthode d’attribution de caractéristiques additive, un modèle linéaire. Cette vue relie les valeurs LIME et Shapley. SHAP spécifie l’explication comme suit :\n\\[g(z')=\\phi_0+\\sum_{j=1}^M\\phi_jz_j'\\]\noù g est le modèle d’explication, \\(z' \\in \\{0,1\\}^M\\) est le vecteur de coalition, M est la taille maximale de la coalition et \\(\\phi_j\\in\\mathbb{R}\\) est l’attribution de caractéristiques pour une caractéristique j, les valeurs de Shapley. Ce que j’appelle « vecteur de coalition » est appelé « fonctionnalités simplifiées » dans l’article SHAP. Je pense que ce nom a été choisi car, par exemple, pour les données d’image, les images ne sont pas représentées au niveau des pixels, mais agrégées en superpixels. Je pense qu’il est utile de considérer les z comme décrivant les coalitions : dans le vecteur de coalition, une entrée de 1 signifie que la valeur de la caractéristique correspondante est « présente » et 0 qu’elle est « absente ». Cela devrait vous sembler familier si vous connaissez les valeurs de Shapley. Pour calculer les valeurs de Shapley, nous simulons que seules certaines valeurs de caractéristiques sont jouées (« présentes ») et d’autres ne le sont pas (« absentes »). La représentation sous forme de modèle linéaire des coalitions est une astuce pour le calcul des \\(\\phi\\). Pour x, l’instance d’intérêt, le vecteur de coalition x’ est un vecteur composé uniquement de 1, c’est-à-dire que toutes les valeurs de caractéristiques sont « présentes ». La formule se simplifie en :\n\\[g(x')=\\phi_0+\\sum_{j=1}^M\\phi_j\\]\nVous pouvez trouver cette formule dans une notation similaire dans le chapitre sur les valeurs de Shapley. Nous en saurons davantage sur l’estimation réelle plus tard. Parlons d’abord des propriétés du \\(\\phi\\). C’est avant d’entrer dans les détails de leur estimation.\nLes valeurs de Shapley sont la seule solution qui satisfait aux propriétés d’efficacité, de symétrie, de factice et d’additivité. SHAP les satisfait également, puisqu’il calcule les valeurs de Shapley. Dans l’article SHAP, vous trouverez des écarts entre les propriétés SHAP et les propriétés Shapley. SHAP décrit les trois propriétés souhaitables suivantes :\n1) Précision locale\n\\[\\hat{f}(x)=g(x')=\\phi_0+\\sum_{j=1}^M\\phi_jx_j'\\]\nSi vous définissez \\(\\phi_0=E_X(\\hat{f}(x))\\) et réglez tout \\(x_j^\\prime\\) to \\(1\\), c’est la propriété d’efficacité de Shapley. Uniquement avec un nom différent et en utilisant le vecteur coalition.\n\\[\\hat{f}(x)=\\phi_0+\\sum_{j=1}^M\\phi_jx_j'=E_X(\\hat{f}(X))+\\sum_{j=1}^M\\phi_j\\]\n2) Manque\n\\[x_j^\\prime = 0 \\Rightarrow \\phi_j = 0\\]\nLe caractère manquant indique qu’une fonctionnalité manquante obtient une attribution de zéro. Noter que \\(x_j^\\prime\\) fait référence aux coalitions où une valeur de \\(0\\) représente l’absence de valeur de caractéristique. En notation de coalition, toutes les valeurs de caractéristiques \\(x_j^\\prime\\) de l’instance à expliquer doit être « \\(1\\) ». La présence d’un 0 signifierait que la valeur de la fonctionnalité est manquante pour l’instance qui vous intéresse. Cette propriété ne fait pas partie des propriétés des valeurs Shapley « normales ». Alors pourquoi en avons-nous besoin pour SHAP ? Lundberg le qualifie de « propriété comptable mineure ». Une caractéristique manquante pourrait – en théorie – avoir une valeur de Shapley arbitraire sans nuire à la propriété de précision locale, puisqu’elle est multipliée par \\(x_j^\\prime = 0\\). La propriété Missingness impose que les fonctionnalités manquantes obtiennent une valeur Shapley de \\(0\\). En pratique, cela n’est pertinent que pour les fonctionnalités constantes.\n3) Cohérence\nLaisser \\(\\hat{f}_x(z^\\prime) = \\hat{f}(h_x(z^\\prime))\\) et \\(z_{-j}^\\prime\\) indique que \\(z_j^\\prime = 0\\). Pour deux modèles \\(f\\) et \\(f^\\prime\\) qui satisfont :\n\\[\\hat{f}_x^\\prime(z^\\prime)-\\hat{f}_x^\\prime(z_{-j}')\\geq{}\\hat{f}_x(z')-\\hat{f}_x(z_{-j}^\\prime)\\]\npour toutes les entrées \\(z^\\prime \\in \\{0, 1\\}^M\\), alors :\n\\[\\phi_j(\\hat{f}^\\prime, x) \\geq \\phi_j(\\hat{f}, x)\\]\nLa propriété de cohérence indique que si un modèle change de telle sorte que la contribution marginale d’une valeur de caractéristique augmente ou reste la même (indépendamment des autres caractéristiques), la valeur de Shapley augmente également ou reste la même. De la cohérence découlent les propriétés de Shapley, linéarité, factice et symétrie, comme décrit dans l’annexe de Lundberg et Lee.\n\n\n9.6.2 KernelSHAP\nKernelSHAP estime pour une instance \\(x\\) les contributions de chaque valeur de fonctionnalité à la prédiction. KernelSHAP se compose de cinq étapes :\n\nExemples de coalitions \\(z_k'\\in\\{0,1\\}^M,\\quad{}k\\in\\{1,\\ldots,K\\}\\) (1 = fonctionnalité présente dans la coalition, 0 = fonctionnalité absente).\nObtenez une prédiction pour chaque \\(z_k'\\) en convertissant d’abord \\(z_k'\\) à l’espace de fonctionnalités d’origine, puis en appliquant le modèle \\(\\hat{f}: \\hat{f}(h_x(z_k'))\\).\nCalculer le poids de chaque \\(z_k'\\) avec le noyau SHAP.\nAjuster le modèle linéaire pondéré.\nRenvoie les valeurs Shapley \\(\\phi_k\\), les coefficients du modèle linéaire.\n\nNous pouvons créer une coalition aléatoire en lançant des pièces de monnaie à plusieurs reprises jusqu’à ce que nous obtenions une chaîne de 0 et de 1. Par exemple, le vecteur \\((1, 0, 1, 0)\\) signifie que nous avons une coalition des première et troisième caractéristiques. Les \\(K\\) coalitions échantillonnées deviennent l’ensemble de données pour le modèle de régression. La cible du modèle de régression est la prédiction d’une coalition. (« Attendez ! », dites-vous. « Le modèle n’a pas été formé sur ces données de coalition binaires et ne peut pas faire de prédictions à leur sujet. ») Pour passer des coalitions de valeurs de caractéristiques à des instances de données valides, nous avons besoin d’une fonction \\(h_x(z')=z\\) où \\(h_x:\\{0,1\\}^M\\rightarrow\\mathbb{R}^p\\). La fonction \\(h_x\\) met en relation les \\(1\\) à la valeur correspondante de l’instance x que nous voulons expliquer. Pour les données tabulaires, elle met en relation les \\(0\\) aux valeurs d’une autre instance que nous échantillonnons à partir des données. Cela signifie que nous assimilons « la valeur de caractéristique est absente » à « la valeur de caractéristique est remplacée par une valeur de caractéristique aléatoire à partir des données ». Pour les données tabulaires, la figure suivante visualise le mappage des coalitions aux valeurs de caractéristiques :\n\n\n\nLa fonction \\(h_x\\) met en relation une coalition à une instance valide. Pour les fonctionnalités actuelles (1), \\(h_x\\) fait correspondre aux valeurs des caractéristiques de \\(x\\). Pour les fonctionnalités absentes (0), \\(h_x\\) fait correspondre aux valeurs d’une instance de données échantillonnées de manière aléatoire.\n\n\n\\(h_x\\) pour la fonctionnalité de traitement des données tabulaires \\(X_j\\) et \\(X_{-j}\\) (les autres caractéristiques) comme indépendantes et s’intègre sur la distribution marginale :\n\\[\\hat{f}(h_x(z')) = E_{X_{-j}}[\\hat{f}(x)]\\]\nÉchantillonner à partir de la distribution marginale signifie ignorer la structure de dépendance entre les caractéristiques présentes et absentes. KernelSHAP souffre donc du même problème que toutes les méthodes d’interprétation basées sur les permutations. L’estimation accorde trop de poids aux cas improbables. Les résultats peuvent devenir peu fiables. Mais il est nécessaire de procéder à un échantillonnage à partir de la distribution marginale. La solution serait d’échantillonner à partir de la distribution conditionnelle, ce qui modifie la fonction valeur, et donc le jeu pour lequel les valeurs de Shapley sont la solution. Par conséquent, les valeurs Shapley ont une interprétation différente : par exemple, une fonctionnalité qui n’a peut-être pas été utilisée du tout par le modèle peut avoir une valeur Shapley non nulle lorsque l’échantillonnage conditionnel est utilisé. Pour le jeu marginal, cette valeur de fonctionnalité obtiendrait toujours une valeur Shapley de 0, car sinon elle violerait l’axiome factice.\nPour les images, la figure suivante décrit une fonction de mappage possible :\n\n\n\nFonction \\(h_x\\) mappe des coalitions de superpixels (sp) en images. Les superpixels sont des groupes de pixels. Pour les fonctionnalités actuelles (1), \\(h_x\\) renvoie la partie correspondante de l’image originale. Pour les fonctionnalités absentes (0), \\(h_x\\) grise la zone correspondante. L’attribution de la couleur moyenne des pixels environnants ou similaire serait également une option.\n\n\nLa grande différence avec LIME réside dans la pondération des instances dans le modèle de régression. LIME pondère les instances en fonction de leur proximité avec l’instance d’origine. Plus il y a de \\(0\\) dans le vecteur coalition, plus le poids dans LIME est petit. SHAP pondère les instances échantillonnées en fonction du poids que la coalition obtiendrait dans l’estimation de la valeur de Shapley. Les petites coalitions (quelques 1) et les grandes coalitions (c’est-à-dire plusieurs 1) obtiennent les pondérations les plus élevées. L’intuition derrière cela est la suivante : nous en apprenons davantage sur les caractéristiques individuelles si nous pouvons étudier leurs effets de manière isolée. Si une coalition est constituée d’une seule caractéristique, nous pouvons en apprendre davantage sur l’effet principal isolé de cette caractéristique sur la prédiction. Si une coalition comprend toutes les fonctionnalités sauf une, nous pouvons en apprendre davantage sur l’effet total de cette fonctionnalité (effet principal plus interactions entre fonctionnalités). Si une coalition est composée de la moitié des fonctionnalités, nous en apprenons peu sur la contribution d’une fonctionnalité individuelle, car il existe de nombreuses coalitions possibles avec la moitié des fonctionnalités. Pour obtenir une pondération conforme à Shapley, Lundberg et al. propose le noyau SHAP :\n\\[\\pi_{x}(z') = \\frac{(M-1)}{\\binom{M}{|z'|}|z'|(M-|z'|)}\\]\nIci, M est la taille maximale de la coalition et \\(|z^\\prime|\\) le nombre de fonctionnalités présentes dans l’instance z’. Lundberg et Lee montrent que la régression linéaire avec ce poids de noyau donne des valeurs de Shapley. Si vous utilisiez le noyau SHAP avec LIME sur les données de la coalition, LIME estimerait également les valeurs de Shapley !\nNous pouvons être un peu plus intelligents dans l’échantillonnage des coalitions : les coalitions les plus petites et les plus grandes pèsent le plus lourd. Nous obtenons de meilleures estimations de la valeur de Shapley en utilisant une partie du budget d’échantillonnage \\(K\\) pour inclure ces coalitions de poids élevé au lieu d’échantillonner aveuglément. Nous commençons par toutes les coalitions possibles avec les caractéristiques \\(1\\) et \\(M-1\\), ce qui fait \\(2 \\times M\\) coalitions au total. Lorsque nous disposons de suffisamment de budget (le budget actuel est de \\(K\\) à \\(2M\\)), nous pouvons inclure des coalitions avec 2 fonctionnalités et avec des fonctionnalités \\(M-2\\), etc. À partir des tailles de coalition restantes, nous échantillonnons avec des pondérations réajustées.\nNous avons les données, la cible et les poids; Tout ce dont nous avons besoin pour construire notre modèle de régression linéaire pondérée :\n\\[g(z')=\\phi_0+\\sum_{j=1}^M\\phi_jz_j'\\]\nNous entraînons le modèle linéaire \\(g\\) en optimisant la fonction de perte \\(L\\) suivante :\n\\[L(\\hat{f},g,\\pi_{x}) = \\sum_{z'\\in{}Z}[\\hat{f}(h_x(z'))-g(z')]^2\\pi_{x}(z')\\]\noù \\(Z\\) représente les données d’entraînement. C’est la bonne vieille somme ennuyeuse des carrés des erreurs que nous optimisons habituellement pour les modèles linéaires. Les coefficients estimés du modèle, les \\(\\phi_j\\), sont les valeurs de Shapley.\nPuisque nous sommes dans un contexte de régression linéaire, nous pouvons également utiliser les outils standards de régression. Par exemple, nous pouvons ajouter des termes de régularisation pour rendre le modèle clairsemé. Si nous ajoutons une pénalité \\(L1\\) à la perte \\(L\\), nous pouvons créer des explications clairsemées. (Je ne suis pas sûr que les coefficients résultants soient toujours des valeurs Shapley valides.)\n\n\n9.6.3 - ArbreSHAP\nLundberg et coll. (2018)2 ont proposé TreeSHAP, une variante de SHAP pour les modèles d’apprentissage automatique basés sur des arbres tels que les arbres de décision, les forêts aléatoires et les arbres améliorés par gradient. TreeSHAP a été présenté comme une alternative rapide et spécifique au modèle à KernelSHAP, mais il s’est avéré qu’il peut produire des attributions de fonctionnalités peu intuitives.\nTreeSHAP définit la fonction de valeur en utilisant l’attente conditionnelle \\(E_{X_j|X_{-j}}(\\hat{f}(x)|x_j)\\) au lieu de l’attente marginale. Le problème avec l’espérance conditionnelle est que les caractéristiques qui n’ont aucune influence sur la fonction de prédiction \\(f\\) peuvent obtenir une estimation TreeSHAP différente de zéro, comme le montrent Sundararajan et al. (2019)3 et Janzing et al. (2019)4. L’estimation non nulle peut se produire lorsque la caractéristique est corrélée à une autre caractéristique qui a réellement une influence sur la prédiction.\nÀ quel point TreeSHAP est-il plus rapide ? Comparé à KernelSHAP exact, il réduit la complexité de calcul de \\(E_{X_j|X_{-j}}(\\hat{f}(x)|x_j)\\), où \\(T\\) est le nombre d’arbres, \\(L\\) est le nombre maximum de feuilles dans n’importe quel arbre et \\(D\\) la profondeur maximale de n’importe quel arbre.\nTreeSHAP utilise l’attente conditionnelle \\(E_{X_j|X_{-j}}(\\hat{f}(x)|x_j)\\) pour estimer les effets. Je vais vous donner une intuition sur la façon dont nous pouvons calculer la prédiction attendue pour un seul arbre, une instance \\(x\\) et un sous-ensemble de fonctionnalités \\(S\\). Si nous conditionnons toutes les fonctionnalités – si \\(S\\) était l’ensemble de toutes les fonctionnalités – alors la prédiction du nœud dans à laquelle tombe l’instance \\(x\\) serait la prédiction attendue. Si nous ne conditionnions la prédiction sur aucune caractéristique – si \\(S\\) était vide – nous utiliserions la moyenne pondérée des prédictions de tous les noeuds terminaux. Si \\(S\\) contient certaines fonctionnalités, mais pas toutes, nous ignorons les prédictions de nœuds inaccessibles. Inaccessible signifie que le chemin de décision qui mène à ce nœud contredit les valeurs de \\(x_S\\). À partir des noeds terminaux restants, nous faisons la moyenne des prédictions pondérées par la taille des noeuds (c’est-à-dire le nombre d’échantillons d’apprentissage dans ce noeud). La moyenne des noeuds terminaux restants, pondérée par le nombre d’instances par noeud, est la prédiction attendue pour \\(x\\) étant donné \\(S\\). Le problème est que nous devons appliquer cette procédure pour chaque sous-ensemble \\(S\\) possible des valeurs des caractéristiques. TreeSHAP calcule en temps polynomial au lieu d’exponentiel. L’idée de base est de pousser tous les sous-ensembles \\(S\\) possibles vers le bas de l’arborescence en même temps. Pour chaque nœud de décision, nous devons suivre le nombre de sous-ensembles. Cela dépend des sous-ensembles du nœud parent et de la fonctionnalité fractionnée. Par exemple, lorsque la première division d’un arbre concerne la fonctionnalité \\(x3\\), alors tous les sous-ensembles contenant la fonctionnalité \\(x3\\) iront vers un noeud (celui où va \\(x\\)). Les sous-ensembles qui ne contiennent pas la fonctionnalité \\(x3\\) vont aux deux noeuds avec un poids réduit. Malheureusement, des sous-ensembles de tailles différentes ont des poids différents. L’algorithme doit garder une trace du poids global des sous-ensembles dans chaque noeud. Cela complique l’algorithme. Je me réfère à l’article original pour plus de détails sur TreeSHAP. Le calcul peut être étendu à davantage d’arbres : grâce à la propriété d’additivité des valeurs de Shapley, les valeurs de Shapley d’un ensemble d’arbres sont la moyenne (pondérée) des valeurs de Shapley des arbres individuels.\nEnsuite, nous examinerons les explications SHAP en action.\n\n\n9.6.4 - Exemples\nJ’ai formé un classificateur forestier aléatoire avec 100 arbres pour prédire le risque de cancer du col de l’utérus. Nous utiliserons SHAP pour expliquer les prédictions individuelles. Nous pouvons utiliser la méthode d’estimation rapide TreeSHAP au lieu de la méthode KernelSHAP, plus lente, puisqu’une forêt aléatoire est un ensemble d’arbres. Mais au lieu de s’appuyer sur la distribution conditionnelle, cet exemple utilise la distribution marginale. Ceci est décrit dans l’emballage, mais pas dans le document d’origine. La fonction Python TreeSHAP est plus lente avec la distribution marginale, mais toujours plus rapide que KernelSHAP, car elle évolue linéairement avec les lignes des données.\nParce que nous utilisons ici la distribution marginale, l’interprétation est la même que dans le chapitre sur les valeurs de Shapley. Mais avec le package Python shap vient une visualisation différente : vous pouvez visualiser les attributions de fonctionnalités telles que les valeurs Shapley sous forme de « forces ». Chaque valeur de caractéristique est une force qui augmente ou diminue la prédiction. La prédiction part de la ligne de base. La ligne de base des valeurs Shapley est la moyenne de toutes les prédictions. Dans le graphique, chaque valeur de Shapley est une flèche qui pousse pour augmenter (valeur positive) ou diminuer (valeur négative) la prédiction. Ces forces s’équilibrent lors de la prédiction réelle de l’instance de données.\nLa figure suivante montre les tracés de force d’explication SHAP pour deux femmes de l’ensemble de données sur le cancer du col de l’utérus :\n\n\n\n\nValeurs SHAP pour expliquer les probabilités prédites de cancer de deux individus. La ligne de base – la probabilité moyenne prédite – est de 0,066. La première femme a un risque prédit faible de 0,06. Les effets augmentant le risque, comme les MST, sont compensés par des effets décroissants, comme l’âge. La deuxième femme a un risque prédit élevé de 0,71. L’âge de 51 ans et 34 ans de tabagisme augmentent son risque prévu de cancer.\n\n\nC’étaient des explications pour des prédictions individuelles.\nLes valeurs de Shapley peuvent être combinées dans des explications globales. Si nous exécutons SHAP pour chaque instance, nous obtenons une matrice de valeurs Shapley. Cette matrice comporte une ligne par instance de données et une colonne par fonctionnalité. Nous pouvons interpréter l’ensemble du modèle en analysant les valeurs de Shapley dans cette matrice.\nNous commençons par l’importance des fonctionnalités SHAP.\n\n\n9.6.5 - Importance des fonctionnalités SHAP\nL’idée derrière l’importance des fonctionnalités SHAP est simple : les fonctionnalités avec de grandes valeurs absolues de Shapley sont importantes. Puisque nous voulons l’importance globale, nous faisons la moyenne des valeurs absolues de Shapley par caractéristique sur l’ensemble des données :\n\\[I_j=\\frac{1}{n}\\sum_{i=1}^n{}|\\phi_j^{(i)}|\\]\nEnsuite, nous trions les caractéristiques par importance décroissante et les traçons. La figure suivante montre l’importance de la fonctionnalité SHAP pour la forêt aléatoire formée auparavant pour prédire le cancer du col de l’utérus.\n\n\n\nImportance des caractéristiques SHAP mesurée en tant que valeurs absolues moyennes de Shapley. Le nombre d’années d’utilisation de contraceptifs hormonaux était la caractéristique la plus importante, modifiant en moyenne la probabilité absolue prédite de cancer de 2,4 points de pourcentage (0,024 sur l’axe des x).\n\n\nL’importance des fonctionnalités SHAP est une alternative à l’importance des fonctionnalités de permutation. Il existe une grande différence entre les deux mesures d’importance : l’importance des caractéristiques de permutation est basée sur la diminution des performances du modèle. SHAP est basé sur l’ampleur des attributions de fonctionnalités.\nLe graphique de l’importance des fonctionnalités est utile, mais ne contient aucune information au-delà des importances. Pour un tracé plus informatif, nous examinerons ensuite le tracé récapitulatif.\n\n\n9.6.6 - Tracé récapitulatif SHAP\nLe tracé récapitulatif combine l’importance des fonctionnalités avec les effets des fonctionnalités. Chaque point du tracé récapitulatif est une valeur Shapley pour une fonctionnalité et une instance. La position sur l’axe des y est déterminée par la caractéristique et sur l’axe des x par la valeur de Shapley. La couleur représente la valeur de la fonctionnalité de faible à élevée. Les points qui se chevauchent sont instables dans la direction de l’axe y, nous avons donc une idée de la distribution des valeurs Shapley par entité. Les fonctionnalités sont classées selon leur importance.\n\n\n\nGraphique récapitulatif SHAP. Un faible nombre d’années sous contraceptif hormonal réduit le risque prévu de cancer, un grand nombre d’années augmente le risque. Votre rappel régulier : tous les effets décrivent le comportement du modèle et ne sont pas nécessairement causals dans le monde réel.\n\n\nDans le graphique récapitulatif, nous voyons les premières indications de la relation entre la valeur d’une caractéristique et l’impact sur la prédiction. Mais pour voir la forme exacte de la relation, nous devons examiner les diagrammes de dépendance SHAP.\n\n\n9.6.7 - Diagramme de dépendance SHAP\nLa dépendance des fonctionnalités SHAP pourrait être le tracé d’interprétation globale le plus simple : 1) Choisissez une fonctionnalité. 2) Pour chaque instance de données, tracez un point avec la valeur de la caractéristique sur l’axe des x et la valeur Shapley correspondante sur l’axe des y. 3) Terminé.\nMathématiquement, l’intrigue contient les points suivants : \\(\\{(x_j^{(i)},\\phi_j^{(i)})\\}_{i=1}^n\\)\nLa figure suivante montre la dépendance de la fonction SHAP aux contraceptifs hormonaux pendant des années :\n\n\n\nGraphique de dépendance SHAP pendant des années aux contraceptifs hormonaux. Par rapport à 0 an, quelques années diminuent la probabilité prédite et un nombre élevé d’années augmente la probabilité prédite de cancer.\n\n\nLes diagrammes de dépendance SHAP sont une alternative aux diagrammes de dépendance partielle et aux effets locaux accumulés. Alors que les tracés PDP et ALE montrent les effets moyens, la dépendance SHAP montre également la variance sur l’axe des y. Surtout en cas d’interactions, le tracé de dépendance SHAP sera beaucoup plus dispersé sur l’axe des y. Le tracé des dépendances peut être amélioré en mettant en évidence ces interactions de fonctionnalités.\n\n\n9.6.8 - Valeurs d’interaction SHAP\nL’effet d’interaction est l’effet de caractéristique combiné supplémentaire après avoir pris en compte les effets de caractéristiques individuels. L’indice d’interaction de Shapley issu de la théorie des jeux est défini comme :\n\\[\\phi_{i,j} = \\sum_{S\\subseteq\\backslash\\{i,j\\}}\\frac{|S|!(M-|S|-2)!}{2(M-1)!}\\delta_{ij}(S)\\]\nquand \\(i \\neq j\\) et :\n\\[\\delta_{ij}(S)=\\hat{f}_x(S\\cup\\{i,j\\})-\\hat{f}_x(S\\cup\\{i\\})-\\hat{f}_x(S\\cup\\{j\\})+\\hat{f}_x(S)\\]\nCette formule soustrait l’effet principal des caractéristiques afin que nous obtenions l’effet d’interaction pur après avoir pris en compte les effets individuels. Nous faisons la moyenne des valeurs sur toutes les coalitions de caractéristiques possibles S, comme dans le calcul des valeurs de Shapley. Lorsque nous calculons les valeurs d’interaction SHAP pour toutes les fonctionnalités, nous obtenons une matrice par instance avec les dimensions M x M, où M est le nombre de fonctionnalités.\nComment pouvons-nous utiliser l’indice d’interaction ? Par exemple, pour colorer automatiquement le tracé de dépendance des fonctionnalités SHAP avec l’interaction la plus forte :\n\n\n\nGraphique de dépendance des fonctionnalités SHAP avec visualisation des interactions. Les années passées sous contraceptifs hormonaux interagissent avec les MST. Dans les cas proches de 0 ans, la survenue d’une MST augmente le risque prévu de cancer. Pendant plusieurs années sous contraceptif, la survenue d’une MST réduit le risque prévu. Encore une fois, il ne s’agit pas d’un modèle causal. Les effets pourraient être dus à des facteurs de confusion (par exemple, les MST et un risque moindre de cancer pourraient être corrélés à un plus grand nombre de visites chez le médecin).\n\n\n\n\n9.6.9 - Regroupement des valeurs Shapley\nVous pouvez regrouper vos données à l’aide des valeurs Shapley. Le but du clustering est de trouver des groupes d’instances similaires. Normalement, le clustering est basé sur des fonctionnalités. Les fonctionnalités sont souvent à différentes échelles. Par exemple, la hauteur peut être mesurée en mètres, l’intensité des couleurs de 0 à 100 et certaines sorties de capteurs entre \\(-1\\) et \\(1\\). La difficulté est de calculer les distances entre des instances présentant des caractéristiques aussi différentes et non comparables.\nLe clustering SHAP fonctionne en regroupant les valeurs Shapley de chaque instance. Cela signifie que vous regroupez les instances par similarité d’explication. Toutes les valeurs SHAP ont la même unité : l’unité de l’espace de prédiction. Vous pouvez utiliser n’importe quelle méthode de clustering. L’exemple suivant utilise le clustering agglomératif hiérarchique pour trier les instances.\nLe tracé se compose de nombreux tracés de force, chacun expliquant la prédiction d’une instance. Nous faisons pivoter les tracés de force verticalement et les plaçons côte à côte en fonction de leur similarité de regroupement.\n\n\n\nExplications SHAP empilées regroupées par similarité d’explication. Chaque position sur l’axe des X est une instance des données. Les valeurs SHAP rouges augmentent la prédiction, les valeurs bleues la diminuent. Un groupe se démarque : à droite se trouve un groupe avec un risque de cancer prévu élevé.\n\n\n\n\n9.6.10 - Avantages\nPuisque SHAP calcule les valeurs de Shapley, tous les avantages des valeurs de Shapley s’appliquent : SHAP a une base théorique solide en théorie des jeux. La prédiction est équitablement répartie entre les valeurs des caractéristiques. Nous obtenons des explications contrastées qui comparent la prédiction avec la prédiction moyenne.\nSHAP connecte les valeurs LIME et Shapley. Ceci est très utile pour mieux comprendre les deux méthodes. Cela contribue également à unifier le domaine de l’apprentissage automatique interprétable.\nSHAP a une implémentation rapide pour les modèles arborescents. Je pense que cela a été la clé de la popularité de SHAP, car le plus grand obstacle à l’adoption des valeurs de Shapley est la lenteur des calculs.\nLe calcul rapide permet de calculer les nombreuses valeurs de Shapley nécessaires aux interprétations du modèle global. Les méthodes d’interprétation globale incluent l’importance des caractéristiques, la dépendance des caractéristiques, les interactions, le regroupement et les tracés récapitulatifs. Avec SHAP, les interprétations globales sont cohérentes avec les explications locales, puisque les valeurs de Shapley sont « l’unité atomique » des interprétations globales. Si vous utilisez LIME pour les explications locales et les diagrammes de dépendance partielle ainsi que l’importance des fonctionnalités de permutation pour les explications globales, il vous manque une base commune.\n\n\n9.6.11 - Inconvénients\nKernelSHAP est lent. Cela rend KernelSHAP peu pratique à utiliser lorsque vous souhaitez calculer les valeurs Shapley pour de nombreuses instances. De plus, toutes les méthodes SHAP globales telles que l’importance des fonctionnalités SHAP nécessitent le calcul des valeurs Shapley pour de nombreuses instances.\nKernelSHAP ignore la dépendance aux fonctionnalités. La plupart des autres méthodes d’interprétation basées sur la permutation ont ce problème. En remplaçant les valeurs des caractéristiques par des valeurs provenant d’instances aléatoires, il est généralement plus facile d’échantillonner aléatoirement à partir de la distribution marginale. Cependant, si les caractéristiques sont dépendantes, par exemple corrélées, cela conduit à accorder trop de poids à des points de données improbables. TreeSHAP résout ce problème en modélisant explicitement la prédiction conditionnelle attendue.\nTreeSHAP peut produire des attributions de fonctionnalités peu intuitives. Bien que TreeSHAP résolve le problème de l’extrapolation à des points de données improbables, il le fait en modifiant la fonction de valeur et change donc légèrement la donne. TreeSHAP modifie la fonction de valeur en s’appuyant sur la prédiction conditionnelle attendue. Avec le changement de fonction de valeur, les entités qui n’ont aucune influence sur la prédiction peuvent obtenir une valeur TreeSHAP différente de zéro.\nLes inconvénients des valeurs de Shapley s’appliquent également à SHAP : les valeurs de Shapley peuvent être mal interprétées et l’accès aux données est nécessaire pour les calculer pour de nouvelles données (sauf pour TreeSHAP).\nIl est possible de créer des interprétations intentionnellement trompeuses avec SHAP, qui peuvent cacher des biais5. Si vous êtes le data scientist qui crée les explications, ce n’est pas un réel problème (ce serait même un avantage si vous êtes le data scientist maléfique qui veut créer des explications trompeuses). Pour les destinataires d’une explication SHAP, c’est un inconvénient : ils ne peuvent pas être sûrs de la véracité de l’explication.\n\n\n9.6.12 - Logiciel\nLes auteurs ont implémenté SHAP dans le module Python shap.\n\n\n\n\n\n\nNote\n\n\n\nLe livre Interpreting Machine Learning Models with SHAP couvre en profondeur l’application de SHAP avec le package shap.\n\n\nCette implémentation fonctionne pour les modèles arborescents dans la bibliothèque d’apprentissage automatique scikit-learn pour Python. Le package shap a également été utilisé pour les exemples de ce chapitre. SHAP est intégré aux frameworks d’amélioration d’arborescence xgboost et LightGBM. Dans R, il existe les modules shapper et fastshap. SHAP est également inclus dans le module R xgboost.",
    "crumbs": [
      "9 - Méthodes locales indépendantes du modèle",
      "9.6 - SHAP (SHapley Additive exPlanations)"
    ]
  },
  {
    "objectID": "09-local_model_agnostic_methods/09.6-shap.html#footnotes",
    "href": "09-local_model_agnostic_methods/09.6-shap.html#footnotes",
    "title": "Apprentissage automatique interprétable",
    "section": "Notes de bas de page",
    "text": "Notes de bas de page\n\n\nLundberg, Scott M., and Su-In Lee. “A unified approach to interpreting model predictions.” Advances in Neural Information Processing Systems (2017).↩︎\nLundberg, Scott M., Gabriel G. Erion, and Su-In Lee. “Consistent individualized feature attribution for tree ensembles.” arXiv preprint arXiv:1802.03888 (2018).↩︎\nSundararajan, Mukund, and Amir Najmi. “The many Shapley values for model explanation.” arXiv preprint arXiv:1908.08474 (2019).↩︎\nJanzing, Dominik, Lenon Minorics, and Patrick Blöbaum. “Feature relevance quantification in explainable AI: A causal problem.” International Conference on Artificial Intelligence and Statistics. PMLR (2020).↩︎\nSlack, Dylan, Sophie Hilgard, Emily Jia, Sameer Singh, and Himabindu Lakkaraju. “Fooling lime and shap: Adversarial attacks on post hoc explanation methods.” In Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society, pp. 180-186 (2020).↩︎",
    "crumbs": [
      "9 - Méthodes locales indépendantes du modèle",
      "9.6 - SHAP (SHapley Additive exPlanations)"
    ]
  },
  {
    "objectID": "09-local_model_agnostic_methods/09.4-anchors.html",
    "href": "09-local_model_agnostic_methods/09.4-anchors.html",
    "title": "Apprentissage automatique interprétable",
    "section": "",
    "text": "Avertissement\n\n\n\nEn cours de traduction.",
    "crumbs": [
      "9 - Méthodes locales indépendantes du modèle",
      "9.4 - Règles de portée (ancres)"
    ]
  },
  {
    "objectID": "09-local_model_agnostic_methods/09.4-anchors.html#règles-de-portée-ancres",
    "href": "09-local_model_agnostic_methods/09.4-anchors.html#règles-de-portée-ancres",
    "title": "Apprentissage automatique interprétable",
    "section": "9.4 - Règles de portée (ancres)",
    "text": "9.4 - Règles de portée (ancres)\nAuteurs : Tobias Goerke et Magdalena Lang\nLa méthode des ancres explique les prédictions individuelles de tout modèle de classification par boîte noire en trouvant une règle de décision qui « ancre » suffisamment la prédiction. Une règle ancre une prédiction si les modifications apportées aux autres valeurs de caractéristiques n’affectent pas la prédiction. Anchors utilise des techniques d’apprentissage par renforcement en combinaison avec un algorithme de recherche de graphiques pour réduire le nombre d’appels de modèle (et donc le temps d’exécution requis) au minimum tout en étant capable de récupérer des optima locaux. Ribeiro, Singh et Guestrin ont proposé l’algorithme en 20181 – les mêmes chercheurs qui ont introduit l’ algorithme LIME.\nComme son prédécesseur, l’approche des ancres déploie une stratégie basée sur les perturbations pour générer des explications locales pour les prédictions des modèles d’apprentissage automatique en boîte noire. Cependant, au lieu des modèles de substitution utilisés par LIME, les explications résultantes sont exprimées sous forme de règles SI-ALORS faciles à comprendre , appelées ancres. Ces règles sont réutilisables, car elles sont étendues : les ancres incluent la notion de couverture, indiquant précisément à quelles autres instances, éventuellement invisibles, elles s’appliquent. Trouver des points d’ancrage implique un problème d’exploration ou de bandit à plusieurs bras, qui trouve son origine dans la discipline de l’apprentissage par renforcement. À cette fin, des voisins, ou perturbations, sont créés et évalués pour chaque instance expliquée. Cela permet à l’approche de ne pas tenir compte de la structure de la boîte noire et de ses paramètres internes afin que ceux-ci puissent rester à la fois inobservés et inchangés. Ainsi, l’algorithme est indépendant du modèle, ce qui signifie qu’il peut être appliqué à n’importe quelle classe de modèle.\nDans leur article, les auteurs comparent leurs deux algorithmes et visualisent à quel point ils consultent différemment le voisinage d’une instance pour obtenir des résultats. Pour cela, la figure suivante représente à la fois LIME et les ancres expliquant localement un classificateur binaire complexe (prédit soit - soit + ) à l’aide de deux instances exemplaires. Les résultats de LIME n’indiquent pas leur fidélité, car LIME apprend uniquement une limite de décision linéaire qui se rapproche le mieux du modèle étant donné un espace de perturbation \\(D\\). Étant donné le même espace de perturbations, l’approche des ancres construit des explications dont la couverture est adaptée au comportement du modèle et l’approche exprime clairement leurs limites. Ainsi, ils sont fidèles par conception et indiquent exactement pour quelles instances ils sont valables. Cette propriété rend les ancres particulièrement intuitives et faciles à comprendre.\n\n\n\nLIME vs. Anchors – Une visualisation de jouet. Figure de Ribeiro, Singh et Guestrin (2018).\n\n\nComme mentionné précédemment, les résultats ou explications de l’algorithme se présentent sous la forme de règles, appelées ancres. L’exemple simple suivant illustre une telle ancre. Par exemple, supposons que nous disposions d’un modèle de boîte noire bivariée qui prédit si un passager a survécu ou non au désastre du Titanic. Nous aimerions maintenant savoir pourquoi le modèle prédit la survie d’un individu spécifique. L’algorithme d’ancrage fournit une explication des résultats comme celle présentée ci-dessous.\n\n\n\nAttribut\nValeur\n\n\n\n\nAge\n20\n\n\nSexe\nféminin\n\n\nClasse\npremière\n\n\nPrix du Ticket\n300$\n\n\nPlus attributes\n…\n\n\nSurvivant\nvrai\n\n\n\nEt l’explication des ancres correspondantes est :\nSI Sex = féminin ET Classe = première PUIS PRÉVISIONS Survivant = vrai AVEC PRÉCISION \\(97%\\) ET COUVERTURE \\(15%\\).\nL’exemple montre comment les ancres peuvent fournir des informations essentielles sur la prédiction d’un modèle et son raisonnement sous-jacent. Le résultat montre quels attributs ont été pris en compte par le modèle, qui dans ce cas est le sexe féminin et la première classe. Les humains, étant primordiaux pour l’exactitude, peuvent utiliser cette règle pour valider le comportement du modèle. L’ancre nous indique en outre qu’elle s’applique à \\(15%\\) des instances de l’espace de perturbation. Dans ces cas, l’explication est précise à \\(97%\\), ce qui signifie que les prédicats affichés sont presque exclusivement responsables du résultat prédit.\nUne ancre \\(A\\) est formellement défini comme suit :\n\\[ \\mathbb{E}_{\\mathcal{D}_x(z|A)}[1_{\\hat{f}(x)=\\hat{f}(z)}]\\geq\\tau,A(x)=1 \\]\noù: - \\(x\\) représente l’instance expliquée (par exemple, une ligne dans un ensemble de données tabulaires). - \\(A\\) est un ensemble de prédicats, c’est-à-dire la règle ou l’ancre résultante, telle que \\(A(x)=1\\) lorsque tous les prédicats de fonctionnalités définis par \\(A\\) correspondent aux valeurs des attributs de \\(x\\). - \\(f\\) désigne le modèle de classification à expliquer (par exemple un modèle de réseau neuronal artificiel). Il peut être interrogé pour prédire une étiquette pour \\(x\\) et ses perturbations. - \\(D_x (\\cdot|A)\\) indique la répartition des voisins de \\(x\\), correspondant à \\(A\\). - \\(0 \\leq \\tau \\leq 1\\) spécifie un seuil de précision. Seules les règles qui atteignent une fidélité locale d’au moins \\(\\tau\\) sont considérés comme un résultat valide.\nLa description formelle peut être intimidante et peut être formulée en mots :\n\nÉtant donné une instance \\(x\\) à expliquer, une règle ou une ancre \\(A\\) doit être trouvé, tel qu’il s’applique à \\(x\\), alors que la même classe que pour \\(x\\) est prédit pour une fraction d’au moins \\(\\tau\\) de les voisins étaient pareils \\(A\\) est applicable. La précision d’une règle résulte de l’évaluation des voisins ou des perturbations (suite à \\(D_x (z|A)\\)) à l’aide du modèle d’apprentissage automatique fourni (indiqué par la fonction d’indicateur \\(1_{\\hat{f}(x) = \\hat{f}(z)}\\)).\n\n\n9.4.1 - Trouver des points d’ancrage\nBien que la description mathématique des ancres puisse sembler claire et simple, la construction de règles particulières est irréalisable. Il faudrait évaluer \\(1_{\\hat{f}(x) = \\hat{f}(z)}\\) pour tous \\(z \\in \\mathcal{D}_x(\\cdot|A)\\) ce qui n’est pas possible dans des espaces d’entrée continus ou grands. Les auteurs proposent donc d’introduire le paramètre \\(0 \\leq \\delta \\leq 1\\) pour créer une définition probabiliste. De cette façon, les échantillons sont tirés jusqu’à ce qu’il existe une confiance statistique quant à leur précision. La définition probabiliste se lit comme suit :\n\\[P(prec(A)\\geq\\tau)\\geq{}1-\\delta\\quad\\textrm{with}\\quad{}prec(A)=\\mathbb{E}_{\\mathcal{D}_x(z|A)}[1_{\\hat{f}(x)=\\hat{f}(z)}]\\]\nLes deux définitions précédentes sont combinées et étendues par la notion de couverture. Sa logique consiste à trouver des règles qui s’appliquent à une partie de préférence importante de l’espace d’entrée du modèle. La couverture est formellement définie comme la probabilité qu’une ancre s’applique à ses voisines, c’est-à-dire son espace de perturbation :\n\\[cov(A)=\\mathbb{E}_{\\mathcal{D}_{(z)}}[A(z)]\\]\nL’inclusion de cet élément conduit à la définition finale de l’ancre prenant en compte la maximisation de la couverture :\n\\[\\underset{A\\:\\textrm{s.t.}\\;P(prec(A)\\geq\\tau)\\geq{}1-\\delta}{\\textrm{max}}cov(A)\\]\nAinsi, la procédure vise à obtenir une règle qui a la couverture la plus élevée parmi toutes les règles éligibles (toutes celles qui satisfont au seuil de précision compte tenu de la définition probabiliste). Ces règles sont considérées comme plus importantes, car elles décrivent une plus grande partie du modèle. Notez que les règles avec plus de prédicats ont tendance à avoir une plus grande précision que les règles avec moins de prédicats. En particulier, une règle qui corrige chaque fonctionnalité de \\(x\\) réduit le voisinage évalué à des instances identiques. Ainsi, le modèle classe tous les voisins de manière égale et la précision de la règle est \\(1\\). Dans le même temps, une règle qui corrige de nombreuses fonctionnalités est trop spécifique et ne s’applique qu’à quelques instances. Il existe donc un compromis entre précision et couverture.\nL’approche des ancres utilise quatre composants principaux pour trouver des explications.\nGénération de candidats : génère de nouveaux candidats d’explication. Au premier tour, un candidat par fonctionnalité de \\(x\\) est créé et fixe la valeur respective des perturbations possibles. Dans un tour sur deux, les meilleurs candidats du tour précédent sont étendus par un prédicat de fonctionnalité qui n’y est pas encore contenu.\nMeilleure identification du candidat : les règles des candidats doivent être comparées en fonction de la règle qui explique \\(x\\) le meilleur. À cette fin, les perturbations correspondant à la règle actuellement observée sont créées et évaluées en appelant le modèle. Cependant, ces appels doivent être minimisés afin de limiter la surcharge de calcul. C’est pourquoi, au cœur de cette composante, il y a un bandit multi-armé d’exploration pure (MAB ; KL-LUCB2, pour être précis). Les MAB sont utilisés pour explorer et exploiter efficacement différentes stratégies (appelées armes par analogie avec les machines à sous) en utilisant la sélection séquentielle. Dans le contexte donné, chaque règle candidate doit être considérée comme un bras qui peut être tiré. Chaque fois qu’elle est tirée, les voisins respectifs sont évalués, et nous obtenons ainsi plus d’informations sur le gain de la règle candidate (précision dans le cas de l’ancre). La précision indique ainsi dans quelle mesure la règle décrit bien l’instance à expliquer.\nValidation de la précision du candidat : prend davantage d’échantillons au cas où il n’y aurait pas encore de certitude statistique que le candidat dépasse le seuil \\(\\tau\\).\nRecherche de faisceau modifiée : tous les composants ci-dessus sont assemblés dans une recherche de faisceau, qui est un algorithme de recherche graphique et une variante de l’algorithme de largeur d’abord. Il porte les \\(B\\) meilleurs candidats de chaque tour au suivant (où \\(B\\) est appelée la largeur du faisceau). Ces \\(B\\) meilleures règles sont ensuite utilisées pour créer de nouvelles règles. La recherche du faisceau conduit au maximum \\(nombreAttributs(x)\\) tours, car chaque fonctionnalité ne peut être incluse dans une règle qu’une seule fois. Ainsi, à chaque tour \\(i\\), il génère des candidats avec exactement \\(i\\) prédicat et sélectionne le meilleur B de celui-ci. Par conséquent, en définissant \\(B\\) élevé, l’algorithme est plus susceptible d’éviter les optima locaux. Cela nécessite à son tour un nombre élevé d’appels de modèles et augmente ainsi la charge de calcul.\nCes quatre composants sont illustrés dans la figure ci-dessous.\n\n\n\nLes composants de l’algorithme d’ancrage et leurs interrelations (simplifié).\n\n\nCette approche est une recette apparemment parfaite pour dériver efficacement des informations statistiquement valables sur les raisons pour lesquelles un système a classé une instance de la manière dont il l’a fait. Il expérimente systématiquement les entrées du modèle et conclut en observant les sorties respectives. Il s’appuie sur des méthodes d’apprentissage automatique (MAB) bien établies et étudiées pour réduire le nombre d’appels effectués au modèle. Cela réduit considérablement le temps d’exécution de l’algorithme.\n\n\n9.4.2 - Complexité et durée d’exécution\nConnaître le comportement d’exécution asymptotique de l’approche des ancres permet d’évaluer ses performances attendues sur des problèmes spécifiques. Laisser \\(B\\) désignent la largeur du faisceau et \\(p\\) le nombre de toutes les fonctionnalités. Ensuite, l’algorithme des ancres est soumis à :\n\\[\\mathcal{O}(B\\cdot{}p^2+p^2\\cdot\\mathcal{O}_{\\textrm{MAB}\\lbrack{}B\\cdot{}p,B\\rbrack})\\]\nCette limite fait abstraction des hyperparamètres indépendants du problème, tels que la confiance statistique \\(\\delta\\). Ignorer les hyperparamètres permet de réduire la complexité de la limite (voir l’article original pour plus d’informations). Puisque le MAB extrait le \\(B\\) le meilleur de \\(B . p\\) candidats à chaque tour, la plupart des MAB et leurs durées d’exécution multiplient le \\(p^2\\) facteur plus que tout autre paramètre.\nCela devient alors évident : l’efficacité de l’algorithme diminue lorsque de nombreuses fonctionnalités sont présentes.\n\n\n9.4.3 - Exemple de données tabulaires\nLes données tabulaires sont des données structurées représentées par des tableaux, dans lesquels les colonnes incarnent des fonctionnalités et des instances de lignes. Par exemple, nous utilisons les données de location de vélos pour démontrer le potentiel de l’approche des ancres à expliquer les prédictions de ML pour des instances sélectionnées. Pour cela, nous transformons la régression en problème de classification et formons une forêt aléatoire comme modèle de boîte noire. Il s’agit de déterminer si le nombre de vélos loués se situe au-dessus ou en dessous de la ligne de tendance.\nAvant de créer des explications d’ancrage, il faut définir une fonction de perturbation. Un moyen simple d’y parvenir consiste à utiliser un espace de perturbation intuitif par défaut pour les cas d’explication tabulaires qui peuvent être construits par échantillonnage, par exemple à partir des données d’entraînement. Lors de la perturbation d’une instance, cette approche par défaut conserve les valeurs de la fonctionnalité qui sont soumises aux prédicats des ancres, tout en remplaçant les fonctionnalités non fixes par des valeurs extraites d’une autre instance échantillonnée aléatoirement avec une probabilité spécifiée. Ce processus génère de nouvelles instances similaires à celle expliquée, mais qui ont adopté certaines valeurs d’autres instances aléatoires. Ainsi, ils ressemblent aux voisins de l’instance expliquée.\n\n\n\nAncres expliquant six instances de l’ensemble de données de location de vélos. Chaque ligne représente une explication ou une ancre, et chaque barre représente les prédicats de fonctionnalités qu’elle contient. L’axe des X affiche la précision d’une règle et l’épaisseur d’une barre correspond à sa couverture. La règle « de base » ne contient aucun prédicat. Ces ancres montrent que le modèle prend principalement en compte la température pour les prédictions.\n\n\nLes résultats sont instinctivement interprétables et montrent pour chaque instance expliquée quelles caractéristiques sont les plus importantes pour la prédiction du modèle. Comme les ancres n’ont que quelques prédicats, elles ont en outre une couverture élevée et s’appliquent donc à d’autres cas. Les règles présentées ci-dessus ont été générées avec \\(\\tau = 0,9\\). Ainsi, nous demandons des ancres dont les perturbations évaluées soutiennent fidèlement l’étiquette avec une précision d’au moins \\(90%\\). En outre, la discrétisation a été utilisée pour augmenter l’expressivité et l’applicabilité des caractéristiques numériques.\nToutes les règles précédentes ont été générées pour les cas où le modèle décide en toute confiance sur la base de quelques fonctionnalités. Cependant, d’autres instances ne sont pas classées de manière aussi distincte par le modèle, car davantage de fonctionnalités sont importantes. Dans de tels cas, les ancres deviennent plus spécifiques, comprennent plus de fonctionnalités et s’appliquent à moins d’instances.\n\n\n\nExpliquer les instances proches des limites de décision conduit à des règles spécifiques comprenant un nombre plus élevé de prédicats de fonctionnalités et une couverture plus faible. De plus, la règle vide, c’est-à-dire la fonctionnalité de base, devient moins importante. Cela peut être interprété comme un signal pour une limite de décision, car l’instance est située dans un quartier instable.\n\n\nBien que le choix de l’espace de perturbation par défaut soit un choix confortable, il peut avoir un impact important sur l’algorithme et conduire ainsi à des résultats biaisés. Par exemple, si la rame est déséquilibrée (il existe un nombre inégal d’instances de chaque classe), l’espace des perturbations l’est également. Cette condition affecte en outre la recherche de règles et la précision du résultat.\nL’ ensemble de données sur le cancer du col de l’utérus constitue un excellent exemple de cette situation. L’application de l’algorithme d’ancrage conduit à l’une des situations suivantes :\n\nExpliquer les instances étiquetées healthy donne des règles vides car tous les voisins générés sont évalués comme healthy.\nLes explications pour les instances étiquetées cancer sont trop spécifiques, c’est-à-dire comprennent de nombreux prédicats de caractéristiques, puisque l’espace de perturbation couvre principalement les valeurs des instances saines .\n\n\n\n\nConstruire des ancres dans des espaces de perturbations déséquilibrés conduit à des résultats inexpressifs.\n\n\nCe résultat peut être indésirable et peut être abordé de plusieurs manières. Par exemple, un espace de perturbation personnalisé peut être défini. Cette perturbation personnalisée peut être échantillonnée différemment, par exemple à partir d’un ensemble de données déséquilibré ou d’une distribution normale. Cela a cependant un effet secondaire : les voisins échantillonnés ne sont pas représentatifs et modifient la portée de la couverture. Alternativement, nous pourrions modifier la confiance du MAB \\(\\delta\\) et valeurs des paramètres d’erreur \\(\\epsilon\\). Cela amènerait le MAB à prélever davantage d’échantillons, ce qui conduirait finalement à ce que la minorité soit échantillonnée plus souvent en termes absolus.\nPour cet exemple, nous utilisons un sous-ensemble de l’ensemble des cancers du col de l’utérus dans lequel la majorité des cas sont étiquetés cancer . Nous disposons alors du cadre pour créer un espace de perturbation correspondant à partir de celui-ci. Les perturbations sont désormais plus susceptibles de conduire à des prédictions variables, et l’algorithme d’ancrage peut identifier des caractéristiques importantes. Il faut cependant tenir compte de la définition de la couverture : elle n’est définie que dans l’espace des perturbations. Dans les exemples précédents, nous avons utilisé la rame comme base de l’espace de perturbation. Puisque nous n’utilisons ici qu’un sous-ensemble, une couverture élevée n’indique pas nécessairement une importance globale élevée des règles.\n\n\n\nÉquilibrer l’ensemble de données avant de construire des ancrages montre le raisonnement du modèle pour les décisions dans les cas minoritaires.\n\n\n\n\n9.4.4 - Avantages\nL’approche des ancres offre de multiples avantages par rapport à LIME. Premièrement, le résultat de l’algorithme est plus facile à comprendre, car les règles sont faciles à interpréter (même pour les profanes).\nDe plus, les ancres sont sous-configurables et indiquent même une mesure d’importance en incluant la notion de couverture. Deuxièmement, l’approche des ancres fonctionne lorsque les prédictions du modèle sont non linéaires ou complexes dans le voisinage d’une instance. Comme l’approche déploie des techniques d’apprentissage par renforcement au lieu d’ajuster des modèles de substitution, elle est moins susceptible de sous-ajuster le modèle.\nEn dehors de cela, l’algorithme est indépendant du modèle et donc applicable à n’importe quel modèle.\nDe plus, il est très efficace car il peut être parallélisé en utilisant des MAB prenant en charge l’échantillonnage par lots (par exemple BatchSAR).\n\n\n9.4.5 - Inconvénients\nL’algorithme souffre d’une configuration hautement configurable et percutante, tout comme la plupart des explicateurs basés sur les perturbations. Non seulement les hyperparamètres tels que la largeur du faisceau ou le seuil de précision doivent être ajustés pour produire des résultats significatifs, mais la fonction de perturbation doit également être explicitement conçue pour un domaine/cas d’utilisation. Pensez à la façon dont les données tabulaires sont perturbées et réfléchissez à la manière d’appliquer les mêmes concepts aux données d’image (indice : ceux-ci ne peuvent pas être appliqués). Heureusement, des approches par défaut peuvent être utilisées dans certains domaines (par exemple tabulaires), facilitant ainsi la configuration initiale d’une explication.\nEn outre, de nombreux scénarios nécessitent une discrétisation, sinon les résultats sont trop spécifiques, ont une faible couverture et ne contribuent pas à la compréhension du modèle. Bien que la discrétisation puisse aider, elle peut également brouiller les limites de décision si elle est utilisée avec négligence et ainsi avoir l’effet exactement opposé. Puisqu’il n’existe pas de meilleure technique de discrétisation, les utilisateurs doivent connaître les données avant de décider comment discrétiser les données afin de ne pas obtenir de mauvais résultats.\nLa construction d’ancres nécessite de nombreux appels au modèle ML, comme tous les explicateurs basés sur les perturbations. Même si l’algorithme déploie des MAB pour minimiser le nombre d’appels, sa durée d’exécution dépend toujours beaucoup des performances du modèle et est donc très variable.\nEnfin, la notion de couverture est indéfinie dans certains domaines. Par exemple, il n’existe pas de définition évidente ou universelle de la façon dont les superpixels d’une image se comparent à ceux d’autres images.\n\n\n9.4.6 - Logiciels et alternatives\nActuellement, deux implémentations sont disponibles : Anchor, un module Python (également intégré par Alibi) et une implémentation Java. Le premier est la référence des auteurs de l’algorithme d’ancrage et le second est une implémentation haute performance livrée avec une interface R, appelée anchors, qui a été utilisée pour les exemples de ce chapitre. À l’heure actuelle, l’implémentation des ancres ne prend en charge que les données tabulaires. Cependant, des ancres peuvent théoriquement être construites pour n’importe quel domaine ou type de données.",
    "crumbs": [
      "9 - Méthodes locales indépendantes du modèle",
      "9.4 - Règles de portée (ancres)"
    ]
  },
  {
    "objectID": "09-local_model_agnostic_methods/09.4-anchors.html#footnotes",
    "href": "09-local_model_agnostic_methods/09.4-anchors.html#footnotes",
    "title": "Apprentissage automatique interprétable",
    "section": "Notes de bas de page",
    "text": "Notes de bas de page\n\n\nMarco Tulio Ribeiro, Sameer Singh and Carlos Guestrin. “Anchors: high-precision model-agnostic explanations”. AAAI Conference on Artificial Intelligence (AAAI), 2018↩︎\nEmilie Kaufmann and Shivaram Kalyanakrishnan. “Information complexity in bandit subset selection”. Proceedings of Machine Learning Research (2013).↩︎",
    "crumbs": [
      "9 - Méthodes locales indépendantes du modèle",
      "9.4 - Règles de portée (ancres)"
    ]
  },
  {
    "objectID": "09-local_model_agnostic_methods/09.2-lime.html",
    "href": "09-local_model_agnostic_methods/09.2-lime.html",
    "title": "9.2.1 - LIME pour les données tabulaires",
    "section": "",
    "text": "Avertissement\n\n\n\nEn cours de traduction.",
    "crumbs": [
      "9 - Méthodes locales indépendantes du modèle",
      "9.2 - Substitut local (LIME)"
    ]
  },
  {
    "objectID": "09-local_model_agnostic_methods/09.2-lime.html#substitut-local-lime",
    "href": "09-local_model_agnostic_methods/09.2-lime.html#substitut-local-lime",
    "title": "9.2.1 - LIME pour les données tabulaires",
    "section": "9.2 - Substitut local (LIME)",
    "text": "9.2 - Substitut local (LIME)\nLes modèles de substitution locaux sont des modèles interprétables utilisés pour expliquer les prédictions individuelles des modèles d’apprentissage automatique en boîte noire. Explications locales interprétables et indépendantes du modèle (LIME) 50 est un article dans lequel les auteurs proposent une implémentation concrète de modèles de substitution locaux. Les modèles de substitution sont formés pour se rapprocher des prédictions du modèle de boîte noire sous-jacent. Au lieu de former un modèle de substitution global, LIME se concentre sur la formation de modèles de substitution locaux pour expliquer les prédictions individuelles.\nL’idée est assez intuitive. Tout d’abord, oubliez les données d’entraînement et imaginez que vous n’avez que le modèle de boîte noire dans lequel vous pouvez saisir des points de données et obtenir les prédictions du modèle. Vous pouvez sonder la boîte aussi souvent que vous le souhaitez. Votre objectif est de comprendre pourquoi le modèle d’apprentissage automatique a fait une certaine prédiction. LIME teste ce qui arrive aux prédictions lorsque vous fournissez des variations de vos données dans le modèle d’apprentissage automatique. LIME génère un nouvel ensemble de données composé d’échantillons perturbés et des prédictions correspondantes du modèle de boîte noire. Sur ce nouvel ensemble de données, LIME entraîne ensuite un modèle interprétable, qui est pondéré par la proximité des instances échantillonnées par rapport à l’instance d’intérêt. Le modèle interprétable peut provenir du chapitre sur les modèles interprétables, par exemple Lasso ou un arbre de décision. Le modèle appris doit être une bonne approximation locale des prédictions du modèle d’apprentissage automatique, mais il n’est pas nécessaire qu’il soit une bonne approximation globale. Ce type de précision est également appelé fidélité locale.\nMathématiquement, les modèles de substitution locaux avec contrainte d’interprétabilité peuvent être exprimés comme suit :\n\\[\\text{explanation}(x)=\\arg\\min_{g\\in{}G}L(f,g,\\pi_x)+\\Omega(g)\\]\nLe modèle d’explication par exemple \\(x\\) est le modèle \\(g\\) (par exemple modèle de régression linéaire) qui minimise la perte \\(L\\) (par exemple erreur quadratique moyenne), qui mesure à quel point l’explication est proche de la prédiction du modèle d’origine \\(f\\) (par exemple un modèle xgboost), tandis que la complexité du modèle \\(\\Omega(g)\\) est maintenu faible (par exemple, préférez moins de fonctionnalités). \\(G\\) est la famille des explications possibles, par exemple tous les modèles de régression linéaire possibles. La mesure de proximité \\(\\pi_x\\) définit la taille du voisinage autour de l’instance \\(x\\) que nous considérons pour l’explication. En pratique, LIME optimise uniquement la partie perte. L’utilisateur doit déterminer la complexité, par exemple en sélectionnant le nombre maximum de fonctionnalités que le modèle de régression linéaire peut utiliser.\nLa recette pour former des modèles de substitution locaux :\n\nSélectionnez votre instance d’intérêt pour laquelle vous souhaitez obtenir une explication de sa prédiction de boîte noire.\nPerturbez votre ensemble de données et obtenez les prédictions de la boîte noire pour ces nouveaux points.\nPondérez les nouveaux échantillons en fonction de leur proximité avec l’instance d’intérêt.\nEntraînez un modèle pondéré et interprétable sur l’ensemble de données avec les variations.\nExpliquez la prédiction en interprétant le modèle local.\n\nDans les implémentations actuelles dans R et Python , par exemple, la régression linéaire peut être choisie comme modèle de substitution interprétable. Au préalable, vous devez sélectionner \\(K\\), le nombre de fonctionnalités que vous souhaitez avoir dans votre modèle interprétable. Plus \\(K\\) est faible, plus il est facile d’interpréter le modèle. Un \\(K\\) plus élevé produit potentiellement des modèles avec une fidélité plus élevée. Il existe plusieurs méthodes pour entraîner des modèles avec exactement \\(K\\) fonctionnalités. Un bon choix est Lasso. Un modèle Lasso avec un paramètre de régularisation élevé \\(\\lambda\\) donne un modèle sans aucune fonctionnalité. En recyclant les modèles Lasso avec une diminution lente \\(\\lambda\\), l’une après l’autre, les entités obtiennent des estimations de poids différentes de zéro. S’il y a \\(K\\) fonctionnalités dans le modèle, vous avez atteint le nombre de fonctionnalités souhaité. D’autres stratégies consistent en une sélection avant ou arrière de fonctionnalités. Cela signifie que vous commencez soit avec le modèle complet (= contenant toutes les fonctionnalités), soit avec un modèle avec uniquement l’interception, puis testez quelle fonctionnalité apporterait la plus grande amélioration une fois ajoutée ou supprimée, jusqu’à ce qu’un modèle avec \\(K\\) fonctionnalités soit atteint.\nComment obtenir les variations des données ? Cela dépend du type de données, qui peuvent être du texte, des images ou des données tabulaires. Pour le texte et les images, la solution consiste à activer ou désactiver des mots simples ou des super-pixels. Dans le cas de données tabulaires, LIME crée de nouveaux échantillons en perturbant chaque entité individuellement, en s’appuyant sur une distribution normale avec moyenne et écart type tirés de l’entité.",
    "crumbs": [
      "9 - Méthodes locales indépendantes du modèle",
      "9.2 - Substitut local (LIME)"
    ]
  },
  {
    "objectID": "09-local_model_agnostic_methods/09.2-lime.html#footnotes",
    "href": "09-local_model_agnostic_methods/09.2-lime.html#footnotes",
    "title": "9.2.1 - LIME pour les données tabulaires",
    "section": "Notes de bas de page",
    "text": "Notes de bas de page\n\n\nAlvarez-Melis, David, and Tommi S. Jaakkola. “On the robustness of interpretability methods.” arXiv preprint arXiv:1806.08049 (2018).↩︎\nSlack, Dylan, Sophie Hilgard, Emily Jia, Sameer Singh, and Himabindu Lakkaraju. “Fooling lime and shap: Adversarial attacks on post hoc explanation methods.” In Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society, pp. 180-186 (2020).↩︎",
    "crumbs": [
      "9 - Méthodes locales indépendantes du modèle",
      "9.2 - Substitut local (LIME)"
    ]
  },
  {
    "objectID": "08-global_model_agnostic_methods/08.7-prototype-criticisms.html",
    "href": "08-global_model_agnostic_methods/08.7-prototype-criticisms.html",
    "title": "Apprentissage automatique interprétable",
    "section": "",
    "text": "Avertissement\n\n\n\nEn cours de traduction.",
    "crumbs": [
      "8 - Méthodes globales indépendantes du modèle",
      "8.7 - Prototypes et critiques"
    ]
  },
  {
    "objectID": "08-global_model_agnostic_methods/08.7-prototype-criticisms.html#prototypes-et-critiques",
    "href": "08-global_model_agnostic_methods/08.7-prototype-criticisms.html#prototypes-et-critiques",
    "title": "Apprentissage automatique interprétable",
    "section": "8.7 - Prototypes et critiques",
    "text": "8.7 - Prototypes et critiques\nUn prototype est une instance de données représentative de toutes les données. Une critique est une instance de données qui n’est pas bien représentée par l’ensemble des prototypes. Le but des critiques est de fournir des informations avec des prototypes, en particulier pour les points de données que les prototypes ne représentent pas bien. Les prototypes et les critiques peuvent être utilisés indépendamment d’un modèle d’apprentissage automatique pour décrire les données, mais ils peuvent également être utilisés pour créer un modèle interprétable ou pour rendre interprétable un modèle de boîte noire.\nDans ce chapitre, j’utilise l’expression “point de données” pour faire référence à une instance unique, afin de souligner l’interprétation selon laquelle une instance est également un point dans un système de coordonnées où chaque entité est une dimension. La figure suivante montre une distribution de données simulée, avec certaines instances choisies comme prototypes et d’autres comme critiques. Les petits points sont les données, les grands points les critiques et les grands carrés les prototypes. Les prototypes sont sélectionnés (manuellement) pour couvrir les centres de distribution des données et les critiques sont des points dans un cluster sans prototype. Les prototypes et les critiques sont toujours des instances réelles issues des données.\n\n\n\nPrototypes et critiques pour une distribution de données à deux fonctionnalités x1 et x2.\n\n\nJ’ai sélectionné les prototypes manuellement, ce qui ne s’adapte pas bien et conduit probablement à de mauvais résultats. Il existe de nombreuses approches pour trouver des prototypes dans les données. L’un d’eux est k-medoids, un algorithme de clustering lié à l’algorithme k-means. Tout algorithme de clustering qui renvoie des points de données réels en tant que centres de cluster serait éligible à la sélection de prototypes. Mais la plupart de ces méthodes ne trouvent que des prototypes, mais aucune critique. Ce chapitre présente le critique MMD de Kim et al. (2016)1, une approche qui combine prototypes et critiques dans un cadre unique.\nMMD-critic compare la répartition des données et la répartition des prototypes sélectionnés. C’est le concept central pour comprendre la méthode critique MMD. MMD-critic sélectionne des prototypes qui minimisent l’écart entre les deux distributions. Les points de données situés dans des zones à forte densité constituent de bons prototypes, en particulier lorsque les points sont sélectionnés dans différents « groupes de données ». Les points de données provenant de régions qui ne sont pas bien expliqués par les prototypes sont sélectionnés comme critiques.\nApprofondissons la théorie.\n\n8.7.1 - Théorie\nLa procédure critique du MMD à un niveau élevé peut être résumée brièvement :\n\nSélectionnez le nombre de prototypes et de critiques que vous souhaitez retrouver.\nTrouvez des prototypes avec une recherche gourmande. Les prototypes sont sélectionnés de manière à ce que la répartition des prototypes soit proche de la répartition des données.\nTrouvez des critiques grâce à une recherche gourmande. Les points sont sélectionnés comme critiques lorsque la répartition des prototypes diffère de la répartition des données.\n\nNous avons besoin de quelques ingrédients pour trouver des prototypes et des critiques pour un ensemble de données avec MMD-critique. En tant qu’ingrédient le plus élémentaire, nous avons besoin d’une fonction noyau pour estimer les densités de données. Un noyau est une fonction qui pondère deux points de données en fonction de leur proximité. Sur la base des estimations de densité, nous avons besoin d’une mesure qui nous indique à quel point deux distributions sont différentes afin de pouvoir déterminer si la distribution des prototypes que nous sélectionnons est proche de la distribution des données. Ceci est résolu en mesurant l’écart moyen maximum (MMD). Également basé sur la fonction noyau, nous avons besoin de la fonction témoin pour nous indiquer à quel point deux distributions sont différentes à un point de données particulier. Avec la fonction témoin, nous pouvons sélectionner des critiques, c’est-à-dire des points de données pour lesquels la répartition des prototypes et des données diverge et la fonction témoin prend de grandes valeurs absolues. Le dernier ingrédient est une stratégie de recherche de bons prototypes et de critiques, qui est résolue par une simple recherche gourmande.\nCommençons par l’écart moyen maximum (MMD), qui mesure l’écart entre deux distributions. La sélection des prototypes crée une répartition de densité des prototypes. Nous voulons évaluer si la distribution des prototypes diffère de la distribution des données. Nous estimons les deux avec des fonctions de densité de noyau. L’écart moyen maximum mesure la différence entre deux distributions, qui est le supremum sur un espace fonctionnel des différences entre les attentes selon les deux distributions. Tout est clair? Personnellement, je comprends beaucoup mieux ces concepts quand je vois comment quelque chose est calculé avec des données. La formule suivante montre comment calculer la mesure MMD au carré (MMD2) :\n\\[MMD^2=\\frac{1}{m^2}\\sum_{i,j=1}^m{}k(z_i,z_j)-\\frac{2}{mn}\\sum_{i,j=1}^{m,n}k(z_i,x_j)+\\frac{1}{n^2}\\sum_{i,j=1}^n{}k(x_i,x_j)\\]\n\\(k\\) est une fonction noyau qui mesure la similarité de deux points, mais nous en reparlerons plus tard. \\(m\\) est le nombre de prototypes \\(z\\) et \\(n\\) est le nombre de points de données \\(x\\) dans notre ensemble de données d’origine. Les prototypes \\(z\\) sont une sélection de points de données \\(x\\). Chaque point est multidimensionnel, c’est-à-dire qu’il peut avoir plusieurs caractéristiques. Le but de MMD-critic est de minimiser MMD2. Plus MMD2 est proche de zéro, plus la distribution des prototypes s’adapte aux données. La clé pour ramener MMD2 à zéro est le terme au milieu, qui calcule la proximité moyenne entre les prototypes et tous les autres points de données (multipliée par 2). Si ce terme s’additionne au premier terme (la proximité moyenne des prototypes les uns par rapport aux autres) plus le dernier terme (la proximité moyenne des points de données les uns par rapport aux autres), alors les prototypes expliquent parfaitement les données. Essayez ce qui arriverait à la formule si vous utilisiez les \\(n\\) points de données comme prototypes.\nLe graphique suivant illustre la mesure MMD2. Le premier graphique montre les points de données avec deux caractéristiques, l’estimation de la densité des données étant affichée sur un arrière-plan ombré. Chacune des autres parcelles montre différentes sélections de prototypes, ainsi que la mesure MMD2 dans les titres des parcelles. Les prototypes sont les gros points et leur répartition est représentée par des courbes de niveau. La sélection des prototypes qui couvrent le mieux les données dans ces scénarios (en bas à gauche) présente la valeur d’écart la plus faible.\n\n\n\nThe squared maximum mean discrepancy measure (MMD2) for a dataset with two features and different selections of prototypes.\n\n\nUn choix pour le noyau est le noyau de fonction de base radiale : \\[k(x,x^\\prime) = exp\\left(-\\gamma\\|x-x^\\prime\\|^2\\right)\\]\noù \\(\\|x-x^\\prime\\|^2\\) st la distance euclidienne entre deux points et \\(\\gamma\\) est un paramètre de mise à l’échelle. La valeur du noyau décroît avec la distance entre les deux points et est comprise entre zéro et un : Zéro lorsque les deux points sont infiniment éloignés l’un de l’autre ; un lorsque les deux points sont égaux.\nNous combinons la mesure MMD2, la recherche noyau et glouton dans un algorithme de recherche de prototypes :\n\nCommencer avec une liste vide de prototypes.\nAlors que le nombre de prototypes est inférieur au nombre m choisi :\n\nPour chaque point de l’ensemble de données, vérifiez de combien MMD2 est réduit lorsque le point est ajouté à la liste des prototypes. Ajoutez le point de données qui minimise le MMD2 à la liste.\n\nRenvoie la liste des prototypes.\n\nLe dernier ingrédient permettant de formuler des critiques est la fonction témoin, qui nous indique dans quelle mesure deux estimations de densité diffèrent en un point particulier.\nOn peut l’estimer à l’aide de : \\[witness(x) = \\frac{1}{n} \\sum_{i=1}^n k(x, x_i) - \\frac{1}{m} \\sum_{j=1}^m k(x, z_j)\\]\nPour deux ensembles de données (avec les mêmes caractéristiques), la fonction témoin vous donne le moyen d’évaluer dans quelle distribution empirique le point \\(x\\) s’adapte le mieux. Pour trouver des critiques, nous recherchons des valeurs extrêmes de la fonction témoin dans les directions négative et positive. Le premier terme de la fonction témoin est la proximité moyenne entre le point \\(x\\) et les données, et respectivement le deuxième terme est la proximité moyenne entre le point \\(x\\) et les prototypes. Si la fonction témoin pour un point \\(x\\) est proche de zéro, la fonction de densité des données et des prototypes sont proches, ce qui signifie que la distribution des prototypes ressemble à la distribution des données au point \\(x\\). Une fonction témoin négative au point \\(x\\) signifie que la distribution du prototype surestime la distribution des données (par exemple si nous sélectionnons un prototype mais qu’il n’y a que quelques points de données à proximité); une fonction témoin positive au point \\(x\\) signifie que la distribution du prototype sous-estime la distribution des données (par exemple s’il y a de nombreux points de données autour de \\(x\\) mais que nous n’avons sélectionné aucun prototype à proximité).\nPour vous donner plus d’intuition, réutilisons les prototypes du tracé au préalable avec le MMD2 le plus bas et affichons la fonction témoin pour quelques points sélectionnés manuellement. Les étiquettes du tracé suivant montrent la valeur de la fonction témoin pour différents points marqués sous forme de triangles. Seul le point central a une valeur absolue élevée et constitue donc un bon candidat pour une critique.\n\n\n\nÉvaluations de la fonction témoin en différents points.\n\n\nLa fonction témoin nous permet de rechercher explicitement des instances de données qui ne sont pas bien représentées par les prototypes. Les critiques sont des points à haute valeur absolue dans la fonction de témoin. Comme pour les prototypes, les critiques sont également trouvées grâce à des recherches gourmandes. Mais au lieu de réduire le MMD2 global, nous recherchons des points qui maximisent une fonction de coût incluant la fonction témoin et un terme régularisateur. Le terme supplémentaire dans la fonction d’optimisation impose la diversité des points, ce qui est nécessaire pour que les points proviennent de différents clusters.\nCette deuxième étape est indépendante de la manière dont les prototypes sont trouvés. J’aurais également pu sélectionner quelques prototypes et utiliser la procédure décrite ici pour apprendre les critiques. Ou bien les prototypes pourraient provenir de n’importe quelle procédure de clustering, comme les k-médoïdes.\nVoilà pour les parties importantes de la théorie critique du MMD. Une question demeure : comment utiliser MMD-critic pour un apprentissage automatique interprétable ?\nMMD-critic peut ajouter de l’interprétabilité de trois manières : en aidant à mieux comprendre la distribution des données ; en construisant un modèle interprétable ; en rendant interprétable un modèle de boîte noire.\nSi vous appliquez MMD-critic à vos données pour trouver des prototypes et des critiques, cela améliorera votre compréhension des données, surtout si vous avez une distribution de données complexe avec des cas extrêmes. Mais avec MMD-critic, vous pouvez faire plus !\nPar exemple, vous pouvez créer un modèle de prédiction interprétable : ce que l’on appelle le « modèle prototype le plus proche ». La fonction de prédiction est définie comme :\n\\[\\hat{f}(x)=argmax_{i\\in{}S}k(x,x_i)\\]\nce qui signifie que nous sélectionnons le prototype \\(i\\) parmi l’ensemble de prototypes \\(S\\) le plus proche du nouveau point de données, dans le sens où il donne la valeur la plus élevée de la fonction noyau. Le prototype lui-même est renvoyé comme explication de la prédiction. Cette procédure comporte trois paramètres de réglage : le type de noyau, le paramètre de mise à l’échelle du noyau et le nombre de prototypes. Tous les paramètres peuvent être optimisés dans une boucle de validation croisée. Les critiques ne sont pas utilisées dans cette approche.\nComme troisième option, nous pouvons utiliser MMD-critic pour rendre n’importe quel modèle d’apprentissage automatique globalement explicable en examinant les prototypes et les critiques ainsi que leurs prédictions de modèle. La procédure est la suivante :\n\nRetrouvez des prototypes et des critiques avec MMD-critic.\nEntraînez un modèle d’apprentissage automatique comme d’habitude.\nPrédisez les résultats des prototypes et des critiques avec le modèle d’apprentissage automatique.\nAnalysez les prédictions : dans quels cas l’algorithme s’est-il trompé ? Vous disposez désormais d’un certain nombre d’exemples qui représentent bien les données et vous aident à trouver les faiblesses du modèle d’apprentissage automatique.\n\nComment est-ce que ça aide ? Vous souvenez-vous de l’époque où le classificateur d’images de Google identifiait les Noirs comme des gorilles ? Peut-être auraient-ils dû utiliser la procédure décrite ici avant de déployer leur modèle de reconnaissance d’images. Il ne suffit pas de vérifier les performances du modèle, car si celui-ci était correct à \\(99%\\), ce problème pourrait encore se situer dans les \\(1%\\). Et les étiquettes peuvent aussi être fausses ! Passer en revue toutes les données d’entraînement et effectuer un contrôle d’intégrité si la prédiction est problématique aurait pu révéler le problème, mais serait irréalisable. Mais la sélection de – disons quelques milliers – de prototypes et de critiques est réalisable et aurait pu révéler un problème avec les données : elle aurait pu montrer qu’il y a un manque d’images de personnes à la peau foncée, ce qui indique un problème avec la diversité des individus. l’ensemble de données. Ou bien il aurait pu montrer une ou plusieurs images d’une personne à la peau foncée comme prototype ou (probablement) comme critique avec la fameuse classification de « gorille ». Je ne promets pas que les critiques de MMD intercepteraient certainement ce genre d’erreurs, mais c’est un bon test de bon sens.\n\n\n8.7.2 - Exemples\nL’exemple suivant de critique MMD utilise un ensemble de données de chiffres manuscrits.\nEn regardant les prototypes réels, vous remarquerez peut-être que le nombre d’images par chiffre est différent. En effet, un nombre fixe de prototypes ont été recherchés dans l’ensemble de données et non avec un nombre fixe par classe. Comme prévu, les prototypes montrent différentes manières d’écrire les chiffres.\n\n\n\nPrototypes pour un ensemble de données de chiffres manuscrits.\n\n\n\n\n8.7.3 - Avantages\nDans une étude utilisateur, les auteurs de MMD-critic ont donné des images aux participants, qu’ils ont dû faire correspondre visuellement à l’un des deux ensembles d’images, chacun représentant l’une des deux classes (par exemple deux races de chiens). Les participants ont obtenu de meilleurs résultats lorsque les décors montraient des prototypes et des critiques plutôt que des images aléatoires d’une classe.\nVous êtes libre de choisir le nombre de prototypes et de critiques.\nMMD-critic fonctionne avec des estimations de densité des données. Cela fonctionne avec tout type de données et tout type de modèle d’apprentissage automatique.\nL’algorithme est facile à mettre en oeuvre.\nMMD-critic est très flexible dans la manière dont il est utilisé pour augmenter l’interprétabilité. Il peut être utilisé pour comprendre des distributions de données complexes. Il peut être utilisé pour créer un modèle d’apprentissage automatique interprétable. Ou encore, cela peut éclairer la prise de décision d’un modèle d’apprentissage automatique en boîte noire.\nLa recherche de critiques est indépendante du processus de sélection des prototypes. Mais il est logique de sélectionner les prototypes en fonction des critiques MMD, car les prototypes et les critiques sont alors créés en utilisant la même méthode de comparaison des prototypes et des densités de données.\n\n\n8.7.4 - Inconvénients\nSi, mathématiquement, les prototypes et les critiques sont définis différemment, leur distinction repose sur une valeur seuil (le nombre de prototypes). Supposons que vous choisissiez un nombre trop faible de prototypes pour couvrir la distribution des données. Les critiques aboutiraient dans des domaines qui ne sont pas très bien expliqués. Mais si vous deviez ajouter davantage de prototypes, ils se retrouveraient également dans les mêmes zones. Toute interprétation doit tenir compte du fait que les critiques dépendent fortement des prototypes existants et de la valeur seuil (arbitraire) du nombre de prototypes.\nIl faut choisir le nombre de prototypes et de critiques. Même si cela peut être agréable, c’est aussi un inconvénient. De combien de prototypes et de critiques avons-nous réellement besoin ? Plus il y en a, mieux c’est ? Moins il y en a, mieux c’est ? Une solution consiste à sélectionner le nombre de prototypes et de critiques en mesurant le temps dont disposent les humains pour regarder les images, qui dépend de l’application particulière. Ce n’est qu’en utilisant MMD-critic pour créer un classificateur que nous avons un moyen de l’optimiser directement. Une solution pourrait être un scénario montrant le nombre de prototypes sur l’axe des x et la mesure MMD2 sur l’axe des y. Nous choisirions le nombre de prototypes où la courbe MMD2 s’aplatit.\nLes autres paramètres sont le choix du noyau et le paramètre de mise à l’échelle du noyau. Nous avons le même problème qu’avec le nombre de prototypes et les critiques : comment sélectionner un noyau et son paramètre de mise à l’échelle ? Encore une fois, lorsque nous utilisons MMD-critic comme classificateur de prototype le plus proche, nous pouvons ajuster les paramètres du noyau. Cependant, pour les cas d’utilisation non supervisés de MMD-critique, ce n’est pas clair. (Peut-être que je suis un peu dur ici, puisque toutes les méthodes non supervisées ont ce problème.)\nIl prend toutes les caractéristiques en entrée, sans tenir compte du fait que certaines caractéristiques pourraient ne pas être pertinentes pour prédire le résultat qui nous intéresse. Une solution consiste à utiliser uniquement les fonctionnalités pertinentes, par exemple les intégrations d’images au lieu des pixels bruts. Cela fonctionne tant que nous disposons d’un moyen de projeter l’instance d’origine sur une représentation qui ne contient que des informations pertinentes.\nIl existe du code disponible, mais il n’est pas encore implémenté comme un logiciel bien emballé et documenté.\n\n\n8.7.5 - Code et alternatives\nUne implémentation de MMD-critic peut être trouvée dans le référentiel GitHub des auteurs.\nRécemment une extension de MMD-critic a été développée : Protodash. Les auteurs revendiquent des avantages par rapport au critique MMD dans leur publication. Une implémentation Protodash est disponible dans l’ outil IBM AIX360.\nL’alternative la plus simple à la recherche de prototypes est celle des k-médoïdes de Kaufman et al. (1987)2.",
    "crumbs": [
      "8 - Méthodes globales indépendantes du modèle",
      "8.7 - Prototypes et critiques"
    ]
  },
  {
    "objectID": "08-global_model_agnostic_methods/08.7-prototype-criticisms.html#footnotes",
    "href": "08-global_model_agnostic_methods/08.7-prototype-criticisms.html#footnotes",
    "title": "Apprentissage automatique interprétable",
    "section": "Notes de bas de page",
    "text": "Notes de bas de page\n\n\nKim, Been, Rajiv Khanna, and Oluwasanmi O. Koyejo. “Examples are not enough, learn to criticize! Criticism for interpretability.” Advances in Neural Information Processing Systems (2016).↩︎\nKaufman, Leonard, and Peter Rousseeuw. “Clustering by means of medoids”. North-Holland (1987).↩︎",
    "crumbs": [
      "8 - Méthodes globales indépendantes du modèle",
      "8.7 - Prototypes et critiques"
    ]
  },
  {
    "objectID": "08-global_model_agnostic_methods/08.4-functional-decomposition.html",
    "href": "08-global_model_agnostic_methods/08.4-functional-decomposition.html",
    "title": "Apprentissage automatique interprétable",
    "section": "",
    "text": "Avertissement\n\n\n\nEn cours de traduction.",
    "crumbs": [
      "8 - Méthodes globales indépendantes du modèle",
      "8.4 - Functional Decomposition"
    ]
  },
  {
    "objectID": "08-global_model_agnostic_methods/08.4-functional-decomposition.html#functional-decomposition",
    "href": "08-global_model_agnostic_methods/08.4-functional-decomposition.html#functional-decomposition",
    "title": "Apprentissage automatique interprétable",
    "section": "8.4 - Functional Decomposition",
    "text": "8.4 - Functional Decomposition\nUn modèle d’apprentissage automatique supervisé peut être considéré comme une fonction qui prend un vecteur de caractéristiques de grande dimension en entrée et produit un score de prédiction ou de classification en sortie. La décomposition fonctionnelle est une technique d’interprétation qui déconstruit la fonction de grande dimension et l’exprime comme une somme d’effets de caractéristiques individuelles et d’effets d’interaction pouvant être visualisés. De plus, la décomposition fonctionnelle est un principe fondamental qui sous-tend de nombreuses techniques d’interprétation : elle vous aide à mieux comprendre les autres méthodes d’interprétation.\nAllons droit au but et examinons une fonction particulière. Cette fonction prend deux fonctionnalités en entrée et produit une sortie unidimensionnelle :\n\\[y = \\hat{f}(x_1, x_2) = 2 + e^{x_1} - x_2 + x_1 \\cdot x_2\\]\nConsidérez la fonction comme un modèle d’apprentissage automatique. Nous pouvons visualiser la fonction avec un tracé 3D ou une carte thermique avec des courbes de niveau :\n\n\n\nSurface de prédiction d’une fonction à deux caractéristiques \\(x_1\\) et \\(x_2\\)\n\n\nLa fonction prend de grandes valeurs lorsque \\(x_1\\) est grand et \\(x_2\\) est petit, et il faut de petites valeurs pour de grandes \\(x_2\\) et petit \\(x_1\\). La fonction de prédiction n’est pas simplement un effet additif entre les deux caractéristiques, mais une interaction entre les deux. La présence d’une interaction est visible sur la figure - l’effet de la modification des valeurs de la fonctionnalité \\(x_1\\) dépend de la valeur de cette fonctionnalité \\(x_2\\) a.\nNotre travail consiste maintenant à décomposer cette fonction en effets principaux de fonctionnalités \\(x_1\\) et \\(x_2\\) et un terme d’interaction. Pour une fonction bidimensionnelle \\(\\hat{f}\\), cela dépend de seulement deux fonctionnalités d’entrée : \\(\\hat{f}(x_1, x_2)\\), nous voulons que chaque composant représente un effet principal (\\(\\hat{f}_1\\) et \\(\\hat{f}_2\\)), interaction (\\(\\hat{f}_{1,2}\\)) ou intercepter (\\(\\hat{f}_0\\)):\n\\[\\hat{f}(x_1, x_2) = \\hat{f}_0 + \\hat{f}_1(x_1) + \\hat{f}_2(x_2) + \\hat{f}_{1,2}(x_{1},x_{2})\\]\nLes principaux effets indiquent comment chaque caractéristique affecte la prédiction, indépendamment des valeurs de l’autre caractéristique. L’effet d’interaction indique l’effet conjoint des caractéristiques. L’interception nous indique simplement quelle est la prédiction lorsque tous les effets de caractéristiques sont définis sur zéro. Notez que les composants eux-mêmes sont des fonctions (à l’exception de l’interception) avec une dimensionnalité d’entrée différente.\n\n\n\nDécomposition d’une fonction.\n\n\nPensez-vous que les composants ont du sens étant donné la vraie formule ci-dessus, en ignorant le fait que la valeur d’origine semble un peu aléatoire ? Le \\(x_1\\) la fonctionnalité montre un effet principal exponentiel, et \\(x_2\\) montre un effet linéaire négatif. Le terme d’interaction ressemble un peu à une puce Pringles. En termes moins croustillants et plus mathématiques, il s’agit d’un paraboloïde hyperbolique, comme on pourrait s’y attendre pour \\(x_1 \\cdot x_2\\). Alerte dévulgachage : la décomposition est basée sur des tracés d’effets locaux accumulés, dont nous parlerons plus tard dans le chapitre.\n\n8.4.1 - Comment ne pas calculer les composantes I\nMais pourquoi tout cet engouement ? Un coup d’œil à la formule nous donne déjà la réponse à la décomposition, donc pas besoin de méthodes sophistiquées, n’est-ce pas ? Pour la fonctionnalité \\(x_1\\), on peut prendre toutes les sommes qui contiennent uniquement \\(x_1\\) comme composant de cette fonctionnalité. Ce serait \\(\\hat{f}_1(x_1) = e^{x_1}\\) et \\(\\hat{f}_2(x_2) = -x_2\\) pour la fonctionnalité \\(x_2\\). L’interaction est alors \\(\\hat{f}_{12}(x_{1},x_{2}) = x_1 \\cdot x_2\\). Bien que ce soit la bonne réponse pour cet exemple (jusqu’aux constantes), cette approche pose deux problèmes : Problème 1) : Bien que l’exemple ait commencé avec la formule, la réalité est que presque aucun modèle d’apprentissage automatique ne peut être décrit avec une telle formule. formule soignée. Le problème 2) est beaucoup plus complexe et concerne ce qu’est une interaction. Imaginez une fonction simple \\(\\hat{f}(x_1,x_2) = x_1 \\cdot x_2\\), où les deux caractéristiques prennent des valeurs supérieures à zéro et sont indépendantes l’une de l’autre. En utilisant notre tactique d’examen de la formule, nous conclurions qu’il existe une interaction entre les fonctionnalités \\(x_1\\) et \\(x_2\\), mais pas les effets de fonctionnalités individuelles. Mais peut-on vraiment dire que cette fonctionnalité \\(x_1\\) n’a-t-il aucun effet individuel sur la fonction de prédiction ? Quelle que soit la valeur de l’autre fonctionnalité \\(x_2\\) prend, la prédiction augmente à mesure que nous augmentons \\(x_1\\). Par exemple, pour \\(x_2 = 1\\), l’effet de \\(x_1\\) est \\(\\hat{f}(x_1, 1) = x_1\\), et quand \\(x_2 = 10\\) l’effet est \\(\\hat{f}(x_1, 10) = 10 \\cdot x_1\\). Il est donc clair que cette fonctionnalité \\(x_1\\) a un effet positif sur la prédiction, indépendamment de \\(x_2\\), et n’est pas nul.\nPour résoudre le problème 1) du manque d’accès à une formule soignée, nous avons besoin d’une méthode qui utilise uniquement la fonction de prédiction ou le score de classification. Pour résoudre le problème 2) du manque de définition, nous avons besoin de quelques axiomes qui nous indiquent à quoi devraient ressembler les composants et comment ils sont liés les uns aux autres. Mais d’abord, il convient de définir plus précisément ce qu’est la décomposition fonctionnelle.\n\n\n8.4.2 - Décomposition fonctionnelle\nUne fonction de prédiction prend \\(p\\) fonctionnalités en entrée, \\(\\hat{f}: \\mathbb{R}^p \\mapsto \\mathbb{R}\\) et produit une sortie. Cela peut être une fonction de régression, mais cela peut aussi être la probabilité de classification pour une classe donnée ou le score pour un cluster donné (apprentissage automatique non supervisé). Entièrement décomposée, nous pouvons représenter la fonction de prédiction comme la somme de composants fonctionnels :\n\\[\\begin{align*}\n\\hat{f}(x) = & \\hat{f}_0 + \\hat{f}_1(x_1) + \\ldots + \\hat{f}_p(x_p) \\\\\n& + \\hat{f}_{1,2}(x_1, x_2) + \\ldots + \\hat{f}_{1,p}(x_1, x_p) + \\ldots + \\hat{f}_{p-1,p}(x_{p-1}, x_p) \\\\\n& + \\ldots  \\\\ & +  \\hat{f}_{1,\\ldots,p}(x_1, \\ldots, x_p)\n\\end{align*}\\]\nNous pouvons rendre la formule de décomposition un peu plus agréable en indexant tous les sous-ensembles possibles de combinaisons de fonctionnalités : \\(S\\subseteq\\{1,\\ldots,p\\}\\). Cet ensemble contient l’interception (\\(S=\\emptyset\\)), principaux effets (\\(|S|=1\\)), et toutes les interactions (\\(|S|\\geq{}1\\)). Avec ce sous-ensemble défini, nous pouvons écrire la décomposition comme suit :\n\\[\\hat{f}(x) = \\sum_{S\\subseteq\\{1,\\ldots,p\\}} \\hat{f}_S(x_S)\\]\nDans la formule, \\(x_S\\) est le vecteur des caractéristiques dans l’ensemble d’index \\(S\\). Et chaque sous-ensemble \\(S\\) représente un composant fonctionnel, par exemple un effet principal si S ne contient qu’une seule fonctionnalité, ou une interaction si \\(|S| &gt; 1\\).\nCombien de composants y a-t-il dans la formule ci-dessus ? La réponse se résume au nombre de sous-ensembles possibles \\(S\\) des fonctionnalités \\(1,\\ldots, p\\) nous pouvons former. Et ce sont \\(\\sum_{i=0}^p\\binom{p}{i}=2^p\\) sous-ensembles possibles ! Par exemple, si une fonction utilise 10 fonctionnalités, nous pouvons décomposer la fonction en 1042 composants : 1 intercept, 10 effets principaux, 90 termes d’interaction à 2 voies, 720 termes d’interaction à 3 voies, … Et avec chaque fonctionnalité supplémentaire, le nombre de composants double. De toute évidence, pour la plupart des fonctions, il n’est pas possible de calculer toutes les composantes. Une autre raison de NE PAS calculer toutes les composantes est que les composantes avec \\(|S|&gt;2\\) sont difficiles à visualiser et à interpréter.\n\n\n8.4.3 - Comment ne pas calculer les composantes II\nJusqu’à présent, j’ai évité de parler de la façon dont les composants sont définis et calculés. Les seules contraintes dont nous avons implicitement parlé étaient le nombre et la dimensionnalité des composants, et le fait que la somme des composants devait produire la fonction d’origine. Mais sans autres contraintes sur ce que devraient être les composants, ils ne sont pas uniques. Cela signifie que nous pourrions déplacer les effets entre les effets principaux et les interactions, ou entre les interactions d’ordre inférieur (peu de fonctionnalités) et les interactions d’ordre supérieur (plus de fonctionnalités). Dans l’exemple du début du chapitre, nous pourrions mettre les deux effets principaux à zéro et ajouter leurs effets à l’effet d’interaction.\nVoici un exemple encore plus extrême qui illustre la nécessité de contraintes sur les composants. Supposons que vous ayez une fonction tridimensionnelle. L’apparence de cette fonction n’a pas vraiment d’importance, mais la décomposition suivante fonctionnerait toujours : \\(\\hat{f}_0\\) is 0.12. \\(\\hat{f}_1(x_1)=2\\cdot{}x_1\\) + nombre de chaussures que vous possédez. \\(\\hat{f}_2\\), \\(\\hat{f}_3\\), \\(\\hat{f}_{1,2}\\), \\(\\hat{f}_{2,3}, \\hat{f}_{1,3}\\) sont tous nuls. Et pour que cette astuce fonctionne, je définis \\(\\hat{f}_{1,2,3}(x_1,x_2,x_3)=\\hat{f}(x)-\\sum_{S\\subset\\{1,\\ldots,p\\}}\\hat{f}_S(x_S)\\). Ainsi, le terme d’interaction contenant toutes les caractéristiques aspire simplement tous les effets restants, ce qui, par définition, fonctionne toujours, dans le sens où la somme de toutes les composantes nous donne la fonction de prédiction originale. Cette décomposition ne serait pas très significative et assez trompeuse si vous deviez la présenter comme l’interprétation de votre modèle.\nL’ambiguïté peut être évitée en spécifiant des contraintes supplémentaires ou des méthodes spécifiques de calcul des composants. Dans ce chapitre, nous aborderons trois méthodes qui abordent la décomposition fonctionnelle de différentes manières : - ANOVA fonctionnelle (généralisée) - Effets locaux accumulés - Modèles de régression statistique\n\n\n8.4.4 - ANOVA fonctionnelle\nL’ANOVA fonctionnelle a été proposée par Hooker (2004)1. Une condition requise pour cette approche est que la fonction de prédiction du modèle \\(\\hat{f}\\) est carré intégrable. Comme pour toute décomposition fonctionnelle, l’ANOVA fonctionnelle décompose la fonction en composants :\n\\[\\hat{f}(x) = \\sum_{S\\subseteq\\{1,\\ldots,p\\}} \\hat{f}_S(x_S)\\]\nHooker (2004) définit chaque composant avec la formule suivante :\n\\[\\hat{f}_S(x) = \\int_{X_{-S}} \\left( \\hat{f}(x) - \\sum_{V \\subset S} \\hat{f}_V(x)\\right) d X_{-S}\\]\nD’accord, démontons cette chose. Nous pouvons réécrire le composant comme suit :\n\\[\\hat{f}_S(x) = \\int_{X_{-S}} \\left( \\hat{f}(x)\\right) d X_{-S} - \\int_{X_{-S}} \\left(\\sum_{V \\subset S} \\hat{f}_V(x) \\right) d X_{-S}\\]\nSur le côté gauche se trouve l’intégrale sur la fonction de prédiction par rapport aux caractéristiques exclues de l’ensemble \\(S\\), noté par \\(-S\\). Par exemple, si nous calculons la composante d’interaction bidirectionnelle pour les caractéristiques 2 et 3, nous intégrerions les caractéristiques 1, 4, 5,… L’intégrale peut également être considérée comme la valeur attendue de la fonction de prédiction par rapport à \\(X_{-S}\\), en supposant que toutes les caractéristiques suivent une distribution uniforme de leur minimum à leur maximum. De cet intervalle, nous soustrayons toutes les composantes avec des sous-ensembles de \\(S\\). Cette soustraction supprime l’effet de tous les effets d’ordre inférieur et centre l’effet. Pour \\(S=\\{1,2\\}\\), nous soustrayons les principaux effets des deux caractéristiques \\(\\hat{f}_1\\) eT \\(\\hat{f}_2\\), ainsi que l’interception \\(\\hat{f}_0\\). L’apparition de ces effets d’ordre inférieur rend la formule récursive : nous devons parcourir la hiérarchie des sous-ensembles pour intercepter et calculer toutes ces composantes. Pour le composant d’interception \\(\\hat{f}_0\\), le sous-ensemble est l’ensemble vide \\(S=\\{\\emptyset\\}\\) et donc \\(-S\\) contient toutes les fonctionnalités :\n\\[\\hat{f}_0(x) = \\int_{X} \\hat{f}(x) dX\\]\nIl s’agit simplement de la fonction de prédiction intégrée sur toutes les fonctionnalités. L’ordonnée à l’origine peut également être interprétée comme l’attente de la fonction de prédiction lorsque nous supposons que toutes les caractéristiques sont uniformément distribuées. Maintenant que nous savons \\(\\hat{f}_0\\), on peut calculer \\(\\hat{f}_1\\) (et de manière équivalente \\(\\hat{f}_2\\)) :\n\\[\\hat{f}_1(x) = \\int_{X_{-1}} \\left( \\hat{f}(x) - \\hat{f}_0\\right) d X_{-S}\\]\nPour terminer le calcul du composant \\(\\hat{f}_{1,2}\\), nous pouvons tout assembler :\n\\[\\begin{align*}\\hat{f}_{1,2}(x) &= \\int_{X_{3,4}} \\left( \\hat{f}(x) - (\\hat{f}_0(x) + \\hat{f}_1(x) - \\hat{f}_0 + \\hat{f}_2(x) - \\hat{f}_0)\\right) d X_{3},X_4 \\\\  &= \\int_{X_{3,4}} \\left(\\hat{f}(x) - \\hat{f}_1(x) - \\hat{f}_2(x) + \\hat{f}_0\\right) d X_{3},X_4 \\end{align*}\\]\nCet exemple montre comment chaque effet d’ordre supérieur est défini en intégrant toutes les autres fonctionnalités, mais également en supprimant tous les effets d’ordre inférieur qui sont des sous-ensembles de l’ensemble de fonctionnalités qui nous intéresse.\nHooker (2004) a montré que cette définition des composants fonctionnels satisfait ces axiomes souhaitables :\n\nZéro signifie : \\(\\int{}\\hat{f}_S(x_S)dX_s=0\\) pour chaque \\(S\\neq\\emptyset\\).\nOrthogonalité : \\(\\int{}\\hat{f}_S(x_S)\\hat{f}_V(x_v)dX=0\\) pour \\(S\\neq{}V\\).\nDécomposition de la variance : Soit \\(\\sigma^2_{\\hat{f}}=\\int \\hat{f}(x)^2dX\\), alors \\(\\sigma^2(\\hat{f}) = \\sum_{S \\subseteq \\{1,\\ldots,p\\}} \\sigma^2_S(\\hat{f}_S)\\).\n\nL’axiome des moyens zéro implique que tous les effets ou interactions sont centrés autour de zéro. En conséquence, l’interprétation à une position x est relative à la prédiction centrée et non à la prédiction absolue.\nL’axiome d’orthogonalité implique que les composants ne partagent pas d’informations. Par exemple, l’effet de premier ordre de la fonctionnalité \\(X_1\\) et le terme d’interaction de \\(X_1\\) et \\(X_2\\) ne sont pas corrélées. En raison de l’orthogonalité, toutes les composantes sont « pures » dans le sens où elles ne mélangent pas les effets. Il est tout à fait logique que le composant destiné, par exemple, à la fonctionnalité \\(X_4\\) devrait être indépendant du terme d’interaction entre les fonctionnalités \\(X_1\\) et \\(X_2\\). La conséquence la plus intéressante concerne l’orthogonalité des composants hiérarchiques, où un composant contient des caractéristiques d’un autre, par exemple l’interaction entre \\(X_1\\) et \\(X_2\\) , et l’effet principal de la fonctionnalité \\(X_1\\). En revanche, un diagramme de dépendance partielle bidimensionnel pour \\(X_1\\) et \\(X_2\\) contiendrait quatre effets : l’interception, les deux effets principaux de \\(X_1\\) et \\(X_2\\) et l’interaction entre eux. Le composant ANOVA fonctionnel pour \\(\\hat{f}_{1,2}(x_1,x_2)\\) ne contient que l’interaction pure.\nLa décomposition de la variance nous permet de diviser la variance de la fonction \\(\\hat{f}\\) entre les composants, et garantit qu’il additionne finalement la variance totale de la fonction. La propriété de décomposition de la variance peut également nous expliquer pourquoi la méthode est appelée “ANOVA fonctionnelle”. En statistiques, ANOVA signifie ANalysis Of VAriance. L’ANOVA fait référence à un ensemble de méthodes qui analysent les différences dans la moyenne d’une variable cible. L’ANOVA fonctionne en divisant la variance et en l’attribuant aux variables. L’ANOVA fonctionnelle peut donc être considérée comme une extension de ce concept à n’importe quelle fonction.\nDes problèmes surviennent avec l’ANOVA fonctionnelle lorsque les caractéristiques sont corrélées. Comme solution, l’ANOVA fonctionnelle généralisée a été proposée.\n\n\n8.4.5 - ANOVA fonctionnelle généralisée pour les caractéristiques dépendantes\nSemblable à la plupart des techniques d’interprétation basées sur des données d’échantillonnage (telles que le PDP), l’ANOVA fonctionnelle peut produire des résultats trompeurs lorsque les caractéristiques sont corrélées. Si nous intégrons sur la distribution uniforme, alors qu’en réalité les caractéristiques sont dépendantes, nous créons un nouvel ensemble de données qui s’écarte de la distribution conjointe et extrapole à des combinaisons improbables de valeurs de caractéristiques.\nHooker (2007) 2 a proposé l’ANOVA fonctionnelle généralisée, une décomposition qui fonctionne pour les caractéristiques dépendantes. Il s’agit d’une généralisation de l’ANOVA fonctionnelle que nous avons rencontrée précédemment, ce qui signifie que l’ANOVA fonctionnelle est un cas particulier de l’ANOVA fonctionnelle généralisée. Les composantes sont définies comme des projections de f sur l’espace des fonctions additives :\n\\[\\hat{f}_S(x_S) = argmin_{g_S \\in L^2(\\mathbb{R}^S)_{S \\in P}} \\int \\left(\\hat{f}(x)  - \\sum_{S \\subset P} g_S(x_S)\\right)^2 w(x)dx.\\]\nAu lieu d’orthogonalité, les composants satisfont une condition d’orthogonalité hiérarchique :\n\\[\\forall \\hat{f}_S(x_S)| S \\subset U: \\int \\hat{f}_S(x_S) \\hat{f}_U(x_U) w(x)dx = 0\\]\nL’orthogonalité hiérarchique est différente de l’orthogonalité. Pour deux ensembles de caractéristiques S et U, dont aucun n’est le sous-ensemble de l’autre (par exemple \\(S=\\{1,2\\}\\) et \\(U=\\{2,3\\}\\)), les composants \\(\\hat{f}_S\\) et \\(\\hat{f}_U\\) n’ont pas besoin d’être orthogonaux pour que la décomposition soit hiérarchiquement orthogonale. Mais tous les composants de tous les sous-ensembles de \\(S\\) doit être orthogonal à \\(\\hat{f}_S\\). En conséquence, l’interprétation diffère de manière pertinente : à l’instar du M-Plot du chapitre ALE, les composants fonctionnels généralisés de l’ANOVA peuvent emmêler les effets (marginaux) des caractéristiques corrélées. La question de savoir si les composantes enchevêtrent les effets marginaux dépend également du choix de la fonction de pondération \\(w(x)\\). Si nous choisissons w comme mesure uniforme sur le cube unité, nous obtenons l’ANOVA fonctionnelle à partir de la section ci-dessus. Un choix naturel pour w est la fonction de distribution de probabilité conjointe. Cependant, la distribution conjointe est généralement inconnue et difficile à estimer. Une astuce peut être de commencer par la mesure uniforme sur le cube unité et de découper les zones sans données.\nL’estimation est effectuée sur une grille de points dans l’espace des caractéristiques et est présentée comme un problème de minimisation qui peut être résolu à l’aide de techniques de régression. Cependant, les composants ne peuvent pas être calculés individuellement, ni hiérarchiquement, mais un système complexe d’équations impliquant d’autres composants doit être résolu. Le calcul est donc assez complexe et gourmand en calcul.\n\n\n8.4.6 - Graphiques des effets locaux accumulés\nLes tracés ALE (Apley et Zhu 20203) fournissent également une décomposition fonctionnelle, ce qui signifie que l’ajout de tous les tracés ALE à l’origine, des tracés ALE 1D, des tracés ALE 2D, etc., donne la fonction de prédiction. L’ALE diffère de l’ANOVA fonctionnelle (généralisée), car les composantes ne sont pas orthogonales mais, comme l’appellent les auteurs, pseudo-orthogonales. Pour comprendre la pseudo-orthogonalité, il faut définir l’opérateur \\(H_S\\), qui prend une fonction \\(\\hat{f}\\) et le mappe à son tracé ALE pour le sous-ensemble de fonctionnalités \\(S\\). Par exemple, l’opérateur \\(H_{1,2}\\) prend en entrée un modèle d’apprentissage automatique et produit le tracé ALE 2D pour les fonctionnalités 1 et 2 : \\(H_{1,2}(\\hat{f}) = \\hat{f}_{ALE,12}\\). Si nous appliquons deux fois le même opérateur, nous obtenons le même tracé ALE. Après avoir appliqué l’opérateur \\(H_{1,2}\\) à \\(f\\) une fois, nous obtenons le tracé ALE 2D \\(\\hat{f}_{ALE,12}\\). Ensuite, nous appliquons à nouveau l’opérateur, non pas sur \\(f\\) mais sur \\(\\hat{f}_{ALE,12}\\). Ceci est possible car le composant 2D ALE est lui-même une fonction. Le résultat est encore une fois \\(\\hat{f}_{ALE,12}\\), ce qui signifie que nous pouvons appliquer le même opérateur plusieurs fois et obtenir toujours le même tracé ALE. C’est la première partie de la pseudo-orthogonalité. Mais quel est le résultat si nous appliquons deux opérateurs différents pour différents ensembles de fonctionnalités ? Par exemple, \\(H_{1,2}\\) et \\(H_{1}\\), ou \\(H_{1,2}\\) et \\(H_{3,4,5}\\) ? La réponse est zéro. Si l’on applique d’abord l’opérateur ALE \\(H_S\\) à une fonction puis appliquer \\(H_U\\) au résultat (avec \\(S \\neq U\\)), alors le résultat est nul. En d’autres termes, le tracé ALE d’un tracé ALE est nul, sauf si vous appliquez deux fois le même tracé ALE. Ou en d’autres termes, le tracé ALE pour l’ensemble de fonctionnalités S ne contient aucun autre tracé ALE. Ou en termes mathématiques, l’opérateur ALE mappe les fonctions aux sous-espaces orthogonaux d’un espace produit interne.\nComme le notent Apley et Zhu (2020), la pseudo-orthogonalité peut être plus souhaitable que l’orthogonalité hiérarchique, car elle n’enchevêtre pas les effets marginaux des caractéristiques. De plus, l’ALE ne nécessite pas d’estimation de la distribution conjointe ; les composantes peuvent être estimées de manière hiérarchique, ce qui signifie que le calcul de l’ALE 2D pour les caractéristiques 1 et 2 ne nécessite que les calculs des composantes ALE individuelles de 1 et 2 et du terme d’origine en plus.\n\n\n8.4.7 - Modèles de régression statistique\nCette approche s’inscrit dans le cadre de modèles interprétables, notamment les modèles additifs généralisés. Au lieu de décomposer une fonction complexe, nous pouvons intégrer des contraintes dans le processus de modélisation afin de pouvoir facilement lire les composants individuels. Alors que la décomposition peut être gérée de manière descendante, où nous commençons par une fonction de grande dimension et la décomposons, les modèles additifs généralisés fournissent une approche ascendante, où nous construisons le modèle à partir de composants simples. Les deux approches ont en commun que leur objectif est de fournir des composants individuels et interprétables. Dans les modèles statistiques, nous limitons le nombre de composants afin que tous ne \\(2^p\\) les composants doivent être montés. La version la plus simple est la régression linéaire :\n\\[\\hat{f}(x) = \\beta_0 + \\beta_1 x_1 + \\ldots \\beta_p x_p\\]\nLa formule ressemble beaucoup à la décomposition fonctionnelle, mais avec deux modifications majeures. Modification 1 : Tous les effets d’interaction sont exclus et nous ne conservons que les effets d’interception et principaux. Modification 2 : Les effets principaux ne peuvent être que linéaires dans les fonctionnalités : \\(\\hat{f}_j(x_j)=\\beta_j{}x_j\\). En regardant le modèle de régression linéaire à travers le prisme de la décomposition fonctionnelle, nous voyons que le modèle lui-même représente une décomposition fonctionnelle de la vraie fonction qui mappe les caractéristiques à la cible, mais sous l’hypothèse forte que les effets sont des effets linéaires et qu’il n’y a pas d’interactions.\nLe modèle additif généralisé assouplit la deuxième hypothèse en autorisant des fonctions plus flexibles \\(\\hat{f}_j\\) grâce à l’utilisation de cannelures. Des interactions peuvent également être ajoutées, mais ce processus est plutôt manuel. Des approches telles que GA2M tentent d’ajouter automatiquement des interactions bidirectionnelles à un GAM4.\nConsidérer un modèle de régression linéaire ou un GAM comme une décomposition fonctionnelle peut également prêter à confusion. Si vous appliquez les approches de décomposition du début du chapitre (ANOVA fonctionnelle généralisée et effets locaux accumulés), vous pouvez obtenir des composants différents des composants lus directement à partir du GAM. Cela peut se produire lorsque les effets d’interaction de fonctionnalités corrélées sont modélisés dans le GAM. Cet écart se produit parce que d’autres approches de décomposition fonctionnelle répartissent les effets différemment entre les interactions et les effets principaux.\nAlors, quand devriez-vous utiliser des GAM au lieu d’un modèle complexe + décomposition ? Vous devez vous en tenir aux GAM lorsque la plupart des interactions sont nulles, en particulier lorsqu’il n’y a aucune interaction avec trois fonctionnalités ou plus. Si l’on sait que le nombre maximum de fonctionnalités impliquées dans les interactions est de deux (\\(|S|\\leq{}2\\)), alors nous pouvons utiliser des approches comme MARS ou GA2M. En fin de compte, les performances du modèle sur les données de test peuvent indiquer si un GAM est suffisant ou si un modèle plus complexe fonctionne bien mieux.\n\n\n8.4.8 - Bonus : tracé de dépendance partielle\nLe diagramme de dépendance partielle fournit-il également une décomposition fonctionnelle ? Réponse courte : non. Réponse plus longue : le graphique de dépendance partielle pour un ensemble de fonctionnalités \\(S\\) contient toujours tous les effets de la hiérarchie – le PDP pour \\(\\{1,2\\}\\) contient non seulement l’interaction, mais également les effets de fonctionnalités individuels. Par conséquent, l’ajout de tous les PDP pour tous les sous-ensembles ne donne pas la fonction d’origine et ne constitue donc pas une décomposition valide. Mais pourrions-nous ajuster le PDP, peut-être en supprimant tous les effets inférieurs ? Oui, nous pourrions, mais nous obtiendrions quelque chose de similaire à l’ANOVA fonctionnelle. Cependant, au lieu d’intégrer sur une distribution uniforme, le PDP intègre sur la distribution marginale des \\(X_{-S}\\), qui est estimé à l’aide d’un échantillonnage de Monte Carlo.\n\n\n8.4.9 - Avantages\nJe considère la décomposition fonctionnelle comme un concept central de l’interprétabilité de l’apprentissage automatique.\nLa décomposition fonctionnelle nous donne une justification théorique pour décomposer des modèles d’apprentissage automatique complexes et de grande dimension en effets et interactions individuels – une étape nécessaire qui nous permet d’interpréter les effets individuels. La décomposition fonctionnelle est l’idée centrale de techniques telles que les modèles de régression statistique, l’ALE, l’ANOVA fonctionnelle (généralisée), le PDP, la statistique H et les courbes ICE.\nLa décomposition fonctionnelle permet également de mieux comprendre d’autres méthodes . Par exemple, l’importance des fonctionnalités de permutation rompt l’association entre une fonctionnalité et la cible. Vu à travers le prisme de la décomposition fonctionnelle, nous pouvons voir que la permutation « détruit » l’effet de tous les composants dans lesquels la fonctionnalité était impliquée. Cela affecte l’effet principal de la fonctionnalité, mais également toutes les interactions avec d’autres fonctionnalités. Comme autre exemple, les valeurs de Shapley décomposent une prédiction en effets additifs de la caractéristique individuelle. Mais la décomposition fonctionnelle nous dit qu’il devrait également y avoir des effets d’interaction dans la décomposition, alors où sont-ils ? Les valeurs de Shapley fournissent une attribution équitable des effets aux caractéristiques individuelles, ce qui signifie que toutes les interactions sont également attribuées équitablement aux caractéristiques et donc réparties entre les valeurs de Shapley.\nLorsque l’on considère la décomposition fonctionnelle comme un outil, l’utilisation des tracés ALE offre de nombreux avantages . Les tracés ALE fournissent une décomposition fonctionnelle rapide à calculer, dotée d’implémentations logicielles (voir le chapitre ALE) et de propriétés de pseudo-orthogonalité souhaitables.\n\n\n8.4.10 - Inconvénients\nLe concept de décomposition fonctionnelle atteint rapidement ses limites pour les composants de grande dimension au-delà des interactions entre deux entités. Non seulement cette explosion exponentielle du nombre de caractéristiques limite la praticabilité, puisque nous ne pouvons pas facilement visualiser les interactions d’ordre supérieur, mais le temps de calcul est insensé si nous devions calculer toutes les interactions.\nChaque méthode de décomposition fonctionnelle a ses propres inconvénients. L’approche ascendante – la construction de modèles de régression – est un processus assez manuel et impose de nombreuses contraintes sur le modèle qui peuvent affecter les performances prédictives. L’ANOVA fonctionnelle nécessite des fonctionnalités indépendantes. L’ANOVA fonctionnelle généralisée est très difficile à estimer. Les graphiques des effets locaux accumulés ne fournissent pas de décomposition de la variance.\nL’approche de décomposition fonctionnelle est plus appropriée pour analyser des données tabulaires que du texte ou des images.",
    "crumbs": [
      "8 - Méthodes globales indépendantes du modèle",
      "8.4 - Functional Decomposition"
    ]
  },
  {
    "objectID": "08-global_model_agnostic_methods/08.4-functional-decomposition.html#footnotes",
    "href": "08-global_model_agnostic_methods/08.4-functional-decomposition.html#footnotes",
    "title": "Apprentissage automatique interprétable",
    "section": "Notes de bas de page",
    "text": "Notes de bas de page\n\n\nHooker, Giles. “Discovering additive structure in black box functions.” Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining. (2004).↩︎\nHooker, Giles. “Generalized functional anova diagnostics for high-dimensional functions of dependent variables.” Journal of Computational and Graphical Statistics 16.3 (2007): 709-732.↩︎\nApley, Daniel W., and Jingyu Zhu. “Visualizing the effects of predictor variables in black box supervised learning models.” Journal of the Royal Statistical Society: Series B (Statistical Methodology) 82.4 (2020): 1059-1086.↩︎\nCaruana, Rich, et al. “Intelligible models for healthcare: Predicting pneumonia risk and hospital 30-day readmission.” Proceedings of the 21th ACM SIGKDD international conference on knowledge discovery and data mining. (2015).↩︎",
    "crumbs": [
      "8 - Méthodes globales indépendantes du modèle",
      "8.4 - Functional Decomposition"
    ]
  },
  {
    "objectID": "08-global_model_agnostic_methods/08.2-ale.html",
    "href": "08-global_model_agnostic_methods/08.2-ale.html",
    "title": "Apprentissage automatique interprétable",
    "section": "",
    "text": "Avertissement\n\n\n\nEn cours de traduction.",
    "crumbs": [
      "8 - Méthodes globales indépendantes du modèle",
      "8.2 - Graphique des effets locaux accumulés (ALE)"
    ]
  },
  {
    "objectID": "08-global_model_agnostic_methods/08.2-ale.html#graphique-des-effets-locaux-accumulés-ale",
    "href": "08-global_model_agnostic_methods/08.2-ale.html#graphique-des-effets-locaux-accumulés-ale",
    "title": "Apprentissage automatique interprétable",
    "section": "8.2 - Graphique des effets locaux accumulés (ALE)",
    "text": "8.2 - Graphique des effets locaux accumulés (ALE)\nLes effets locaux accumulés1 décrivent comment les caractéristiques influencent en moyenne la prédiction d’un modèle d’apprentissage automatique. Les tracés ALE sont une alternative plus rapide et impartiale aux tracés de dépendance partielle (PDP).\nJe recommande de lire d’abord le chapitre sur les diagrammes de dépendance partielle, car ils sont plus faciles à comprendre et les deux méthodes partagent le même objectif : les deux décrivent comment une caractéristique affecte la prédiction en moyenne. Dans la section suivante, je veux vous convaincre que les diagrammes de dépendance partielle présentent un sérieux problème lorsque les caractéristiques sont corrélées.\n\n8.2.1 - Motivation et intuition\nSi les caractéristiques d’un modèle d’apprentissage automatique sont corrélées, le tracé de dépendance partielle n’est pas fiable. Le calcul d’un diagramme de dépendance partielle pour une caractéristique fortement corrélée à d’autres caractéristiques implique de faire la moyenne des prédictions d’instances de données artificielles qui sont improbables dans la réalité. Cela peut grandement biaiser l’effet de fonctionnalité estimé. Imaginez calculer des tracés de dépendance partielle pour un modèle d’apprentissage automatique qui prédit la valeur d’une maison en fonction du nombre de pièces et de la taille de la surface habitable. Nous nous intéressons à l’effet de la surface habitable sur la valeur prédite. Pour rappel, la recette des tracés de dépendance partielle est la suivante :\n\nSélectionnez la fonctionnalité.\nDéfinir la grille.\nPar valeur de grille :\n\nRemplacez la fonctionnalité par la valeur de grille et\nles prédictions moyennes.\n\nDessinez une courbe. Pour le calcul de la première valeur de grille du PDP – disons 30 m 2 – nous remplaçons la surface habitable pour toutes les instances par 30 m 2 , même pour les maisons de 10 pièces. Cela me semble être une maison très inhabituelle. Le graphique de dépendance partielle inclut ces maisons irréalistes dans l’estimation des effets de caractéristiques et prétend que tout va bien. La figure suivante illustre deux caractéristiques corrélées et comment il se fait que la méthode du diagramme de dépendance partielle fait la moyenne des prédictions d’instances improbables.\n\n\n\n\nCaractéristiques fortement corrélées x1 et x2. Pour calculer l’effet caractéristique de x1 à 0,75, le PDP remplace x1 de toutes les instances par 0,75, en supposant à tort que la distribution de x2 à x1 = 0,75 est la même que la distribution marginale de x2 (ligne verticale). Cela aboutit à des combinaisons improbables de x1 et x2 (par exemple x2=0,2 à x1=0,75), que le PDP utilise pour le calcul de l’effet moyen.\n\n\nQue pouvons-nous faire pour obtenir une estimation de l’effet des caractéristiques qui respecte la corrélation des caractéristiques ? Nous pourrions faire la moyenne sur la distribution conditionnelle de la fonctionnalité, c’est-à-dire qu’à une valeur de grille de x1, nous faisons la moyenne des prédictions d’instances avec une valeur x1 similaire. La solution pour calculer les effets de caractéristiques à l’aide de la distribution conditionnelle est appelée Marginal Plots, ou M-Plots (nom déroutant, car ils sont basés sur la distribution conditionnelle et non sur la distribution marginale). Attends, je ne t’ai pas promis de parler des complots ALE ? Les M-Plots ne sont pas la solution que nous recherchons. Pourquoi les M-Plots ne résolvent-ils pas notre problème ? Si l’on fait la moyenne des prévisions de toutes les maisons d’environ 30 m 2 , on estime l’ effet combiné de la surface habitable et du nombre de pièces, du fait de leur corrélation. Supposons que la surface habitable n’ait aucun effet sur la valeur prédite d’une maison, seul le nombre de pièces en ait un. Le M-Plot montrerait toujours que la taille de la surface habitable augmente la valeur prédite, puisque le nombre de pièces augmente avec la surface habitable. Le graphique suivant montre pour deux fonctionnalités corrélées le fonctionnement des M-Plots.\n\n\n\nCaractéristiques fortement corrélées x1 et x2. M-Plots fait la moyenne sur la distribution conditionnelle. Ici, la distribution conditionnelle de x2 à x1 = 0,75. La moyenne des prédictions locales conduit à mélanger les effets des deux caractéristiques.\n\n\nLes M-Plots évitent de faire la moyenne des prédictions d’instances de données improbables, mais ils mélangent l’effet d’une fonctionnalité avec les effets de toutes les fonctionnalités corrélées. Les tracés ALE résolvent ce problème en calculant – également sur la base de la distribution conditionnelle des caractéristiques – les différences de prédictions au lieu des moyennes . Pour l’effet de la surface habitable de 30 m 2 , la méthode ALE utilise toutes les maisons d’environ 30 m 2 et obtient les prédictions du modèle prétendant que ces maisons faisaient 31 m 2 moins la prédiction prétendant qu’elles faisaient 29 m 2 . Cela nous donne l’effet pur de l’espace de vie et ne mélange pas l’effet avec les effets des éléments corrélés. L’utilisation de différences bloque l’effet d’autres fonctionnalités. Le graphique suivant donne une idée de la façon dont les tracés ALE sont calculés.\n\n\n\nCalcul de l’ALE pour la caractéristique x1, qui est corrélée à x2. Tout d’abord, nous divisons l’entité en intervalles (lignes verticales). Pour les instances de données (points) dans un intervalle, nous calculons la différence de prédiction lorsque nous remplaçons l’entité par les limites supérieure et inférieure de l’intervalle (lignes horizontales). Ces différences sont ensuite accumulées et centrées, ce qui donne la courbe ALE.\n\n\nPour résumer comment chaque type de tracé (PDP, M, ALE) calcule l’effet d’une caractéristique à une certaine valeur de grille v : Tracés de dépendance partielle : « Laissez-moi vous montrer ce que le modèle prédit en moyenne lorsque chaque instance de données a la valeur v pour cette fonctionnalité. J’ignore si la valeur v a un sens pour toutes les instances de données. M-Plots : « Laissez-moi vous montrer ce que le modèle prédit en moyenne pour les instances de données qui ont des valeurs proches de v pour cette fonctionnalité. L’effet pourrait être dû à cette fonctionnalité, mais également à des fonctionnalités corrélées. Tracés ALE : “Permettez-moi de vous montrer comment les prédictions du modèle changent dans une petite” fenêtre “de la fonctionnalité autour de v pour les instances de données dans cette fenêtre.”\n\n\n8.2.2 - Théorie\nEn quoi les tracés PD, M et ALE diffèrent-ils mathématiquement ? Le point commun aux trois méthodes est qu’elles réduisent la fonction de prédiction complexe f à une fonction qui ne dépend que d’une (ou deux) caractéristiques. Les trois méthodes réduisent la fonction en faisant la moyenne des effets des autres caractéristiques, mais elles diffèrent selon que les moyennes des prédictions ou les différences de prédictions sont calculées et si la moyenne est effectuée sur la distribution marginale ou conditionnelle.\nLes diagrammes de dépendance partielle font la moyenne des prédictions sur la distribution marginale.\n\\[\\begin{align*}\n\\hat{f}_{S,PDP}(x) &=& E_{X_C}\\left[\\hat{f}(x_S,X_C)\\right] \\\\\n&=& \\int_{X_C}\\hat{f}(x_S,X_C)d\\mathbb{P}(X_C)\n\\end{align*}\\]\nIl s’agit de la valeur de la fonction de prédiction f, à la ou aux valeurs de caractéristiques. \\(x_S\\), en moyenne sur toutes les fonctionnalités de \\(X_C\\) (ici traitées comme des variables aléatoires). Faire la moyenne signifie calculer l’espérance marginale E sur les caractéristiques de l’ensemble C, qui est l’intégrale des prédictions pondérées par la distribution de probabilité. Cela semble sophistiqué, mais pour calculer la valeur attendue sur la distribution marginale, nous prenons simplement toutes nos instances de données, les forçons à avoir une certaine valeur de grille pour les caractéristiques de l’ensemble S et faisons la moyenne des prédictions pour cet ensemble de données manipulé. Cette procédure garantit que nous faisons la moyenne sur la distribution marginale des caractéristiques.\nLes M-plots font la moyenne des prédictions sur la distribution conditionnelle.\n\\[\\begin{align*}\n\\hat{f}_{S,M}(x_S) &=& E_{X_C|X_S}\\left[\\hat{f}(X_S,X_C)|X_S=x_s\\right] \\\\\n&=&\\int_{X_C}\\hat{f}(x_S, X_C)d\\mathbb{P}(X_C|X_S = x_S)\\end{align*}\\]\nLa seule chose qui change par rapport aux PDP est que nous faisons la moyenne des prédictions conditionnelles à chaque valeur de grille de l’entité d’intérêt, au lieu de supposer la distribution marginale à chaque valeur de grille. En pratique, cela signifie qu’il faut définir un quartier, par exemple pour le calcul de l’effet de 30 m 2 sur la valeur prédite de la maison, on pourrait faire la moyenne des prédictions de toutes les maisons entre 28 et 32 ​​m 2 .\nLes tracés ALE font la moyenne des changements dans les prédictions et les accumulent sur la grille (nous reviendrons sur le calcul plus tard).\n\\[\\begin{align*}\n\\hat{f}_{S,ALE}(x_S) &=& \\int_{z_{0,S}}^{x_S}E_{X_C|X_S = x_S}\\left[\\hat{f}^S(X_s,X_c)|X_S=z_S\\right]dz_S-\\text{constant}\\\\\n&=& \\int_{z_{0,S}}^{x_S}(\\int_{x_C}\\hat{f}^S(z_s,X_c)d\\mathbb{P}(X_C|X_S = z_S)d{})dz_S-\\text{constant}\n\\end{align*}\\]\nLa formule révèle trois différences par rapport aux M-Plots. Premièrement, nous faisons la moyenne des changements de prédictions, et non des prédictions elles-mêmes. Le changement est défini comme la dérivée partielle (mais plus tard, pour le calcul proprement dit, remplacée par les différences dans les prédictions sur un intervalle).\n\\[\\hat{f}^S(x_s,x_c)=\\frac{\\partial\\hat{f}(x_S,x_C)}{\\partial{}x_S}\\]\nLa deuxième différence est l’intégrale supplémentaire sur z. Nous accumulons les dérivées partielles locales sur la plage de caractéristiques de l’ensemble S, ce qui nous donne l’effet de la caractéristique sur la prédiction. Pour le calcul proprement dit, les z sont remplacés par une grille d’intervalles sur laquelle nous calculons les changements dans la prédiction. Au lieu de faire directement la moyenne des prédictions, la méthode ALE calcule les différences de prédiction conditionnelles aux caractéristiques S et intègre la dérivée sur les caractéristiques S pour estimer l’effet. Eh bien, cela semble stupide. La dérivation et l’intégration s’annulent généralement, comme si l’on soustrayait d’abord, puis ajoutait le même nombre. Pourquoi est-ce que cela a du sens ici ? La dérivée (ou différence d’intervalle) isole l’effet de la caractéristique d’intérêt et bloque l’effet des caractéristiques corrélées.\nLa troisième différence entre les tracés ALE et les tracés M est que nous soustrayons une constante des résultats. Cette étape centre le tracé ALE de sorte que l’effet moyen sur les données soit nul.\nUn problème demeure : tous les modèles ne sont pas accompagnés d’un dégradé, par exemple les forêts aléatoires n’ont pas de dégradé. Mais comme vous le verrez, le calcul réel fonctionne sans gradients et utilise des intervalles. Approfondissons un peu l’estimation des tracés ALE.\n\n\n8.2.3 - Estimation\nJe décrirai d’abord comment les tracés ALE sont estimés pour une seule caractéristique numérique, puis pour deux caractéristiques numériques et pour une seule caractéristique catégorielle. Pour estimer les effets locaux, nous divisons la caractéristique en plusieurs intervalles et calculons les différences dans les prédictions. Cette procédure se rapproche des dérivées et fonctionne également pour les modèles sans dérivées.\nNous estimons d’abord l’effet non centré :\n\\[\\hat{\\tilde{f}}_{j,ALE}(x)=\\sum_{k=1}^{k_j(x)}\\frac{1}{n_j(k)}\\sum_{i:x_{j}^{(i)}\\in{}N_j(k)}\\left[\\hat{f}(z_{k,j},x^{(i)}_{-j})-\\hat{f}(z_{k-1,j},x^{(i)}_{-j})\\right]\\]\nDécomposons cette formule en commençant par le côté droit. Le nom Accumulated Local Effects reflète joliment tous les composants individuels de cette formule. À la base, la méthode ALE calcule les différences dans les prédictions, grâce à quoi nous remplaçons la caractéristique d’intérêt par les valeurs de grille z. La différence de prédiction correspond à l’effet que la fonctionnalité a sur une instance individuelle dans un certain intervalle. La somme de droite additionne les effets de toutes les instances dans un intervalle qui apparaît dans la formule comme voisinage. \\(N_j(k)\\). Nous divisons cette somme par le nombre d’instances dans cet intervalle pour obtenir la différence moyenne des prédictions pour cet intervalle. Cette moyenne dans l’intervalle est couverte par le terme Local dans le nom ALE. Le symbole de somme de gauche signifie que nous accumulons les effets moyens sur tous les intervalles. L’ALE (non centrée) d’une valeur de caractéristique qui se situe, par exemple, dans le troisième intervalle est la somme des effets des premier, deuxième et troisième intervalles. Le mot Accumulé dans ALE reflète cela.\nCet effet est centré pour que l’effet moyen soit nul.\n\\[\\hat{f}_{j,ALE}(x)=\\hat{\\tilde{f}}_{j,ALE}(x)-\\frac{1}{n}\\sum_{i=1}^{n}\\hat{\\tilde{f}}_{j,ALE}(x^{(i)}_{j})\\]\nLa valeur de l’ALE peut être interprétée comme l’effet principal de la fonctionnalité à une certaine valeur par rapport à la prédiction moyenne des données. Par exemple, une estimation ALE de -2 à \\(x_j=3\\) signifie que lorsque la j-ième caractéristique a la valeur 3, alors la prédiction est inférieure de 2 par rapport à la prédiction moyenne.\nLes quantiles de la distribution de l’entité sont utilisés comme grille qui définit les intervalles. L’utilisation des quantiles garantit qu’il y a le même nombre d’instances de données dans chacun des intervalles. Les quantiles présentent l’inconvénient que les intervalles peuvent avoir des longueurs très différentes. Cela peut conduire à des tracés ALE étranges si la caractéristique d’intérêt est très asymétrique, par exemple de nombreuses valeurs faibles et seulement quelques valeurs très élevées.\nGraphiques ALE pour l’interaction de deux fonctionnalités\nLes tracés ALE peuvent également montrer l’effet d’interaction de deux entités. Les principes de calcul sont les mêmes que pour une entité simple, mais nous travaillons avec des cellules rectangulaires au lieu d’intervalles, car nous devons cumuler les effets en deux dimensions. En plus d’ajuster l’effet moyen global, nous ajustons également les effets principaux des deux caractéristiques. Cela signifie que l’ALE pour deux fonctionnalités estime l’effet de second ordre, qui n’inclut pas les principaux effets des fonctionnalités. En d’autres termes, l’ALE pour deux fonctionnalités montre uniquement l’effet d’interaction supplémentaire des deux fonctionnalités. Je vous épargne les formules des tracés ALE 2D car elles sont longues et désagréables à lire. Si le calcul vous intéresse, je vous renvoie au papier, formules (13) – (16). Je m’appuierai sur des visualisations pour développer l’intuition sur le calcul ALE du second ordre.\n\n\n\nCalcul de 2D-ALE. Nous plaçons une grille sur les deux entités. Dans chaque cellule de la grille, nous calculons les différences de 2e ordre pour toutes les instances qu’elle contient. Nous remplaçons d’abord les valeurs de x1 et x2 par les valeurs des coins des cellules. Si a, b, c et d représentent les prédictions de « coin » d’une instance manipulée (comme indiqué dans le graphique), alors la différence du 2ème ordre est (d - c) - (b - a). La différence moyenne du 2ème ordre dans chaque cellule est accumulée sur la grille et centrée.\n\n\nDans la figure précédente, de nombreuses cellules sont vides en raison de la corrélation. Dans le tracé ALE, cela peut être visualisé avec une case grisée ou sombre. Vous pouvez également remplacer l’estimation ALE manquante d’une cellule vide par l’estimation ALE de la cellule non vide la plus proche.\nÉtant donné que les estimations ALE pour deux caractéristiques ne montrent que l’effet de second ordre des caractéristiques, l’interprétation nécessite une attention particulière. L’effet de second ordre est l’effet d’interaction supplémentaire des fonctionnalités après avoir pris en compte les principaux effets des fonctionnalités. Supposons que deux caractéristiques n’interagissent pas, mais que chacune ait un effet linéaire sur le résultat prédit. Dans le tracé ALE 1D pour chaque entité, nous verrions une ligne droite comme courbe ALE estimée. Mais lorsque nous traçons les estimations ALE 2D, elles devraient être proches de zéro, car l’effet de second ordre n’est que l’effet supplémentaire de l’interaction. Les tracés ALE et PD diffèrent à cet égard : les PDP montrent toujours l’effet total, les tracés ALE montrent l’effet de premier ou de second ordre. Ce sont des décisions de conception qui ne dépendent pas des mathématiques sous-jacentes. Vous pouvez soustraire les effets d’ordre inférieur dans un diagramme de dépendance partielle pour obtenir les effets principaux ou secondaires purs ou, vous pouvez obtenir une estimation du total des diagrammes ALE en vous abstenant de soustraire les effets d’ordre inférieur.\nLes effets locaux accumulés pourraient également être calculés pour des ordres arbitrairement supérieurs (interactions de trois caractéristiques ou plus), mais comme indiqué dans le chapitre PDP, seulement deux caractéristiques au maximum ont du sens, car les interactions supérieures ne peuvent pas être visualisées ni même interprétées de manière significative.\nALE pour les fonctionnalités catégorielles\nLa méthode des effets locaux accumulés nécessite – par définition – que les valeurs des caractéristiques aient un ordre, car la méthode accumule les effets dans une certaine direction. Les caractéristiques catégorielles n’ont aucun ordre naturel. Pour calculer un tracé ALE pour une caractéristique catégorielle, nous devons d’une manière ou d’une autre créer ou trouver un ordre. L’ordre des catégories influence le calcul et l’interprétation des effets locaux accumulés.\nUne solution consiste à classer les catégories en fonction de leur similarité en fonction des autres caractéristiques. La distance entre deux catégories est la somme des distances de chaque entité. La distance par caractéristique compare soit la distribution cumulative dans les deux catégories, également appelée distance de Kolmogorov-Smirnov (pour les caractéristiques numériques), soit les tableaux de fréquences relatives (pour les caractéristiques catégorielles). Une fois que nous avons les distances entre toutes les catégories, nous utilisons une mise à l’échelle multidimensionnelle pour réduire la matrice de distance à une mesure de distance unidimensionnelle. Cela nous donne un ordre des catégories basé sur la similarité.\nPour rendre cela un peu plus clair, voici un exemple : supposons que nous ayons les deux caractéristiques catégorielles « saison » et « météo » et une caractéristique numérique « température ». Pour la première caractéristique catégorielle (saison), nous souhaitons calculer les ALE. La fonctionnalité comporte les catégories « printemps », « été », « automne » et « hiver ». Nous commençons par calculer la distance entre les catégories « printemps » et « été ». La distance est la somme des distances sur les caractéristiques température et météo. Pour la température, nous prenons toutes les instances avec la saison « printemps », calculons la fonction de distribution cumulée empirique et faisons de même pour les instances avec la saison « été » et mesurons leur distance avec la statistique de Kolmogorov-Smirnov. Pour la fonction météo, nous calculons pour toutes les instances « printemps » les probabilités pour chaque type de temps, faisons de même pour les instances « été » et résumons les distances absolues dans la distribution de probabilité. Si le « printemps » et « l’été » ont des températures et des conditions météorologiques très différentes, la distance totale par catégorie est grande. Nous répétons la procédure avec les autres paires saisonnières et réduisons la matrice de distance résultante à une seule dimension par mise à l’échelle multidimensionnelle.\n\n\n8.2.4 - Exemples\nVoyons les complots ALE en action. J’ai construit un scénario dans lequel les tracés de dépendance partielle échouent. Le scénario se compose d’un modèle de prédiction et de deux fonctionnalités fortement corrélées. Le modèle de prédiction est principalement un modèle de régression linéaire, mais il fait quelque chose de bizarre en combinant les deux caractéristiques pour lesquelles nous n’avons jamais observé d’instances.\n\n\n\nDeux caractéristiques et le résultat prévu. Le modèle prédit la somme des deux caractéristiques (fond ombré), à la différence que si x1 est supérieur à 0,7 et x2 inférieur à 0,3, le modèle prédit toujours 2. Cette zone est éloignée de la distribution des données (nuage de points) et n’affecte pas les performances du modèle et ne devrait pas non plus affecter son interprétation.\n\n\nEst-ce un scénario réaliste et pertinent ? Lorsque vous entraînez un modèle, l’algorithme d’apprentissage minimise la perte des instances de données d’entraînement existantes. Des choses étranges peuvent se produire en dehors de la distribution des données d’entraînement, car le modèle n’est pas pénalisé pour avoir fait des choses étranges dans ces domaines. Quitter la distribution des données est appelé extrapolation, qui peut également être utilisée pour tromper les modèles d’apprentissage automatique, décrits dans le chapitre sur les exemples contradictoires. Voyez dans notre petit exemple comment les tracés de dépendance partielle se comportent par rapport aux tracés ALE.\n\n\n\nComparaison des effets de caractéristiques calculés avec PDP (ligne supérieure) et ALE (ligne inférieure). Les estimations du PDP sont influencées par le comportement étrange du modèle en dehors de la distribution des données (sauts abrupts dans les tracés). Les tracés ALE identifient correctement que le modèle d’apprentissage automatique a une relation linéaire entre les fonctionnalités et la prédiction, ignorant les zones sans données.\n\n\nMais n’est-il pas intéressant de voir que notre modèle se comporte bizarrement à x1 &gt; 0,7 et x2 &lt; 0,3 ? Eh bien, oui et non. Puisqu’il s’agit d’instances de données qui pourraient être physiquement impossibles ou du moins extrêmement improbables, il n’est généralement pas pertinent de les examiner. Mais si vous pensez que la distribution de vos tests peut être légèrement différente et que certaines instances se situent réellement dans cette plage, il serait alors intéressant d’inclure cette zone dans le calcul des effets des fonctionnalités. Mais cela doit être une décision consciente d’inclure des zones pour lesquelles nous n’avons pas encore observé de données et cela ne devrait pas être un effet secondaire de la méthode choisie comme le PDP. Si vous pensez que le modèle sera utilisé ultérieurement avec des données distribuées différemment, je vous recommande d’utiliser des tracés ALE et de simuler la distribution des données que vous attendez.\nEn nous tournant vers un ensemble de données réelles, prévoyons le nombre de vélos loués en fonction de la météo et du jour et vérifions si les tracés ALE fonctionnent vraiment aussi bien que promis. Nous formons un arbre de régression pour prédire le nombre de vélos loués un jour donné et utilisons des tracés ALE pour analyser comment la température, l’humidité relative et la vitesse du vent influencent les prévisions. Regardons ce que disent les tracés ALE :\n\n\n\nGraphiques ALE pour le modèle de prévision du vélo par température, humidité et vitesse du vent. La température a un effet important sur la prévision. La prévision moyenne augmente avec la température, mais retombe au-dessus de 25 degrés Celsius. L’humidité a un effet négatif : lorsqu’elle est supérieure à 60 %, plus l’humidité relative est élevée, plus la prévision est faible. La vitesse du vent n’affecte pas beaucoup les prévisions.\n\n\nExaminons la corrélation entre la température, l’humidité et la vitesse du vent ainsi que toutes les autres caractéristiques. Étant donné que les données contiennent également des caractéristiques catégorielles, nous ne pouvons pas utiliser uniquement le coefficient de corrélation de Pearson, qui ne fonctionne que si les deux caractéristiques sont numériques. Au lieu de cela, j’entraîne un modèle linéaire pour prédire, par exemple, la température en fonction de l’une des autres caractéristiques en entrée. Ensuite, je mesure la variance expliquée par l’autre caractéristique du modèle linéaire et je prends la racine carrée. Si l’autre caractéristique était numérique, alors le résultat est égal à la valeur absolue du coefficient de corrélation standard de Pearson. Mais cette approche basée sur un modèle de « variance expliquée » (également appelée ANOVA, qui signifie ANalysis Of VAriance) fonctionne même si l’autre caractéristique est catégorique. La mesure « variance expliquée » se situe toujours entre 0 (pas d’association) et 1 (la température peut être parfaitement prédite à partir de l’autre caractéristique). Nous calculons la variance expliquée de la température, de l’humidité et de la vitesse du vent avec toutes les autres caractéristiques. Plus la variance expliquée (corrélation) est élevée, plus il y a de problèmes (potentiels) avec les tracés PD. La figure suivante montre dans quelle mesure les caractéristiques météorologiques sont corrélées avec d’autres caractéristiques.\n\n\n\nLa force de la corrélation entre la température, l’humidité et la vitesse du vent avec toutes les caractéristiques, mesurée comme la quantité de variance expliquée, lorsque nous entraînons un modèle linéaire avec par exemple la température à prédire et la saison comme caractéristique. Pour la température, nous observons – sans surprise – une forte corrélation avec la saison et le mois. L’humidité est en corrélation avec la situation météorologique.\n\n\nCette analyse de corrélation révèle que nous pouvons rencontrer des problèmes avec les diagrammes de dépendance partielle, notamment pour la caractéristique de température. Eh bien, voyez par vous-même :\n\n\n\nPDP pour la température, l’humidité et la vitesse du vent. Par rapport aux tracés ALE, les PDP montrent une diminution plus faible du nombre prévu de vélos en cas de température ou d’humidité élevée. Le PDP utilise toutes les instances de données pour calculer l’effet des températures élevées, même s’il s’agit, par exemple, d’instances avec la saison « hiver ». Les tracés ALE sont plus fiables.\n\n\nVoyons ensuite les tracés ALE en action pour une fonctionnalité catégorielle. Le mois est une caractéristique catégorielle dont nous souhaitons analyser l’effet sur le nombre de vélos prévu. On peut soutenir que les mois ont déjà un certain ordre (janvier à décembre), mais essayons de voir ce qui se passe si nous réordonnons d’abord les catégories par similarité, puis calculons les effets. Les mois sont classés selon la similitude des jours de chaque mois en fonction d’autres caractéristiques, telles que la température ou s’il s’agit d’un jour férié.\n\n\n\nGraphique ALE pour le mois caractéristique catégoriel. Les mois sont classés en fonction de leur similarité les uns par rapport aux autres, en fonction des répartitions des autres fonctionnalités par mois. On observe que janvier, mars et avril, mais surtout décembre et novembre, ont un effet moindre sur le nombre prévu de vélos loués par rapport aux autres mois.\n\n\nÉtant donné que de nombreuses caractéristiques sont liées à la météo, l’ordre des mois reflète fortement la similitude des conditions météorologiques entre les mois. Tous les mois les plus froids se trouvent sur le côté gauche (de février à avril) et les mois les plus chauds sur le côté droit (d’octobre à août). Gardez à l’esprit que des caractéristiques non météorologiques ont également été incluses dans le calcul de similarité, par exemple la fréquence relative des vacances a le même poids que la température pour calculer la similarité entre les mois.\nNous considérons ensuite l’effet de second ordre de l’humidité et de la température sur le nombre prévu de vélos. N’oubliez pas que l’effet de second ordre est l’effet d’interaction supplémentaire des deux fonctionnalités et n’inclut pas les effets principaux. Cela signifie que, par exemple, vous ne verrez pas l’effet principal selon lequel une humidité élevée entraîne en moyenne un nombre inférieur de vélos prévus dans le tracé ALE de second ordre.\n\n\n\nGraphique ALE pour l’effet du 2ème ordre de l’humidité et de la température sur le nombre prévu de vélos loués. Une teinte plus claire indique une prévision supérieure à la moyenne et une teinte plus foncée une prévision inférieure à la moyenne lorsque les principaux effets sont déjà pris en compte. Le graphique révèle une interaction entre la température et l’humidité : un temps chaud et humide augmente la prévision. Par temps froid et humide, un effet négatif supplémentaire sur le nombre de vélos prévu est constaté.\n\n\nGardez à l’esprit que les deux principaux effets de l’humidité et de la température indiquent que le nombre prévu de vélos diminue par temps très chaud et humide. Par temps chaud et humide, l’effet combiné de la température et de l’humidité n’est donc pas la somme des effets principaux, mais plus important que la somme. Pour souligner la différence entre l’effet pur du second ordre (le tracé ALE 2D que vous venez de voir) et l’effet total, regardons le tracé de dépendance partielle. Le PDP montre l’effet total, qui combine la prédiction moyenne, les deux effets principaux et l’effet de second ordre (l’interaction).\n\n\n\nPDP de l’effet total de la température et de l’humidité sur le nombre prévu de vélos. Le tracé combine l’effet principal de chacune des caractéristiques et leur effet d’interaction, contrairement au tracé 2D-ALE qui ne montre que l’interaction.\n\n\nSi vous n’êtes intéressé que par l’interaction, vous devriez examiner les effets de second ordre, car l’effet total mélange les effets principaux dans l’intrigue. Mais si vous voulez connaître l’effet combiné des fonctionnalités, vous devez regarder l’effet total (que montre le PDP). Par exemple, si vous souhaitez connaître le nombre prévu de vélos à 30 degrés Celsius et 80 % d’humidité, vous pouvez le lire directement à partir du PDP 2D. Si vous souhaitez lire la même chose à partir des tracés ALE, vous devez examiner trois tracés : le tracé ALE pour la température, pour l’humidité et pour la température + humidité et vous devez également connaître la prévision moyenne globale. Dans un scénario dans lequel deux éléments n’ont aucune interaction, le tracé des effets totaux des deux éléments pourrait être trompeur car il montre probablement un paysage complexe, suggérant une certaine interaction, mais il s’agit simplement du produit des deux effets principaux. L’effet du second ordre montrerait immédiatement qu’il n’y a pas d’interaction.\nAssez de vélos pour l’instant, passons à une tâche de classification. Nous formons une forêt aléatoire pour prédire la probabilité de cancer du col de l’utérus en fonction de facteurs de risque. Nous visualisons les effets locaux accumulés pour deux des fonctionnalités :\n\n\n\nGraphiques ALE pour l’effet de l’âge et des années d’utilisation des contraceptifs hormonaux sur la probabilité prédite de cancer du col de l’utérus. Pour la caractéristique âge, le graphique ALE montre que la probabilité prévue de cancer est faible en moyenne jusqu’à 40 ans et augmente par la suite. Le nombre d’années passées sous contraceptifs hormonaux est associé à un risque de cancer prédit plus élevé après 8 ans.\n\n\nNous examinons ensuite l’interaction entre le nombre de grossesses et l’âge.\n\n\n\nGraphique ALE de l’effet du 2ème ordre du nombre de grossesses et de l’âge. L’interprétation de l’intrigue est un peu peu concluante, montrant ce qui semble être un surapprentissage. Par exemple, le graphique montre un comportement de modèle étrange entre 18 et 20 ans et plus de 3 grossesses (jusqu’à 5 points de pourcentage d’augmentation de la probabilité de cancer). Il n’y a pas beaucoup de femmes dans les données avec cette constellation d’âge et de nombre de grossesses (les données réelles sont affichées sous forme de points), donc le modèle n’est pas sévèrement pénalisé pendant la formation pour avoir commis des erreurs avec ces femmes.\n\n\n\n\n8.2.5 - Avantages\nLes tracés ALE sont impartiaux, ce qui signifie qu’ils fonctionnent toujours lorsque les caractéristiques sont corrélées. Les tracés de dépendance partielle échouent dans ce scénario car ils marginalisent des combinaisons improbables, voire physiquement impossibles, de valeurs de caractéristiques.\nLes tracés ALE sont plus rapides à calculer que les PDP et évoluent avec O(n), puisque le plus grand nombre possible d’intervalles est le nombre d’instances avec un intervalle par instance. Le PDP nécessite n fois le nombre d’estimations de points de grille. Pour 20 points de grille, les PDP nécessitent 20 fois plus de prédictions que le pire des cas de tracé ALE où autant d’intervalles que d’instances sont utilisés.\nL’ interprétation des tracés ALE est claire : conditionnellement à une valeur donnée, l’effet relatif du changement de caractéristique sur la prédiction peut être lu à partir du tracé ALE. Les tracés ALE sont centrés sur zéro. Cela rend leur interprétation intéressante, car la valeur en chaque point de la courbe ALE correspond à la différence par rapport à la prédiction moyenne. Le tracé ALE 2D montre uniquement l’interaction : Si deux entités n’interagissent pas, le tracé ne montre rien.\nL’ensemble de la fonction de prédiction peut être décomposée en une somme de fonctions ALE de dimension inférieure, comme expliqué dans le chapitre sur la décomposition fonctionnelle.\nDans l’ensemble, dans la plupart des situations, je préférerais les tracés ALE aux PDP, car les fonctionnalités sont généralement corrélées dans une certaine mesure.\n\n\n8.2.6 - Inconvénients\nUne interprétation de l’effet sur plusieurs intervalles n’est pas autorisée si les caractéristiques sont fortement corrélées. Prenons le cas où vos caractéristiques sont fortement corrélées et où vous regardez l’extrémité gauche d’un tracé 1D-ALE. La courbe ALE pourrait inviter à l’interprétation erronée suivante : « La courbe ALE montre comment la prédiction change, en moyenne, lorsque nous modifions progressivement la valeur de la caractéristique respective pour une instance de données, tout en gardant les instances des autres valeurs de caractéristiques fixes. » Les effets sont calculés par intervalle (localement) et donc l’interprétation de l’effet ne peut être que locale. Pour plus de commodité, les effets par intervalle sont cumulés pour afficher une courbe lisse, mais gardez à l’esprit que chaque intervalle est créé avec différentes instances de données.\nLes effets ALE peuvent différer des coefficients spécifiés dans un modèle de régression linéaire lorsque les caractéristiques interagissent et sont corrélées. Grömping (2020)2 a montré que dans un modèle linéaire avec deux caractéristiques corrélées et un terme d’interaction supplémentaire (\\(\\hat{f}(x) = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_1 x_2\\)) les tracés ALE de premier ordre ne montrent pas de ligne droite. Au lieu de cela, ils sont légèrement incurvés car ils intègrent des parties de l’interaction multiplicative des caractéristiques. Pour comprendre ce qui se passe ici, je vous recommande de lire le chapitre sur la décomposition des fonctions. En bref, l’ALE définit les effets de premier ordre (ou 1D) différemment de la formule linéaire qui les décrit. Ce n’est pas nécessairement faux, car lorsque les caractéristiques sont corrélées, l’attribution des interactions n’est pas aussi claire. Mais il n’est certainement pas intuitif que l’ALE et le coefficient linéaire ne correspondent pas.\nLes tracés ALE peuvent devenir un peu fragiles (beaucoup de petits hauts et bas) avec un nombre élevé d’intervalles. Dans ce cas, la réduction du nombre d’intervalles rend les estimations plus stables, mais atténue et masque également une partie de la véritable complexité du modèle de prédiction. Il n’existe pas de solution parfaite pour définir le nombre d’intervalles . Si le nombre est trop petit, les tracés ALE risquent de ne pas être très précis. Si le nombre est trop élevé, la courbe peut devenir fragile.\nContrairement aux PDP, les tracés ALE ne sont pas accompagnés de courbes ICE. Pour les PDP, les courbes ICE sont excellentes car elles peuvent révéler une hétérogénéité dans l’effet des caractéristiques, ce qui signifie que l’effet d’une caractéristique semble différent pour des sous-ensembles de données. Pour les tracés ALE, vous pouvez uniquement vérifier par intervalle si l’effet est différent entre les instances, mais chaque intervalle a des instances différentes, ce n’est donc pas la même chose que les courbes ICE.\nLes estimations ALE de second ordre ont une stabilité variable dans l’espace des fonctionnalités, qui n’est en aucun cas visualisée. La raison en est que chaque estimation d’un effet local dans une cellule utilise un nombre différent d’instances de données. En conséquence, toutes les estimations ont une précision différente (mais elles restent les meilleures estimations possibles). Le problème existe dans une version moins grave pour les tracés ALE à effet principal. Le nombre d’instances est le même dans tous les intervalles, grâce à l’utilisation de quantiles comme grille, mais dans certaines zones, il y aura de nombreux intervalles courts et la courbe ALE sera composée de beaucoup plus d’estimations. Mais pour les intervalles longs, qui peuvent représenter une grande partie de la courbe entière, il y a comparativement moins d’instances. Cela s’est produit par exemple dans le graphique ALE de prédiction du cancer du col de l’utérus pour un âge élevé.\nLes tracés des effets de second ordre peuvent être un peu ennuyeux à interpréter, car il faut toujours garder les effets principaux à l’esprit. Il est tentant de lire les cartes thermiques comme l’effet total des deux fonctionnalités, mais il ne s’agit que de l’effet supplémentaire de l’interaction. L’effet de second ordre pur est intéressant pour découvrir et explorer les interactions, mais pour interpréter à quoi ressemble l’effet, je pense qu’il est plus logique d’intégrer les effets principaux dans l’intrigue.\nLa mise en œuvre des tracés ALE est beaucoup plus complexe et moins intuitive que celle des tracés de dépendance partielle.\nMême si les tracés ALE ne sont pas biaisés en cas de caractéristiques corrélées, l’interprétation reste difficile lorsque les caractéristiques sont fortement corrélées. Parce que s’ils ont une très forte corrélation, il est logique d’analyser l’effet de la modification des deux caractéristiques ensemble et non isolément. Cet inconvénient n’est pas spécifique aux tracés ALE, mais constitue un problème général de caractéristiques fortement corrélées.\nSi les caractéristiques ne sont pas corrélées et que le temps de calcul ne pose pas de problème, les PDP sont légèrement préférables car ils sont plus faciles à comprendre et peuvent être tracés avec les courbes ICE.\nLa liste des inconvénients est devenue assez longue, mais ne vous laissez pas tromper par le nombre de mots que j’utilise : En règle générale : utilisez ALE au lieu de PDP.\n\n\n8.2.7 - Mise en œuvre et alternatives\nAi-je mentionné que les tracés de dépendance partielle et les courbes d’espérances conditionnelles individuelles sont une alternative ? =)\nLes tracés ALE sont implémentés dans R dans le package ALEPlot R par l’inventeur lui-même et une fois dans le package iml. ALE dispose également d’au moins deux implémentations Python avec le package ALEPython et dans Alibi.",
    "crumbs": [
      "8 - Méthodes globales indépendantes du modèle",
      "8.2 - Graphique des effets locaux accumulés (ALE)"
    ]
  },
  {
    "objectID": "08-global_model_agnostic_methods/08.2-ale.html#footnotes",
    "href": "08-global_model_agnostic_methods/08.2-ale.html#footnotes",
    "title": "Apprentissage automatique interprétable",
    "section": "Notes de bas de page",
    "text": "Notes de bas de page\n\n\nApley, Daniel W., and Jingyu Zhu. “Visualizing the effects of predictor variables in black box supervised learning models.” Journal of the Royal Statistical Society: Series B (Statistical Methodology) 82.4 (2020): 1059-1086.↩︎\nGrömping, Ulrike. “Model-Agnostic Effects Plots for Interpreting Machine Learning Models.” Reports in Mathematics, Physics and Chemistry: Department II, Beuth University of Applied Sciences Berlin. Report 1/2020 (2020)↩︎",
    "crumbs": [
      "8 - Méthodes globales indépendantes du modèle",
      "8.2 - Graphique des effets locaux accumulés (ALE)"
    ]
  },
  {
    "objectID": "07-example/index.html",
    "href": "07-example/index.html",
    "title": "7 - Explications basées sur des exemples",
    "section": "",
    "text": "Avertissement\n\n\n\nEn cours de traduction.",
    "crumbs": [
      "7 - Explications basées sur des exemples"
    ]
  },
  {
    "objectID": "07-example/index.html#footnotes",
    "href": "07-example/index.html#footnotes",
    "title": "7 - Explications basées sur des exemples",
    "section": "Notes de bas de page",
    "text": "Notes de bas de page\n\n\nAamodt, Agnar, and Enric Plaza. “Case-based reasoning: Foundational issues, methodological variations, and system approaches.” AI communications 7.1 (1994): 39-59.↩︎\nKim, Been, Rajiv Khanna, and Oluwasanmi O. Koyejo. “Examples are not enough, learn to criticize! Criticism for interpretability.” Advances in Neural Information Processing Systems (2016).↩︎",
    "crumbs": [
      "7 - Explications basées sur des exemples"
    ]
  },
  {
    "objectID": "05-interpretable_models/05.6-rulefit.html",
    "href": "05-interpretable_models/05.6-rulefit.html",
    "title": "Apprentissage automatique interprétable",
    "section": "",
    "text": "Avertissement\n\n\n\nEn cours de traduction.",
    "crumbs": [
      "5 - Modèles interprétables",
      "5.6 - Ajustement des règles"
    ]
  },
  {
    "objectID": "05-interpretable_models/05.6-rulefit.html#ajustement-des-règles",
    "href": "05-interpretable_models/05.6-rulefit.html#ajustement-des-règles",
    "title": "Apprentissage automatique interprétable",
    "section": "5.6 - Ajustement des règles",
    "text": "5.6 - Ajustement des règles\nL’algorithme RuleFit de Friedman et Popescu (2008)1 apprend des modèles linéaires clairsemés qui incluent des effets d’interaction automatiquement détectés sous la forme de règles de décision.\nLe modèle de régression linéaire ne tient pas compte des interactions entre les caractéristiques. Ne serait-il pas pratique d’avoir un modèle aussi simple et interprétable que les modèles linéaires, mais intégrant également des interactions de fonctionnalités ? RuleFit comble cette lacune. RuleFit apprend un modèle linéaire clairsemé avec les fonctionnalités d’origine ainsi qu’un certain nombre de nouvelles fonctionnalités qui sont des règles de décision. Ces nouvelles fonctionnalités capturent les interactions entre les fonctionnalités d’origine. RuleFit génère automatiquement ces fonctionnalités à partir d’arbres de décision. Chaque chemin à travers un arbre peut être transformé en règle de décision en combinant les décisions fractionnées en une règle. Les prédictions de nœuds sont ignorées et seules les divisions sont utilisées dans les règles de décision :\n\n\n\n4 règles peuvent être générées à partir d’un arbre à 3 nœuds terminaux.\n\n\nD’où viennent ces arbres de décision ? Les arbres sont dressés pour prédire le résultat qui nous intéresse. Cela garantit que les divisions sont significatives pour la tâche de prédiction. Tout algorithme générant de nombreux arbres peut être utilisé pour RuleFit, par exemple une forêt aléatoire. Chaque arbre est décomposé en règles de décision qui sont utilisées comme fonctionnalités supplémentaires dans un modèle de régression linéaire clairsemé (Lasso).\nL’article RuleFit utilise les données sur le logement de Boston pour illustrer ceci : l’objectif est de prédire la valeur médiane des maisons d’un quartier de Boston. L’une des règles générées par RuleFit est : IF number of rooms &gt; 6.64 ET concentration of nitric oxide &lt; 0.67 ALORS 1 SINON 0.\nRuleFit est également livré avec une mesure de l’importance des fonctionnalités qui permet d’identifier les termes linéaires et les règles importants pour les prédictions. L’importance des caractéristiques est calculée à partir des poids du modèle de régression. La mesure d’importance peut être agrégée pour les caractéristiques originales (qui sont utilisées sous leur forme « brute » et éventuellement dans de nombreuses règles de décision).\nRuleFit introduit également des tracés de dépendance partielle pour montrer le changement moyen de la prédiction en modifiant une fonctionnalité. Le tracé de dépendance partielle est une méthode indépendante du modèle qui peut être utilisée avec n’importe quel modèle et est expliquée dans le chapitre du livre sur les tracés de dépendance partielle.\n\n5.6.1 - Interprétation et exemple\nPuisque RuleFit estime finalement un modèle linéaire, l’interprétation est la même que pour les modèles linéaires « normaux » . La seule différence est que le modèle possède de nouvelles fonctionnalités dérivées des règles de décision. Les règles de décision sont des caractéristiques binaires : une valeur de 1 signifie que toutes les conditions de la règle sont remplies, sinon la valeur est 0. Pour les termes linéaires dans RuleFit, l’interprétation est la même que dans les modèles de régression linéaire : si la caractéristique augmente d’une unité , le résultat prédit change en fonction du poids des caractéristiques correspondant.\nDans cet exemple, nous utilisons RuleFit pour prédire le nombre de vélos loués un jour donné. Le tableau présente cinq des règles générées par RuleFit, ainsi que leurs poids et importances Lasso. Le calcul est expliqué plus loin dans le chapitre.\n\nLa règle la plus importante était : « jours_depuis_2011 &gt; 111 & temps de repos (« GOOD », « MISTY ») » et le poids correspondant est 795. L’interprétation est : Si jours_depuis_2011 &gt; 111 & temps de repos (« GOOD », « MISTY ») , le nombre prévu de vélos augmente de 795, lorsque toutes les autres valeurs de caractéristiques restent fixes. Au total, 278 règles de ce type ont été créées à partir des 8 fonctionnalités originales. Beaucoup! Mais grâce au Lasso, seuls 59 des 278 ont un poids différent de 0.\nLe calcul de l’importance des caractéristiques globales révèle que la tendance de la température et du temps sont les caractéristiques les plus importantes :\n\n\n\nMesures d’importance des caractéristiques pour un modèle RuleFit prédisant le nombre de vélos. Les caractéristiques les plus importantes pour les prévisions étaient la température et la tendance temporelle.\n\n\nLa mesure de l’importance de la fonctionnalité inclut l’importance du terme brut de la fonctionnalité et toutes les règles de décision dans lesquelles la fonctionnalité apparaît.\nModèle d’interprétation\nL’interprétation est analogue aux modèles linéaires : le résultat prévu change de \\(\\beta_j\\) si la fonctionnalité \\(x_j\\) change d’une unité, à condition que toutes les autres caractéristiques restent inchangées. L’interprétation pondérée d’une règle de décision est un cas particulier : si toutes les conditions d’une règle de décision \\(r_k\\) s’appliquent, le résultat prévu change de \\(\\alpha_k\\) (le poids appris de la règle \\(r_k\\) dans le modèle linéaire).\nPour la classification (en utilisant la régression logistique au lieu de la régression linéaire) : Si toutes les conditions de la règle de décision \\(r_k\\) s’appliquent, les chances d’un événement ou d’un non-événement changent d’un facteur de \\(\\alpha_k\\).\n\n\n5.6.2 - Théorie\nExaminons plus en détail les détails techniques de l’algorithme RuleFit. RuleFit se compose de deux composants : le premier composant crée des « règles » à partir d’arbres de décision et le deuxième composant ajuste un modèle linéaire avec les fonctionnalités d’origine et les nouvelles règles en entrée (d’où le nom « RuleFit »).\nÉtape 1 : Génération de règles\nA quoi ressemble une règle ? Les règles générées par l’algorithme ont une forme simple. Par exemple : SI x2 &lt; 3 ET x5 &lt; 7 ALORS 1 ELSE 0. Les règles sont construites en décomposant des arbres de décision : Tout chemin vers un nœud dans un arbre peut être converti en règle de décision. Les arbres utilisés pour les règles sont ajustés pour prédire le résultat cible. Par conséquent, les divisions et les règles qui en résultent sont optimisées pour prédire le résultat qui vous intéresse. Vous enchaînez simplement les décisions binaires qui mènent à un certain nœud avec « ET », et voilà, vous avez une règle. Il est souhaitable de générer un grand nombre de règles diverses et significatives. L’augmentation de gradient est utilisée pour ajuster un ensemble d’arbres de décision en régressant ou en classant y avec vos fonctionnalités d’origine X. Chaque arbre résultant est converti en plusieurs règles. Non seulement les arbres améliorés, mais n’importe quel algorithme d’ensemble d’arbres peut être utilisé pour générer les arbres pour RuleFit. Un ensemble d’arbres peut être décrit avec cette formule générale :\n\\[\\hat{f}(x) = a_0+\\sum_{m=1}^M a_m \\hat{f}_m(X)\\]\nM est le nombre d’arbres et \\(\\hat{f}_m(x)\\) est la fonction de prédiction du m-ième arbre. Les \\(a\\) sont les poids. Les ensembles en sac, la forêt aléatoire, AdaBoost et MART produisent des ensembles d’arbres et peuvent être utilisés pour RuleFit.\nNous créons les règles à partir de tous les arbres de l’ensemble. Chaque règle \\(r_m\\) prend la forme de :\n\\[r_m(x) = \\prod_{j\\in\\text{T}_m}I(x_j\\in{}s_{jm})\\]\noù \\(\\text{T}_{m}\\) est l’ensemble des fonctionnalités utilisées dans le m-ème arbre, I est la fonction indicatrice qui vaut \\(1\\) lorsque la fonctionnalité \\(x_j\\) est dans le sous-ensemble spécifié de valeurs \\(s\\) pour la \\(j^{ème}\\) fonctionnalité (comme spécifié par les divisions d’arbre) et \\(0\\) sinon. Pour les fonctionnalités numériques, \\(s_{jm}\\) est un intervalle dans la plage de valeurs de la fonctionnalité. L’intervalle ressemble à l’un des deux cas :\n\\[x_{s_{jm},\\text{lower}}&lt;x_j\\]\n\\[x_j&lt;x_{s_{jm},upper}\\]\nDes divisions supplémentaires dans cette fonctionnalité pourraient conduire à des intervalles plus compliqués. Pour les fonctionnalités catégorielles, le sous-ensemble s contient certaines catégories spécifiques de la fonctionnalité.\nUn exemple inventé pour l’ensemble de données de location de vélos :\n\\[\\begin{align*}\nr_{17}(x) & = I(x_{\\text{temp}}&lt;15) . I(x_{\\text{weather}} \\in \\{\\text{good},\\text{cloudy}\\}) \\\\\n    & . I(10\\leq x_{\\text{windspeed}}&lt;20)\n\\end{align*}\\]\nCette règle renvoie 1 si les trois conditions sont remplies, sinon 0. RuleFit extrait toutes les règles possibles d’un arbre, pas seulement des nœuds feuilles. Ainsi, une autre règle qui serait créée est :\n\\[r_{18}(x) = I(x_{\\text{temp}}&lt;15) . I(x_{\\text{weather}} \\in \\{\\text{good},\\text{cloudy}\\})\\]\nAu total, le nombre de règles créées à partir d’un ensemble de M arbres avec \\(t_m\\) les noeuds terminaux sont chacun :\n\\[K = \\sum_{m=1}^M2(t_m-1)\\]\nUne astuce introduite par les auteurs de RuleFit consiste à apprendre des arbres avec une profondeur aléatoire afin de générer de nombreuses règles diverses de longueurs différentes. Notez que nous supprimons la valeur prédite dans chaque nœud et ne conservons que les conditions qui nous conduisent à un nœud, puis nous créons une règle à partir de celui-ci. La pondération des règles de décision se fait à l’étape 2 de RuleFit.\nUne autre façon de voir l’étape 1 : RuleFit génère un nouvel ensemble de fonctionnalités à partir de vos fonctionnalités d’origine. Ces fonctionnalités sont binaires et peuvent représenter des interactions assez complexes de vos fonctionnalités d’origine. Les règles sont choisies pour maximiser la tâche de prédiction. Les règles sont automatiquement générées à partir de la matrice de covariables X. Vous pouvez simplement voir les règles comme de nouvelles fonctionnalités basées sur vos fonctionnalités d’origine.\nÉtape 2 : Modèle linéaire clairsemé\nVous obtenez BEAUCOUP de règles à l’étape 1. Étant donné que la première étape peut être considérée comme uniquement une transformation de caractéristiques, vous n’avez toujours pas fini d’ajuster un modèle. Vous souhaitez également réduire le nombre de règles. En plus des règles, toutes vos caractéristiques « brutes » de votre ensemble de données d’origine seront également utilisées dans le modèle linéaire clairsemé. Chaque règle et chaque fonctionnalité originale devient une fonctionnalité dans le modèle linéaire et obtient une estimation de poids. Les caractéristiques brutes d’origine sont ajoutées car les arbres ne parviennent pas à représenter des relations linéaires simples entre \\(y\\) et \\(x\\). Avant de former un modèle linéaire clairsemé, nous winsorisons les fonctionnalités d’origine afin qu’elles soient plus robustes face aux valeurs aberrantes :\n\\[l_j^*(x_j)=min(\\delta_j^+,max(\\delta_j^-,x_j))\\]\noù \\(\\delta_j^-\\) et \\(\\delta_j^+\\) sont les \\(\\delta\\) quantiles de la distribution des données de l’entité \\(x_j\\). Un choix de 0,05 pour \\(\\delta\\) signifie que toute valeur de fonctionnalité \\(x_j\\) qui se situe dans les \\(5\\%\\) les plus basses ou les \\(5\\%\\) les plus élevées seront définies sur les quantiles à \\(5\\%\\) ou \\(95\\%\\) respectivement. En règle générale, vous pouvez choisir \\(\\delta = 0,025\\). De plus, les termes linéaires doivent être normalisés afin qu’ils aient la même importance préalable qu’une règle de décision typique :\n\\[l_j(x_j)= 0,4 . l^*_j(x_j) / std(l^*_j(x_j))\\]\nLe \\(0,4\\) est l’écart type moyen des règles avec une distribution de support uniforme de \\(s_k\\sim{}U(0,1)\\)\nNous combinons les deux types de fonctionnalités pour générer une nouvelle matrice de fonctionnalités et former un modèle linéaire clairsemé avec Lasso, avec la structure suivante :\n\\[\\hat{f}(x)=\\hat{\\beta}_0+\\sum_{k=1}^K\\hat{\\alpha}_k{}r_k(x)+\\sum_{j=1}^p\\hat{\\beta}_j{}l_j(x_j)\\]\noù \\(\\hat{\\alpha}\\) est le vecteur de poids estimé pour les caractéristiques de la règle et \\(\\hat{\\beta}\\) le vecteur de poids pour les caractéristiques d’origine. Puisque RuleFit utilise Lasso, la fonction de perte obtient la contrainte supplémentaire qui force certains poids à obtenir une estimation nulle :\n\\[\\begin{align*}\n(\\{\\hat{\\alpha}\\}_1^K,\\{\\hat{\\beta}\\}_0^p) & = & argmin_{\\{\\hat{\\alpha}\\}_1^K,\\{\\hat{\\beta}\\}_0^p}\\sum_{i=1}^n L(y^{(i)},f(x^{(i)}))  \\\\\n& + & \\lambda.\\left(\\sum_{k=1}^K|\\alpha_k| + \\sum_{j=1}^p|\\beta_j|\\right)\n\\end{align*}\\]\nLe résultat est un modèle linéaire qui a des effets linéaires pour toutes les fonctionnalités d’origine et pour les règles. L’interprétation est la même que pour les modèles linéaires, la seule différence est que certaines fonctionnalités sont désormais des règles binaires.\nStep 3 (optional): Feature importance\nPour les termes linéaires des caractéristiques d’origine, l’importance des caractéristiques est mesurée avec le prédicteur standardisé :\n\\[I_j=|\\hat{\\beta}_j|. std(l_j(x_j))\\]\noù \\(\\beta_j\\) est le poids du modèle Lasso et \\(std(l_j(x_j))\\) est l’écart type du terme linéaire sur les données.\nPour les termes de la règle de décision, l’importance est calculée avec la formule suivante :\n\\[I_k=|\\hat{\\alpha}_k|.\\sqrt{s_k(1-s_k)}\\]\noù \\(\\hat{\\alpha}_k\\) est le poids Lasso associé de la règle de décision et \\(s_k\\) est la prise en charge de la fonctionnalité dans les données, qui est le pourcentage de points de données auxquels la règle de décision s’applique (où \\(r_k(x)=1\\)) :\n\\[s_k=\\frac{1}{n}\\sum_{i=1}^n r_k(x^{(i)})\\]\nUne caractéristique apparaît sous la forme d’un terme linéaire et éventuellement également dans de nombreuses règles de décision. Comment mesurons-nous l’importance totale d’une fonctionnalité ? L’importance \\(J_j(x)\\) d’une caractéristique peut être mesurée pour chaque prédiction individuelle :\n\\[J_j(x)=I_j(x)+\\sum_{x_j\\in{}r_k}I_k(x)/m_k\\]\noù \\(I_l\\) est l’importance du terme linéaire et \\(I_k\\) l’importance des règles de décision dans lesquelles \\(x_j\\) apparaît, et \\(m_k\\) est le nombre de caractéristiques constituant la règle \\(r_k\\). L’ajout de l’importance des fonctionnalités de toutes les instances nous donne l’importance globale des fonctionnalités :\n\\[J_j(X)=\\sum_{i=1}^n{}J_j(x^{(i)})\\]\nIl est possible de sélectionner un sous-ensemble d’instances et de calculer l’importance des fonctionnalités pour ce groupe.\n\n\n5.6.3 - Avantages\nRuleFit ajoute automatiquement des interactions de fonctionnalités aux modèles linéaires. Par conséquent, cela résout le problème des modèles linéaires dans lesquels vous devez ajouter manuellement des termes d’interaction et cela aide un peu à résoudre le problème de la modélisation des relations non linéaires.\nRuleFit peut gérer à la fois les tâches de classification et de régression.\nLes règles créées sont faciles à interpréter, car ce sont des règles de décision binaires. Soit la règle s’applique à une instance, soit non. Une bonne interprétabilité n’est garantie que si le nombre de conditions au sein d’une règle n’est pas trop grand. Une règle avec 1 à 3 conditions me semble raisonnable. Cela signifie une profondeur maximale de 3 pour les arbres de l’ensemble d’arbres.\nMême s’il existe de nombreuses règles dans le modèle, elles ne s’appliquent pas à toutes les instances. Pour une instance individuelle, seule une poignée de règles s’appliquent (= avoir un poids non nul). Cela améliore l’interprétabilité locale.\nRuleFit propose un tas d’outils de diagnostic utiles. Ces outils sont indépendants du modèle, vous pouvez donc les trouver dans la section indépendante du modèle du livre : importance des fonctionnalités, diagrammes de dépendance partielle et interactions des fonctionnalités.\n\n\n5.6.4 - Inconvénients\nParfois, RuleFit crée de nombreuses règles qui ont un poids non nul dans le modèle Lasso. L’interprétabilité se dégrade avec l’augmentation du nombre de fonctionnalités dans le modèle. Une solution prometteuse consiste à forcer les effets de caractéristiques à être monotones, ce qui signifie qu’une augmentation d’une caractéristique doit conduire à une augmentation de la prédiction.\nUn inconvénient anecdotique : les journaux revendiquent de bonnes performances de RuleFit – souvent proches des performances prédictives des forêts aléatoires ! – mais dans les rares cas où je l’ai essayé personnellement, les performances ont été décevantes. Essayez-le simplement pour votre problème et voyez comment il fonctionne.\nLe produit final de la procédure RuleFit est un modèle linéaire doté de fonctionnalités supplémentaires (les règles de décision). Mais comme il s’agit d’un modèle linéaire, l’interprétation du poids reste peu intuitive. Il est livré avec la même « note de bas de page » qu’un modèle de régression linéaire habituel : « … étant donné que toutes les caractéristiques sont fixes. » Cela devient un peu plus délicat lorsque les règles se chevauchent. Par exemple, une règle de décision (fonctionnalité) pour la prévision du vélo pourrait être : « température &gt; 10 » et une autre règle pourrait être « température &gt; 15 & météo=‘GOOD’ ». S’il fait beau et que la température est supérieure à 15 degrés, la température est automatiquement supérieure à 10. Dans les cas où la deuxième règle s’applique, la première règle s’applique également. L’interprétation du poids estimé pour la deuxième règle est la suivante : « En supposant que toutes les autres caractéristiques restent fixes, le nombre prévu de vélos augmente de \\(\\beta_2\\) quand il fait beau et que la température est supérieure à 15 degrés. Mais maintenant, il devient très clair que « toutes les autres caractéristiques sont corrigées » est problématique, car si la règle 2 s’applique, la règle 1 s’applique également et l’interprétation est absurde.\n\n\n5.6.5 - Logiciels et alternatives\nL’algorithme RuleFit est implémenté dans R par Fokkema et Christoffersen (2017)2 et vous pouvez trouver une version Python sur GitHub.\nUn framework très similaire est skope-rules, un module Python qui extrait également les règles des ensembles. Il diffère dans la manière dont il apprend les règles finales : premièrement, les règles skope suppriment les règles peu performantes, sur la base de seuils de rappel et de précision. Ensuite, les règles en double et similaires sont supprimées en effectuant une sélection basée sur la diversité des termes logiques (variable + opérateur plus grand/plus petit) et les performances (score F1) des règles. Cette dernière étape ne repose pas sur l’utilisation de Lasso, mais considère uniquement le score F1 sorti du sac et les termes logiques qui forment les règles.\nLe package imodels contient également des implémentations d’autres ensembles de règles, tels que les ensembles de règles bayésiennes, les ensembles de règles boostés et les ensembles de règles SLIPPER en tant que package Python avec une interface scikit-learn unifiée.",
    "crumbs": [
      "5 - Modèles interprétables",
      "5.6 - Ajustement des règles"
    ]
  },
  {
    "objectID": "05-interpretable_models/05.6-rulefit.html#footnotes",
    "href": "05-interpretable_models/05.6-rulefit.html#footnotes",
    "title": "Apprentissage automatique interprétable",
    "section": "Notes de bas de page",
    "text": "Notes de bas de page\n\n\nFriedman, Jerome H, and Bogdan E Popescu. “Predictive learning via rule ensembles.” The Annals of Applied Statistics. JSTOR, 916–54. (2008).↩︎\nFokkema, Marjolein, and Benjamin Christoffersen. “Pre: Prediction rule ensembles”. https://CRAN.R-project.org/package=pre (2017).↩︎",
    "crumbs": [
      "5 - Modèles interprétables",
      "5.6 - Ajustement des règles"
    ]
  },
  {
    "objectID": "05-interpretable_models/05.4-decision-tree.html",
    "href": "05-interpretable_models/05.4-decision-tree.html",
    "title": "Apprentissage automatique interprétable",
    "section": "",
    "text": "Avertissement\n\n\n\nEn cours de traduction.",
    "crumbs": [
      "5 - Modèles interprétables",
      "5.4 - Arbre de décision"
    ]
  },
  {
    "objectID": "05-interpretable_models/05.4-decision-tree.html#arbre-de-décision",
    "href": "05-interpretable_models/05.4-decision-tree.html#arbre-de-décision",
    "title": "Apprentissage automatique interprétable",
    "section": "5.4 - Arbre de décision",
    "text": "5.4 - Arbre de décision\nLes modèles de régression linéaire et de régression logistique échouent dans des situations où la relation entre les caractéristiques et le résultat n’est pas linéaire ou lorsque les caractéristiques interagissent les unes avec les autres. Il est temps de briller pour l’arbre de décision ! Les modèles basés sur les arbres divisent les données à plusieurs reprises en fonction de certaines valeurs de seuil dans les caractéristiques. En divisant, différents sous-ensembles de l’ensemble de données sont créés, chaque instance appartenant à un sous-ensemble. Les sous-ensembles finaux sont appelés noeuds terminaux ou feuilles, et les sous-ensembles intermédiaires sont appelés noeuds internes ou noeuds de division. Pour prédire le résultat dans chaque noeud feuille, on utilise la moyenne des résultats des données d’entraînement dans ce noeud. Les arbres peuvent être utilisés pour la classification et la régression.\nIl existe plusieurs algorithmes qui peuvent faire croître un arbre. Ils diffèrent dans la structure possible de l’arbre (par exemple, le nombre de divisions par noeud), les critères pour trouver les divisions, quand arrêter les divisions et comment estimer les modèles simples au sein des noeuds feuilles. L’algorithme de classification et de régression par arbres (CART) est probablement l’algorithme le plus populaire pour l’induction d’arbres. Nous nous concentrerons sur CART, mais l’interprétation est similaire pour la plupart des autres types d’arbres. Je recommande le livre ‘Les éléments de l’apprentissage statistique’ (Friedman, Hastie et Tibshirani 2009) 1 pour une introduction plus détaillée à CART.\n\n\n\nArbre de décision avec données artificielles. Les instances avec une valeur supérieure à 3 pour la fonctionnalité x1 se retrouvent dans le noeud 5. Toutes les autres instances sont affectées au noeud 3 ou au noeud 4, selon que les valeurs de la fonctionnalité x2 dépassent 1.\n\n\nLa formule suivante décrit la relation entre la sortie \\(\\hat{y}\\) et les caractéristiques \\(x\\).\n\\[\\hat{y}=\\hat{f}(x)=\\sum_{m=1}^Mc_m{}I_{\\{x\\in{}R_m\\}}\\]\nChaque instance tombe exactement dans un noeud feuille (=sous-ensemble \\(R_m\\)). \\(I_{\\{x\\in{}R_m\\}}\\) est la fonction identité qui renvoie 1 si \\(x\\) est dans le sous-ensemble \\(R_m\\) et 0 sinon. Si une instance tombe dans un noeud feuille \\(R_l\\), la valeur prédite est \\(\\hat{y}=c_l\\), où \\(c_l\\) est la moyenne de toutes les instances d’entraînement dans le noeud feuille \\(R_l\\).\nMais d’où viennent les sous-ensembles ? C’est assez simple : CART prend une caractéristique et détermine le point de coupure qui minimise la variance de y pour une tâche de régression ou l’indice de Gini de la distribution de classe de y pour les tâches de classification. La variance nous indique dans quelle mesure les valeurs y dans un noeud sont réparties autour de leur valeur moyenne. L’indice de Gini nous indique à quel point un noeud est “impur”, par exemple, si toutes les classes ont la même fréquence, le noeud est impur, s’il n’y a qu’une seule classe présente, il est maximalement pur. La variance et l’indice de Gini sont minimisés lorsque les points de données dans les noeuds ont des valeurs très similaires pour y. En conséquence, le meilleur point de coupure rend les deux sous-ensembles résultants aussi différents que possible par rapport au résultat cible. Pour les caractéristiques catégorielles, l’algorithme essaie de créer des sous-ensembles en essayant différentes regroupements de catégories. Une fois que le meilleur point de coupure par caractéristique a été déterminé, l’algorithme sélectionne la caractéristique à diviser qui donnerait la meilleure partition en termes de variance ou d’indice de Gini, et ajoute cette division à l’arbre. L’algorithme continue cette recherche et cette division de manière récursive dans les deux nouveaux noeuds jusqu’à ce qu’un critère d’arrêt soit atteint. Les critères possibles sont : un nombre minimum d’instances qui doivent être dans un noeud avant la division, ou le nombre minimum d’instances qui doivent être dans un noeud terminal.\n\n5.4.1 - Interprétation\nL’interprétation est simple : en partant du noeud racine, vous passez aux noeuds suivants et les arêtes vous indiquent quels sous-ensembles vous regardez. Une fois que vous atteignez le noeud feuille, le noeud vous indique le résultat prévu. Tous les bords sont reliés par “ET”.\nModèle : si la caractéristique x est [plus petite/plus grande] que le seuil c ET… alors le résultat prédit est la valeur moyenne de y des instances dans ce noeud.\nImportance des caractéristiques\nL’importance globale d’une caractéristique dans un arbre de décision peut être calculée de la manière suivante : parcourez toutes les divisions pour lesquelles la fonctionnalité a été utilisée et mesurez dans quelle mesure elle a réduit la variance ou l’indice de Gini par rapport au noeud parent. La somme de toutes les importances est réduite à 100. Cela signifie que chaque importance peut être interprétée comme une part de l’importance globale du modèle.\nDécomposition d’un arbre\nLes prédictions individuelles d’un arbre de décision peuvent être expliquées en décomposant le chemin de décision en un composant par fonctionnalité. Nous pouvons suivre une décision à travers l’arbre et expliquer une prédiction par les contributions ajoutées à chaque noeud de décision.\nLe noeud racine d’un arbre de décision est notre point de départ. Si nous devions utiliser le noeud racine pour faire des prédictions, il prédirait la moyenne du résultat des données d’entraînement. Lors de la division suivante, nous soustrayons ou ajoutons un terme à cette somme, en fonction du noeud suivant sur le chemin. Pour arriver à la prédiction finale, nous devons suivre le chemin de l’instance de données que nous voulons expliquer et continuer à ajouter à la formule.\n\\[\\hat{f}(x)=\\bar{y}+\\sum_{d=1}^D\\text{split.contrib(d,x)}=\\bar{y}+\\sum_{j=1}^p\\text{feat.contrib(j,x)}\\]\nLa prédiction d’une instance individuelle est la moyenne du résultat cible plus la somme de toutes les contributions des divisions \\(D\\) qui se produisent entre le noeud racine et le noeud terminal où aboutit l’instance. Cependant, nous ne sommes pas intéressés par les contributions fractionnées, mais par les contributions de fonctionnalités. Une fonctionnalité peut être utilisée pour plusieurs divisions, voire pas du tout. Nous pouvons ajouter les contributions pour chacune des \\(p\\) fonctionnalités et obtenir une interprétation de la contribution de chaque fonctionnalité à une prédiction.\n\n\n5.4.2 - Exemple\nExaminons à nouveau les données sur la location de vélos. Nous voulons prédire le nombre de vélos loués un certain jour avec un arbre de décision. L’arbre appris ressemble à ceci :\n\n\n\nArbre de régression ajusté sur les données de location de vélos. La profondeur maximale autorisée pour l’arbre a été fixée à 2. La fonction de tendance (jours depuis 2011) et la température (temp) ont été sélectionnées pour les divisions. Les boîtes à moustaches montrent la distribution du nombre de vélos dans le nœud terminal.\n\n\nLa première division et l’une des deuxièmes divisions ont été réalisées avec la fonction de tendance, qui compte les jours depuis le début de la collecte de données et couvre la tendance selon laquelle le service de location de vélos est devenu plus populaire au fil du temps. Pour les jours précédant le 105ème jour, le nombre prévu de vélos est d’environ 1800, entre le 106ème et le 430ème jour, il est d’environ 3900. Pour les jours après le 430ème jour, la prévision est soit de 4600 (si la température est inférieure à 12 degrés), soit de 6600. (si la température est supérieure à 12 degrés).\nL’importance de la fonctionnalité nous indique dans quelle mesure une fonctionnalité a contribué à améliorer la pureté de tous les nœuds. Ici, la variance a été utilisée, puisque la prévision des locations de vélos est une tâche de régression.\nL’arbre visualisé montre que la tendance de la température et du temps a été utilisée pour les divisions, mais ne quantifie pas quelle caractéristique était la plus importante. La mesure de l’importance des caractéristiques montre que la tendance temporelle est bien plus importante que la température.\n\n\n\nImportance des caractéristiques mesurées par l’amélioration moyenne de la pureté du noeud.\n\n\n\n\n5.4.3 - Avantages\nLa structure arborescente est idéale pour capturer les interactions entre les fonctionnalités des données.\nLes données se retrouvent dans des groupes distincts qui sont souvent plus faciles à comprendre que des points sur un hyperplan multidimensionnel comme dans la régression linéaire. L’interprétation est sans doute assez simple.\nL’arborescence possède également une visualisation naturelle, avec ses nœuds et ses arêtes.\nLes arbres créent de bonnes explications, comme défini dans le chapitre « Explications respectueuses de l’homme ». La structure arborescente invite automatiquement à considérer les valeurs prédites pour des instances individuelles comme des contrefactuels : « Si une caractéristique avait été supérieure/inférieure au point de partage, la prédiction aurait été y1 au lieu de y2. » Les explications de l’arbre sont contrastées, puisque vous pouvez toujours comparer la prédiction d’une instance avec des scénarios « et si » pertinents (tels que définis par l’arbre) qui sont simplement les autres nœuds feuilles de l’arbre. Si l’arbre est court, comme une à trois fentes de profondeur, les explications qui en résultent sont sélectives. Un arbre d’une profondeur de trois nécessite un maximum de trois entités et points de partage pour créer l’explication de la prédiction d’une instance individuelle. La véracité de la prédiction dépend des performances prédictives de l’arbre. Les explications pour les arbres courts sont très simples et générales, car pour chaque division, l’instance tombe dans l’une ou l’autre feuille, et les décisions binaires sont faciles à comprendre.\nIl n’est pas nécessaire de transformer les fonctionnalités. Dans les modèles linéaires, il est parfois nécessaire de prendre le logarithme d’une caractéristique. Un arbre de décision fonctionne aussi bien avec n’importe quelle transformation monotone d’une fonctionnalité.\n\n\n5.4.4 - Inconvénients\nLes arbres ne parviennent pas à gérer les relations linéaires. Toute relation linéaire entre une caractéristique d’entrée et le résultat doit être approximée par des divisions, créant ainsi une fonction en escalier. Ce n’est pas efficace.\nCela va de pair avec un manque de douceur. De légers changements dans la fonctionnalité d’entrée peuvent avoir un impact important sur le résultat prévu, ce qui n’est généralement pas souhaitable. Imaginez un arbre qui prédit la valeur d’une maison et l’arbre utilise la taille de la maison comme l’une des caractéristiques de division. La division se produit à 100,5 mètres carrés. Imaginez l’utilisateur d’un estimateur de prix de maison utilisant votre modèle d’arbre de décision : il mesure sa maison, arrive à la conclusion que la maison a 99 mètres carrés, la saisit dans le calculateur de prix et obtient une prévision de 200 000 euros. Les utilisateurs remarquent qu’ils ont oublié de mesurer un petit débarras de 2 mètres carrés. Le débarras a un mur en pente, ils ne savent donc pas s’ils peuvent compter toute la surface ou seulement la moitié. Ils décident donc d’essayer 100,0 et 101,0 mètres carrés. Les résultats : Le calculateur de prix indique 200 000 euros et 205 000 euros, ce qui n’est pas intuitif, car il n’y a eu aucun changement de 99 mètres carrés à 100.\nLes arbres sont également assez instables. Quelques modifications dans l’ensemble de données d’entraînement peuvent créer une arborescence complètement différente. En effet, chaque répartition dépend de la répartition parent. Et si une autre fonctionnalité est sélectionnée comme première fonctionnalité divisée, toute la structure arborescente change. Cela ne crée pas de confiance dans le modèle si la structure change si facilement.\nLes arbres de décision sont très interprétables – à condition qu’ils soient courts. Le nombre de nœuds terminaux augmente rapidement avec la profondeur. Plus il y a de nœuds terminaux et plus l’arbre est profond, plus il devient difficile de comprendre les règles de décision d’un arbre. Une profondeur de 1 signifie 2 nœuds terminaux. Une profondeur de 2 signifie max. 4 nœuds. Une profondeur de 3 signifie max. 8 nœuds. Le nombre maximum de nœuds terminaux dans un arbre est de 2 à la puissance de la profondeur.\n\n\n5.4.5 - Logiciels\nPour les exemples de ce chapitre, j’ai utilisé le module R rpart qui implémente CART (arbres de classification et de régression). CART est implémenté dans de nombreux langages de programmation, dont Python. On peut soutenir que CART est un algorithme assez ancien et quelque peu obsolète et il existe de nouveaux algorithmes intéressants pour ajuster les arbres. Vous pouvez trouver un aperçu de certains modiles R pour les arbres de décision dans la vue des tâches CRAN d’apprentissage automatique et d’apprentissage statistique sous le mot-clé « Partitionnement récursif ». En Python, le module imodels fournit divers algorithmes pour faire croître les arbres de décision (par exemple, ajustement gourmand ou optimal), élaguer les arbres et régulariser les arbres.\n\n\n\n\n\n\nNote du traducteur\n\n\n\nLa version en Python des exemples de ce chapitre sont accessibles ici.",
    "crumbs": [
      "5 - Modèles interprétables",
      "5.4 - Arbre de décision"
    ]
  },
  {
    "objectID": "05-interpretable_models/05.4-decision-tree.html#footnotes",
    "href": "05-interpretable_models/05.4-decision-tree.html#footnotes",
    "title": "Apprentissage automatique interprétable",
    "section": "Notes de bas de page",
    "text": "Notes de bas de page\n\n\nFriedman, Jerome, Trevor Hastie, and Robert Tibshirani. “The elements of statistical learning”. hastie.su.domains/ElemStatLearn (2009).↩︎",
    "crumbs": [
      "5 - Modèles interprétables",
      "5.4 - Arbre de décision"
    ]
  },
  {
    "objectID": "04-datasets/04.3-datasets.html",
    "href": "04-datasets/04.3-datasets.html",
    "title": "Apprentissage automatique interprétable",
    "section": "",
    "text": "Avertissement\n\n\n\nEn cours de traduction.",
    "crumbs": [
      "4 - Jeux de données",
      "4.3 - Facteurs de Risque du Cancer du Col de l'Uterus (Classification)"
    ]
  },
  {
    "objectID": "04-datasets/04.3-datasets.html#facteurs-de-risque-du-cancer-du-col-de-luterus-classification",
    "href": "04-datasets/04.3-datasets.html#facteurs-de-risque-du-cancer-du-col-de-luterus-classification",
    "title": "Apprentissage automatique interprétable",
    "section": "4.3 - Facteurs de Risque du Cancer du Col de l’Uterus (Classification)",
    "text": "4.3 - Facteurs de Risque du Cancer du Col de l’Uterus (Classification)\nL’ensemble de données sur le cancer du col de l’utérus contient des indicateurs et des facteurs de risque pour prédire si une femme va développer un cancer du col de l’utérus. Les caractéristiques comprennent des données démographiques (telles que l’âge), le mode de vie et les antécédents médicaux. Les données peuvent être téléchargées depuis le dépôt UCI Machine Learning repository et sont décrites par Fernandes, Cardoso, et Fernandes (2017)1.\nLes caractéristiques des données utilisées dans les exemples du livre sont :\n\nÂge en années\nNombre de partenaires sexuels\nPremier rapport sexuel (âge en années)\nNombre de grossesses\nFumeur oui ou non\nTabagisme (en années)\nContraceptifs hormonaux oui ou non\nContraceptifs hormonaux (en années)\nDispositif intra-utérin oui ou non (DIU)\nNombre d’années avec un dispositif intra-utérin (DIU)\nLa patiente a-t-elle déjà eu une maladie sexuellement transmissible (MST) oui ou non\nNombre de diagnostics de MST\nTemps depuis le premier diagnostic de MST\nTemps depuis le dernier diagnostic de MST\nLes résultats de la biopsie “Saine” ou “Cancer”. Résultat cible.\n\nLa biopsie sert de référence pour le diagnostic du cancer du col de l’utérus. Pour les exemples de ce livre, le résultat de la biopsie a été utilisé comme cible. Les valeurs manquantes pour chaque colonne ont été imputées par le mode (valeur la plus fréquente), ce qui est probablement une mauvaise solution, car la vraie réponse pourrait être corrélée avec la probabilité qu’une valeur manque. Il y a probablement un biais car les questions sont de nature très privée. Mais ce n’est pas un livre sur l’imputation des données manquantes, donc l’imputation par le mode devra suffire pour les exemples.\nPour reproduire les exemples de ce livre avec cet ensemble de données, retrouvez le script de prétraitement R et le fichier RData final dans le dépôt GitHub du livre.\n\n\n\n\n\n\nNote du traducteur\n\n\n\nVersion python + FR",
    "crumbs": [
      "4 - Jeux de données",
      "4.3 - Facteurs de Risque du Cancer du Col de l'Uterus (Classification)"
    ]
  },
  {
    "objectID": "04-datasets/04.3-datasets.html#footnotes",
    "href": "04-datasets/04.3-datasets.html#footnotes",
    "title": "Apprentissage automatique interprétable",
    "section": "Notes de bas de page",
    "text": "Notes de bas de page\n\n\nFernandes, Kelwin, Jaime S Cardoso, and Jessica Fernandes. “Transfer learning with partial observability applied to cervical cancer screening.” In Iberian Conference on Pattern Recognition and Image Analysis, 243–50. Springer. (2017).↩︎",
    "crumbs": [
      "4 - Jeux de données",
      "4.3 - Facteurs de Risque du Cancer du Col de l'Uterus (Classification)"
    ]
  },
  {
    "objectID": "04-datasets/04.1-datasets.html",
    "href": "04-datasets/04.1-datasets.html",
    "title": "Apprentissage automatique interprétable",
    "section": "",
    "text": "Avertissement\n\n\n\nEn cours de traduction.",
    "crumbs": [
      "4 - Jeux de données",
      "4.1 - Location de vélo (Régression)"
    ]
  },
  {
    "objectID": "04-datasets/04.1-datasets.html#location-de-vélo-régression",
    "href": "04-datasets/04.1-datasets.html#location-de-vélo-régression",
    "title": "Apprentissage automatique interprétable",
    "section": "4.1 - Location de vélo (Régression)",
    "text": "4.1 - Location de vélo (Régression)\nCe jeu de données contient les comptages quotidiens de vélos loués par la société de location de vélos Capital-Bikeshare à Washington D.C., ainsi que des informations météorologiques et saisonnières. Les données ont été aimablement rendues publiques par Capital-Bikeshare. Fanaee-T et Gama (2013)1 ont ajouté des données météorologiques et des informations saisonnières. L’objectif est de prédire combien de vélos seront loués en fonction de la météo et du jour. Les données peuvent être téléchargées depuis le dépôt UCI Machine Learning Repository.\nDe nouvelles caractéristiques ont été ajoutées à l’ensemble de données et toutes les caractéristiques originales n’ont pas été utilisées pour les exemples de ce livre. Voici la liste des caractéristiques qui ont été utilisées :\n\nCompte des vélos incluant les utilisateurs occasionnels et enregistrés. Le compte est utilisé comme cible dans la tâche de régression.\nLa saison, soit printemps, été, automne ou hiver.\nIndicateur indiquant si le jour était un jour férié ou non.\nL’année, soit 2011 ou 2012.\nNombre de jours depuis le 01.01.2011 (le premier jour de l’ensemble de données). Cette caractéristique a été introduite pour tenir compte de la tendance au fil du temps.\nIndicateur indiquant si le jour était un jour ouvrable ou un week-end.\nLa situation météorologique de ce jour. L’une des suivantes :\n\nclair, peu de nuages, partiellement nuageux, nuageux\nbrume + nuages, brume + nuages brisés, brume + peu de nuages, brume\nneige légère, pluie légère + orage + nuages dispersés, pluie légère + nuages dispersés\npluie forte + grêle + orage + brume, neige + brume\n\nTempérature en degrés Celsius.\nHumidité relative en pourcentage (de 0 à 100).\nVitesse du vent en km par heure.\n\nPour les exemples de ce livre, les données ont été légèrement traitées. Vous pouvez trouver le script R de traitement dans le dépôt GitHub du livre ainsi que le fichier RData final.\n\n\n\n\n\n\nNote du traducteur\n\n\n\nVersion python + FR",
    "crumbs": [
      "4 - Jeux de données",
      "4.1 - Location de vélo (Régression)"
    ]
  },
  {
    "objectID": "04-datasets/04.1-datasets.html#footnotes",
    "href": "04-datasets/04.1-datasets.html#footnotes",
    "title": "Apprentissage automatique interprétable",
    "section": "Notes de bas de page",
    "text": "Notes de bas de page\n\n\nFanaee-T, Hadi, and Joao Gama. “Event labeling combining ensemble detectors and background knowledge.” Progress in Artificial Intelligence. Springer Berlin Heidelberg, 1–15. https://doi.org/10.1007/s13748-013-0040-3. (2013).↩︎",
    "crumbs": [
      "4 - Jeux de données",
      "4.1 - Location de vélo (Régression)"
    ]
  },
  {
    "objectID": "03-interpretability/03.6-human_friendly_explanations.html",
    "href": "03-interpretability/03.6-human_friendly_explanations.html",
    "title": "Apprentissage automatique interprétable",
    "section": "",
    "text": "Avertissement\n\n\n\nEn cours de traduction.",
    "crumbs": [
      "3 - Interprétabilité",
      "3.6 - Explications conviviales pour l'être humain"
    ]
  },
  {
    "objectID": "03-interpretability/03.6-human_friendly_explanations.html#explications-conviviales-pour-lêtre-humain",
    "href": "03-interpretability/03.6-human_friendly_explanations.html#explications-conviviales-pour-lêtre-humain",
    "title": "Apprentissage automatique interprétable",
    "section": "3.6 - Explications conviviales pour l’être humain",
    "text": "3.6 - Explications conviviales pour l’être humain\nCreusons plus profondément et découvrons ce que nous, humains, considérons comme de “bonnes” explications et quelles sont les implications pour l’apprentissage automatique interprétable. La recherche en sciences humaines peut nous aider à le découvrir. Miller (2017)1 a mené une vaste enquête sur les publications concernant les explications, et ce chapitre s’appuie sur son résumé.\nDans ce chapitre, je veux vous convaincre de ce qui suit : En tant qu’explication d’un événement, les humains préfèrent des explications courtes (seulement 1 ou 2 causes) qui contrastent la situation actuelle avec une situation dans laquelle l’événement ne se serait pas produit. Surtout, les causes anormales fournissent de bonnes explications. Les explications sont des interactions sociales entre l’explicateur et le destinataire de l’explication, et donc le contexte social a une grande influence sur le contenu réel de l’explication.\nLorsque vous avez besoin d’explications avec TOUS les facteurs pour une prédiction ou un comportement particulier, vous ne voulez pas une explication conviviale pour l’homme, mais une attribution causale complète. Vous voulez probablement une attribution causale si vous êtes légalement tenu de spécifier toutes les caractéristiques influençant ou si vous déboguez le modèle d’apprentissage automatique. Dans ce cas, ignorez les points suivants. Dans tous les autres cas, où les personnes profanes ou les personnes ayant peu de temps sont les destinataires de l’explication, les sections suivantes devraient vous intéresser.\n\n3.6.1 - Qu’est-ce qu’une explication ?\nUne explication est la réponse à une question en ‘pourquoi’ (Miller 2017).\n\nPourquoi le traitement n’a-t-il pas fonctionné sur le patient ?\nPourquoi mon prêt a-t-il été refusé ?\nPourquoi n’avons-nous pas encore été contactés par une vie extraterrestre ?\n\nLes deux premières questions peuvent être répondues avec une explication “quotidienne”, tandis que la troisième provient de la catégorie “Phénomènes scientifiques généraux et questions philosophiques”. Nous nous concentrons sur les explications de type “quotidien”, car elles sont pertinentes pour l’apprentissage automatique interprétable. Les questions commençant par “comment” peuvent généralement être reformulées en questions en “pourquoi” : “Comment mon prêt a-t-il été refusé ?” peut être transformé en “Pourquoi mon prêt a-t-il été refusé ?”.\nDans la suite, le terme “explication” se réfère au processus social et cognitif d’explication, mais aussi au produit de ces processus. L’explicateur peut être un être humain ou une machine.\n\n\n3.6.2 - Qu’est-ce qu’une bonne explication ?\nCette section condense davantage le résumé de Miller sur les “bonnes” explications et ajoute des implications concrètes pour l’apprentissage automatique interprétable.\nLes explications sont contrastives (Lipton 19902). Les humains ne demandent généralement pas pourquoi une certaine prédiction a été faite, mais pourquoi cette prédiction a été faite au lieu d’une autre prédiction. Nous avons tendance à penser en termes de cas contrefactuels, c’est-à-dire “Comment aurait été la prédiction si l’entrée X avait été différente ?”. Pour une prédiction de prix de maison, le propriétaire pourrait être intéressé de savoir pourquoi le prix prédit était élevé par rapport au prix plus bas qu’il avait anticipé. Si ma demande de prêt est rejetée, je ne me soucie pas d’entendre tous les facteurs qui parlent généralement pour ou contre un rejet. Je suis intéressé par les facteurs dans ma demande qui devraient changer pour obtenir le prêt. Je veux connaître le contraste entre ma demande et la version de ma demande qui aurait été acceptée. La reconnaissance que les explications contrastives sont importantes est une découverte importante pour l’apprentissage automatique explicable. À partir de la plupart des modèles interprétables, vous pouvez extraire une explication qui contraste implicitement la prédiction d’une instance avec la prédiction d’une instance de données artificielle ou d’une moyenne d’instances. Les médecins pourraient demander : “Pourquoi le médicament n’a-t-il pas fonctionné pour mon patient ?”. Et ils pourraient vouloir une explication qui contraste leur patient avec un patient pour lequel le médicament a fonctionné et qui est similaire au patient non répondeur. Les explications contrastives sont plus faciles à comprendre que les explications complètes. Une explication complète de la question du médecin sur pourquoi le médicament ne fonctionne pas pourrait inclure : Le patient a eu la maladie pendant 10 ans, 11 gènes sont sur-exprimés, le corps du patient décompose très rapidement le médicament en produits chimiques inefficaces, … Une explication contrastive pourrait être beaucoup plus simple : Contrairement au patient répondeur, le patient non répondeur a une certaine combinaison de gènes qui rendent le médicament moins efficace. La meilleure explication est celle qui met en évidence la plus grande différence entre l’objet d’intérêt et l’objet de référence. Ce que cela signifie pour l’apprentissage automatique interprétable : Les humains ne veulent pas d’une explication complète pour une prédiction, mais veulent comparer quelles étaient les différences par rapport à la prédiction d’une autre instance (qui peut être artificielle). Créer des explications contrastives dépend de l’application car cela nécessite un point de référence pour la comparaison. Et cela peut dépendre du point de données à expliquer, mais aussi de l’utilisateur recevant l’explication. Un utilisateur d’un site web de prédiction de prix de maison pourrait vouloir une explication de la prédiction d’un prix de maison contrastive par rapport à sa propre maison, ou peut-être par rapport à une autre maison sur le site, ou peut-être par rapport à une maison moyenne dans le quartier. La solution pour la création automatisée d’explications contrastives pourrait également impliquer la recherche de prototypes ou d’archétypes dans les données.\nLes explications sont sélectionnées. Les gens ne s’attendent pas à des explications qui couvrent la liste complète et réelle des causes d’un événement. Nous avons l’habitude de sélectionner une ou deux causes parmi une variété de causes possibles comme L’explication. Comme preuve, allumez les actualités télévisées : “La baisse des prix des actions est attribuée à un mécontentement croissant contre le produit de l’entreprise en raison de problèmes avec la dernière mise à jour du logiciel.” “Tsubasa et son équipe ont perdu le match à cause d’une défense faible : ils ont laissé trop de place à leurs adversaires pour déployer leur stratégie.” “La méfiance croissante envers les institutions établies et notre gouvernement sont les principaux facteurs qui ont réduit la participation électorale.” Le fait qu’un événement puisse être expliqué par diverses causes est appelé l’Effet Rashomon. Rashomon est un film japonais qui raconte des histoires alternatives et contradictoires (explications) sur la mort d’un samouraï. Pour les modèles d’apprentissage automatique, il est avantageux si une bonne prédiction peut être faite à partir de différentes caractéristiques. Les méthodes d’ensemble qui combinent plusieurs modèles avec différentes caractéristiques (différentes explications) se comportent généralement bien car la moyenne de ces “histoires” rend les prédictions plus robustes et précises. Mais cela signifie aussi qu’il y a plus d’une explication sélective pour laquelle une certaine prédiction a été faite. Ce que cela signifie pour l’apprentissage automatique interprétable : Rendez l’explication très courte, donnez seulement 1 à 3 raisons, même si le monde est plus complexe. La méthode LIME fait du bon travail à cet égard.\nLes explications sont sociales. Elles font partie d’une conversation ou interaction entre l’explicateur et le destinataire de l’explication. Le contexte social détermine le contenu et la nature des explications. Si je voulais expliquer à une personne technique pourquoi les cryptomonnaies numériques valent tant, je dirais des choses comme : “Le registre décentralisé, distribué et basé sur la blockchain, qui ne peut être contrôlé par une entité centrale, résonne avec les personnes qui veulent sécuriser leur richesse, ce qui explique la forte demande et le prix élevé.” Mais à ma grand-mère, je dirais : “Regarde, Mamie : les cryptomonnaies, c’est un peu comme de l’or informatique. Les gens aiment et paient cher pour l’or, et les jeunes aiment et paient cher pour l’or informatique.” Ce que cela signifie pour l’apprentissage automatique interprétable : Faites attention à l’environnement social de votre application d’apprentissage automatique et au public cible. Bien cerner la partie sociale du modèle d’apprentissage automatique dépend entièrement de votre application spécifique. Trouvez des experts en sciences humaines (par exemple, des psychologues et des sociologues) pour vous aider.\nLes explications se concentrent sur l’anormal. Les gens se concentrent davantage sur les causes anormales pour expliquer les événements (Kahnemann et Tversky, 19813). Ce sont des causes qui avaient une faible probabilité mais qui se sont néanmoins produites. L’élimination de ces causes anormales aurait grandement changé le résultat (explication contrefactuelle). Les humains considèrent ces types de causes “anormales” comme de bonnes explications. Un exemple de Štrumbelj et Kononenko (2011)4 est : supposons que nous ayons un ensemble de données de situations de test entre enseignants et étudiants. Les étudiants suivent un cours et réussissent le cours directement après avoir réussi une présentation. L’enseignant a la possibilité de poser en plus des questions à l’étudiant pour tester ses connaissances. Les étudiants qui ne peuvent pas répondre à ces questions échoueront au cours. Les étudiants peuvent avoir différents niveaux de préparation, ce qui se traduit par différentes probabilités de répondre correctement aux questions de l’enseignant (s’ils décident de tester l’étudiant). Nous voulons prédire si un étudiant réussira le cours et expliquer notre prédiction. La chance de réussir est de \\(100\\%\\) si l’enseignant ne pose aucune question supplémentaire, sinon la probabilité de réussir dépend du niveau de préparation de l’étudiant et de la probabilité résultante de répondre correctement aux questions.\n- Scénario 1 : L’enseignant pose généralement des questions supplémentaires aux étudiants (par exemple, 95 fois sur 100). Un étudiant qui n’a pas étudié (\\(10\\%\\) de chances de réussir la partie questions) n’était pas l’un des chanceux et reçoit des questions supplémentaires auxquelles il ne parvient pas à répondre correctement. Pourquoi l’étudiant a-t-il échoué au cours ? Je dirais que c’est la faute de l’étudiant de ne pas avoir étudié.\n- Scénario 2 : L’enseignant pose rarement des questions supplémentaires (par exemple, 2 fois sur 100). Pour un étudiant qui n’a pas étudié pour les questions, nous prédirions une forte probabilité de réussir le cours parce que les questions sont peu probables. Bien sûr, l’un des étudiants ne s’est pas préparé pour les questions, ce qui lui donne \\(10\\%\\) de chances de réussir les questions. Il a la malchance et l’enseignant pose des questions supplémentaires que l’étudiant ne peut pas répondre et il échoue au cours. Quelle est la raison de l’échec ? Je dirais que maintenant, la meilleure explication est “parce que l’enseignant a testé l’étudiant”. Il était peu probable que l’enseignant teste, donc l’enseignant s’est comporté anormalement.\nCe que cela signifie pour l’apprentissage automatique interprétable : Si l’une des caractéristiques d’entrée pour une prédiction était anormale en quelque sorte (comme une catégorie rare d’une caractéristique catégorielle) et que la caractéristique a influencé la prédiction, elle devrait être incluse dans une explication, même si d’autres caractéristiques ‘normales’ ont la même influence sur la prédiction que l’anormale. Une caractéristique anormale dans notre exemple de prédiction de prix de maison pourrait être qu’une maison plutôt chère a deux balcons. Même si une méthode d’attribution trouve que les deux balcons contribuent autant à la différence de prix que la taille de la maison supérieure à la moyenne, le bon quartier ou la rénovation récente, la caractéristique anormale “deux balcons” pourrait être la meilleure explication de pourquoi la maison est si chère.\nLes explications sont véridiques. De bonnes explications se révèlent vraies dans la réalité (c’est-à-dire dans d’autres situations). Mais de manière troublante, ce n’est pas le facteur le plus important pour une “bonne” explication. Par exemple, la sélectivité semble être plus importante que la véracité. Une explication qui ne sélectionne qu’une ou deux causes possibles couvre rarement la liste complète des causes pertinentes. La sélectivité omet une partie de la vérité. Ce n’est pas vrai que seulement un ou deux facteurs, par exemple, ont causé un krach boursier, mais la vérité est qu’il y a des millions de causes qui influencent des millions de personnes à agir de telle manière qu’au final un krach est causé.\nCe que cela signifie pour l’apprentissage automatique interprétable : L’explication devrait prédire l’événement aussi fidèlement que possible, ce qui en apprentissage automatique est parfois appelé fidélité. Donc, si nous disons qu’un deuxième balcon augmente le prix d’une maison, cela devrait également s’appliquer à d’autres maisons (ou du moins à des maisons similaires). Pour les humains, la fidélité d’une explication n’est pas aussi importante que sa sélectivité, son contraste et son aspect social.\nDe bonnes explications sont générales et probables. Une cause qui peut expliquer de nombreux événements est très générale et pourrait être considérée comme une bonne explication. Notez que cela contredit l’affirmation selon laquelle les causes anormales constituent de bonnes explications. À mon avis, les causes anormales l’emportent sur les causes générales. Les causes anormales sont par définition rares dans le scénario donné. En l’absence d’un événement anormal, une explication générale est considérée comme une bonne explication. Rappelez-vous également que les gens ont tendance à mal juger les probabilités d’événements conjoints. (Joe est bibliothécaire. Est-il plus susceptible d’être une personne timide ou une personne timide qui aime lire des livres ?) Un bon exemple est “La maison est chère parce qu’elle est grande”, qui est une explication très générale et bonne de pourquoi les maisons sont chères ou bon marché.\nCe que cela signifie pour l’apprentissage automatique interprétable : La généralité peut facilement être mesurée par le support de la caractéristique, qui est le nombre d’instances auxquelles l’explication s’applique divisé par le nombre total d’instances.",
    "crumbs": [
      "3 - Interprétabilité",
      "3.6 - Explications conviviales pour l'être humain"
    ]
  },
  {
    "objectID": "03-interpretability/03.6-human_friendly_explanations.html#footnotes",
    "href": "03-interpretability/03.6-human_friendly_explanations.html#footnotes",
    "title": "Apprentissage automatique interprétable",
    "section": "Notes de bas de page",
    "text": "Notes de bas de page\n\n\nMiller, Tim. “Explanation in artificial intelligence: Insights from the social sciences.” arXiv Preprint arXiv:1706.07269. (2017).↩︎\nLipton, Peter. “Contrastive explanation.” Royal Institute of Philosophy Supplements 27 (1990): 247-266.↩︎\nKahneman, Daniel, and Amos Tversky. “The simulation heuristic.” Stanford Univ CA Dept of Psychology. (1981).↩︎\nŠtrumbelj, Erik, and Igor Kononenko. “A general method for visualizing and explaining black-box regression models.” In International Conference on Adaptive and Natural Computing Algorithms, 21–30. Springer. (2011).↩︎",
    "crumbs": [
      "3 - Interprétabilité",
      "3.6 - Explications conviviales pour l'être humain"
    ]
  },
  {
    "objectID": "03-interpretability/03.4-evaluation_of_interpretability.html",
    "href": "03-interpretability/03.4-evaluation_of_interpretability.html",
    "title": "Apprentissage automatique interprétable",
    "section": "",
    "text": "Avertissement\n\n\n\nEn cours de traduction.",
    "crumbs": [
      "3 - Interprétabilité",
      "3.4 - Evaluation de l'interprétabilité"
    ]
  },
  {
    "objectID": "03-interpretability/03.4-evaluation_of_interpretability.html#evaluation-de-linterprétabilité",
    "href": "03-interpretability/03.4-evaluation_of_interpretability.html#evaluation-de-linterprétabilité",
    "title": "Apprentissage automatique interprétable",
    "section": "3.4 - Evaluation de l’interprétabilité",
    "text": "3.4 - Evaluation de l’interprétabilité\nIl n’existe pas de consensus réel sur ce qu’est l’interprétabilité en apprentissage automatique. Il n’est pas non plus clair comment la mesurer. Mais il y a des recherches initiales sur ce sujet et une tentative de formuler certaines approches pour l’évaluation, comme décrit dans la section suivante.\nDoshi-Velez et Kim (2017)1 proposent trois niveaux principaux pour l’évaluation de l’interprétabilité :\nÉvaluation au niveau de l’application (tâche réelle) : Intégrez l’explication dans le produit et faites-la tester par l’utilisateur final. Imaginez un logiciel de détection de fractures avec un composant d’apprentissage automatique qui localise et marque les fractures sur les radiographies. Au niveau de l’application, les radiologues testeraient directement le logiciel de détection de fractures pour évaluer le modèle. Cela nécessite une bonne configuration expérimentale et une compréhension de la manière d’évaluer la qualité. Une bonne base de comparaison est toujours de savoir comment un humain serait bon pour expliquer la même décision.\nÉvaluation au niveau humain (tâche simple) est une évaluation au niveau de l’application simplifiée. La différence est que ces expériences ne sont pas menées avec les experts du domaine, mais avec des personnes profanes. Cela rend les expériences moins coûteuses (surtout si les experts du domaine sont des radiologues) et il est plus facile de trouver davantage de testeurs. Un exemple serait de montrer à un utilisateur différentes explications et l’utilisateur choisirait la meilleure.\nÉvaluation au niveau fonctionnel (tâche proxy) ne nécessite pas d’humains. Cela fonctionne mieux lorsque la classe de modèle utilisée a déjà été évaluée par quelqu’un d’autre dans une évaluation au niveau humain. Par exemple, il se pourrait qu’on sache que les utilisateurs finaux comprennent les arbres de décision. Dans ce cas, un proxy pour la qualité de l’explication pourrait être la profondeur de l’arbre. Des arbres plus courts obtiendraient un meilleur score d’explicabilité. Il serait logique d’ajouter la contrainte que la performance prédictive de l’arbre reste bonne et ne diminue pas trop par rapport à un arbre plus grand.\nLe chapitre suivant se concentre sur l’évaluation des explications pour des prédictions individuelles au niveau fonctionnel. Quelles sont les propriétés pertinentes des explications que nous considérerions pour leur évaluation ?",
    "crumbs": [
      "3 - Interprétabilité",
      "3.4 - Evaluation de l'interprétabilité"
    ]
  },
  {
    "objectID": "03-interpretability/03.4-evaluation_of_interpretability.html#footnotes",
    "href": "03-interpretability/03.4-evaluation_of_interpretability.html#footnotes",
    "title": "Apprentissage automatique interprétable",
    "section": "Notes de bas de page",
    "text": "Notes de bas de page\n\n\nDoshi-Velez, Finale, and Been Kim. “Towards a rigorous science of interpretable machine learning,” no. Ml: 1–13. https://arxiv.org/abs/1702.08608 (2017).↩︎",
    "crumbs": [
      "3 - Interprétabilité",
      "3.4 - Evaluation de l'interprétabilité"
    ]
  },
  {
    "objectID": "03-interpretability/03.1-importance_of_interpretability.html",
    "href": "03-interpretability/03.1-importance_of_interpretability.html",
    "title": "Apprentissage automatique interprétable",
    "section": "",
    "text": "Avertissement\n\n\n\nEn cours de traduction.",
    "crumbs": [
      "3 - Interprétabilité",
      "3.1 - Importance de l'Interprétabilité"
    ]
  },
  {
    "objectID": "03-interpretability/03.1-importance_of_interpretability.html#sec-interpretability_importance",
    "href": "03-interpretability/03.1-importance_of_interpretability.html#sec-interpretability_importance",
    "title": "Apprentissage automatique interprétable",
    "section": "3.1 - Importance de l’Interprétabilité",
    "text": "3.1 - Importance de l’Interprétabilité\nSi un modèle d’apprentissage automatique fonctionne bien, pourquoi ne pas simplement faire confiance au modèle et ignorer pourquoi il a pris une certaine décision ? “Le problème est qu’une seule métrique, comme la précision de classification, est une description incomplète de la plupart des tâches dans le monde réel.” (Doshi-Velez et Kim 20171)\nPlongeons plus profondément dans les raisons pour lesquelles l’interprétabilité est si importante. Lorsqu’il s’agit de modélisation prédictive, vous devez faire un compromis : Voulez-vous simplement savoir ce qui est prédit ? Par exemple, la probabilité qu’un client résilie son contrat ou l’efficacité d’un médicament pour un patient. Ou voulez-vous savoir pourquoi la prédiction a été faite et éventuellement payer pour l’interprétabilité par une baisse de la performance prédictive ? Dans certains cas, vous ne vous souciez pas de savoir pourquoi une décision a été prise, il suffit de savoir que la performance prédictive sur un ensemble de test était bonne. Mais dans d’autres cas, connaître le ‘pourquoi’ peut vous aider à en apprendre davantage sur le problème, les données et la raison pour laquelle un modèle pourrait échouer. Certains modèles peuvent ne pas nécessiter d’explications car ils sont utilisés dans un environnement à faible risque, signifiant qu’une erreur n’aura pas de conséquences graves, (par exemple, un système de recommandation de films) ou la méthode a déjà été largement étudiée et évaluée (par exemple, la reconnaissance optique de caractères). Le besoin d’interprétabilité découle d’une incomplétude dans la formalisation du problème (Doshi-Velez et Kim 2017), ce qui signifie que pour certains problèmes ou tâches, il ne suffit pas d’obtenir la prédiction (le quoi). Le modèle doit également expliquer comment il est arrivé à la prédiction (le pourquoi), car une prédiction correcte ne résout que partiellement votre problème initial. Les raisons suivantes motivent la demande d’interprétabilité et d’explications (Doshi-Velez et Kim 2017 et Miller 20172).\nCuriosité humaine et apprentissage : Les humains ont un modèle mental de leur environnement qui est mis à jour lorsqu’un événement inattendu se produit. Cette mise à jour est effectuée en trouvant une explication à l’événement inattendu. Par exemple, un humain se sent inopinément malade et demande : “Pourquoi me sens-je si mal ?”. Il apprend qu’il tombe malade chaque fois qu’il mange ces baies rouges. Il met à jour son modèle mental et décide que les baies sont la cause de la maladie et doivent donc être évitées. Lorsque des modèles d’apprentissage automatique opaques sont utilisés dans la recherche, les découvertes scientifiques restent complètement cachées si le modèle ne fournit que des prédictions sans explications. Pour faciliter l’apprentissage et satisfaire la curiosité quant aux raisons pour lesquelles certaines prédictions ou comportements sont créés par les machines, l’interprétabilité et les explications sont cruciales. Bien sûr, les humains n’ont pas besoin d’explications pour tout ce qui se passe. Pour la plupart des gens, il est acceptable de ne pas comprendre comment fonctionne un ordinateur. Les événements inattendus nous rendent curieux. Par exemple : Pourquoi mon ordinateur s’éteint-il de manière inattendue ?\nÉtroitement liée à l’apprentissage est le désir humain de trouver un sens dans le monde. Nous voulons harmoniser les contradictions ou les incohérences entre les éléments de nos structures de connaissances. “Pourquoi mon chien m’a-t-il mordu alors qu’il ne l’avait jamais fait auparavant ?” pourrait se demander un humain. Il y a une contradiction entre la connaissance du comportement passé du chien et la nouvelle expérience désagréable de la morsure. L’explication du vétérinaire réconcilie la contradiction du propriétaire du chien : “Le chien était sous stress et a mordu.” Plus une décision de machine affecte la vie d’une personne, plus il est important que la machine explique son comportement. Si un modèle d’apprentissage automatique rejette une demande de prêt, cela peut être complètement inattendu pour les demandeurs. Ils ne peuvent réconcilier cette incohérence entre l’attente et la réalité qu’avec une sorte d’explication. Les explications n’ont pas réellement besoin de tout expliquer, mais devraient aborder une cause principale. Un autre exemple est la recommandation de produits algorithmique. Personnellement, je pense toujours à pourquoi certains produits ou films m’ont été recommandés algorithmiquement. Souvent, c’est assez clair : la publicité me suit sur Internet parce que j’ai récemment acheté une machine à laver, et je sais que dans les prochains jours, je serai suivi par des publicités pour des machines à laver. Oui, il est logique de suggérer des gants si j’ai déjà un bonnet d’hiver dans mon panier. L’algorithme recommande ce film, car les utilisateurs qui ont aimé d’autres films que j’ai appréciés ont également aimé le film recommandé. De plus en plus, les entreprises Internet ajoutent des explications à leurs recommandations. Un bon exemple en sont les recommandations de produits, qui sont basées sur des combinaisons de produits fréquemment achetées :\n\n\n\namazon-recommendation\n\n\nDans de nombreuses disciplines scientifiques, on observe un passage de méthodes qualitatives à quantitatives (par exemple en sociologie, psychologie), ainsi qu’une orientation vers l’apprentissage automatique (en biologie, génomique). L’objectif de la science est d’acquérir des connaissances, mais de nombreux problèmes sont résolus avec de grands ensembles de données et des modèles d’apprentissage automatique boîte noire. Le modèle lui-même devient la source de connaissance au lieu des données. L’interprétabilité rend possible l’extraction de ces connaissances supplémentaires capturées par le modèle.\nLes modèles d’apprentissage automatique prennent en charge des tâches réelles qui nécessitent des mesures de sécurité et des tests. Imaginez une voiture autonome détectant automatiquement les cyclistes grâce à un système d’apprentissage profond. Vous voulez être sûr à 100% que l’abstraction apprise par le système est exempte d’erreurs, car renverser des cyclistes est assez grave. Une explication pourrait révéler que la caractéristique la plus importante apprise est de reconnaître les deux roues d’un vélo, et cette explication vous aide à réfléchir à des cas limites comme des vélos avec des sacoches latérales qui couvrent partiellement les roues.\nPar défaut, les modèles d’apprentissage automatique absorbent les biais des données d’entraînement. Cela peut transformer vos modèles d’apprentissage automatique en racistes qui discriminent contre les groupes sous-représentés. L’interprétabilité est un outil de débogage utile pour détecter les biais dans les modèles d’apprentissage automatique. Il se peut que le modèle d’apprentissage automatique que vous avez entraîné pour l’approbation ou le rejet automatique des demandes de crédit discrimine contre une minorité qui a été historiquement privée de droits. Votre objectif principal est d’accorder des prêts uniquement aux personnes qui les rembourseront finalement. L’incomplétude de la formulation du problème dans ce cas réside dans le fait que vous ne voulez pas seulement minimiser les défauts de paiement, mais êtes également obligé de ne pas discriminer sur la base de certaines démographies. C’est une contrainte supplémentaire qui fait partie de votre formulation de problème (octroi de prêts de manière à faible risque et conforme) qui n’est pas couverte par la fonction de perte pour laquelle le modèle d’apprentissage automatique a été optimisé.\nLe processus d’intégration des machines et des algorithmes dans notre vie quotidienne nécessite de l’interprétabilité pour augmenter l’acceptation sociale. Les gens attribuent des croyances, des désirs, des intentions, etc., à des objets. Dans une expérience célèbre, Heider et Simmel (1944)3 ont montré aux participants des vidéos de formes où un cercle ouvrait une “porte” pour entrer dans une “pièce” (qui était simplement un rectangle). Les participants décrivaient les actions des formes comme ils décriraient les actions d’un agent humain, attribuant des intentions et même des émotions et des traits de personnalité aux formes. Les robots sont un bon exemple, comme mon aspirateur, que j’ai nommé “Doge”. Si Doge se coince, je pense : “Doge veut continuer à nettoyer, mais me demande de l’aide car il est coincé.” Plus tard, lorsque Doge termine le nettoyage et cherche la base pour se recharger, je pense : “Doge a le désir de se recharger et a l’intention de trouver la base.” J’attribue aussi des traits de personnalité : “Doge est un peu bête, mais de manière mignonne.” Ce sont mes pensées, surtout lorsque je découvre que Doge a renversé une plante en aspirant consciencieusement la maison. Une machine ou un algorithme qui explique ses prédictions trouvera plus d’acceptation. Voir aussi le chapitre sur les explications, qui soutient que les explications sont un processus social.\nLes explications sont utilisées pour gérer les interactions sociales. En créant une signification partagée de quelque chose, l’explicateur influence les actions, les émotions et les croyances du destinataire de l’explication. Pour qu’une machine interagisse avec nous, elle peut avoir besoin de façonner nos émotions et croyances. Les machines doivent nous “persuader” afin qu’elles puissent atteindre leur objectif prévu. Je n’accepterais pas complètement mon aspirateur robot s’il n’expliquait pas son comportement à un certain degré. L’aspirateur crée une signification partagée, par exemple, d’un “accident” (comme se coincer sur le tapis de la salle de bain… encore) en expliquant qu’il s’est coincé au lieu de simplement s’arrêter de fonctionner sans commentaire. Intéressant, il peut y avoir un désalignement entre l’objectif de la machine expliquatrice (créer de la confiance) et l’objectif du destinataire (comprendre la prédiction ou le comportement). Peut-être que l’explication complète de pourquoi Doge s’est coincé pourrait être que la batterie était très faible, que l’une des roues ne fonctionne pas correctement et qu’il y a un bug qui fait que le robot retourne au même endroit encore et encore même s’il y avait un obstacle. Ces raisons (et quelques autres) ont causé le coincement du robot, mais il n’a expliqué que quelque chose bloquait le chemin, et cela suffisait pour que je fasse confiance à son comportement et que je partage la signification de cet accident. D’ailleurs, Doge s’est coincé dans la salle de bain à nouveau. Nous devons enlever les tapis chaque fois avant de laisser Doge aspirer.\n\n\n\ndoge-stuck\n\n\nLes modèles d’apprentissage automatique ne peuvent être débogués et audités que lorsqu’ils peuvent être interprétés. Même dans des environnements à faible risque, tels que les recommandations de films, la capacité à interpréter est précieuse tant dans la phase de recherche et développement qu’après le déploiement. Plus tard, lorsqu’un modèle est utilisé dans un produit, des problèmes peuvent survenir. Une interprétation d’une prédiction erronée aide à comprendre la cause de l’erreur. Elle fournit une direction sur comment réparer le système. Considérez l’exemple d’un classificateur husky contre loup qui classe mal certains huskies comme loups. En utilisant des méthodes d’apprentissage automatique interprétables, vous trouveriez que la mauvaise classification était due à la neige sur l’image. Le classificateur a appris à utiliser la neige comme caractéristique pour classifier les images comme “loup”, ce qui pourrait avoir du sens en termes de séparation des loups des huskies dans l’ensemble de données d’entraînement, mais pas dans une utilisation réelle.\nSi vous pouvez garantir que le modèle d’apprentissage automatique peut expliquer ses décisions, vous pouvez également vérifier plus facilement les caractéristiques suivantes (Doshi-Velez et Kim 2017) :\n\nÉquité : Assurer que les prédictions sont impartiales et ne discriminent pas implicitement ou explicitement contre les groupes sous-représentés. Un modèle interprétable peut vous dire pourquoi il a décidé qu’une certaine personne ne devrait pas obtenir un prêt, et il devient plus facile pour un humain de juger si la décision est basée sur un biais démographique appris (par exemple, racial).\nConfidentialité : Assurer que les informations sensibles dans les données sont protégées.\nFiabilité ou Robustesse : Assurer que de petits changements dans l’entrée ne conduisent pas à de grands changements dans la prédiction.\nCausalité : Vérifier que seules les relations causales sont prises en compte.\nConfiance : Il est plus facile pour les humains de faire confiance à un système qui explique ses décisions par rapport à une boîte noire.\n\nQuand nous n’avons pas besoin d’interprétabilité.\nLes scénarios suivants illustrent quand nous n’avons pas besoin ou même ne voulons pas d’interprétabilité des modèles d’apprentissage automatique.\nL’interprétabilité n’est pas nécessaire si le modèle n’a pas d’impact significatif. Imaginez quelqu’un nommé Mike travaillant sur un projet d’apprentissage automatique pour prédire où ses amis iront pour leurs prochaines vacances en se basant sur des données Facebook. Mike aime juste surprendre ses amis avec des suppositions éclairées sur leur destination de vacances. Il n’y a pas de vrai problème si le modèle se trompe (au pire juste un peu embarrassant pour Mike), ni s’il ne peut pas expliquer la sortie de son modèle. Il est parfaitement acceptable de ne pas avoir d’interprétabilité dans ce cas. La situation changerait si Mike commençait à construire une entreprise autour de ces prédictions de destinations de vacances. Si le modèle est erroné, l’entreprise pourrait perdre de l’argent, ou le modèle pourrait fonctionner moins bien pour certaines personnes à cause d’un biais racial appris. Dès que le modèle a un impact significatif, qu’il soit financier ou social, l’interprétabilité devient pertinente.\nL’interprétabilité n’est pas nécessaire lorsque le problème est bien étudié. Certaines applications ont été suffisamment bien étudiées de sorte qu’il y a assez d’expérience pratique avec le modèle et les problèmes liés au modèle ont été résolus au fil du temps. Un bon exemple est un modèle d’apprentissage automatique pour la reconnaissance optique de caractères qui traite des images d’enveloppes et extrait les adresses. Il y a des années d’expérience avec ces systèmes et il est clair qu’ils fonctionnent. De plus, nous ne sommes pas vraiment intéressés par l’acquisition de connaissances supplémentaires sur la tâche en question.\nL’interprétabilité pourrait permettre à des personnes ou à des programmes de manipuler le système. Les problèmes avec les utilisateurs qui trompent un système résultent d’un décalage entre les objectifs du créateur et de l’utilisateur d’un modèle. Le scoring de crédit est un tel système parce que les banques veulent s’assurer que les prêts ne sont accordés qu’aux demandeurs susceptibles de les rembourser, et les demandeurs visent à obtenir le prêt même si la banque ne souhaite pas leur accorder. Ce décalage entre les objectifs introduit des incitations pour les demandeurs à manipuler le système pour augmenter leurs chances d’obtenir un prêt. Si un demandeur sait que le fait d’avoir plus de deux cartes de crédit affecte négativement son score, il rend simplement sa troisième carte de crédit pour améliorer son score, et organise une nouvelle carte après l’approbation du prêt. Bien que son score se soit amélioré, la probabilité réelle de rembourser le prêt est restée inchangée. Le système ne peut être manipulé que si les entrées sont des substituts d’une caractéristique causale, mais ne causent pas réellement le résultat. Dans la mesure du possible, les caractéristiques substituts devraient être évitées car elles rendent les modèles manipulables. Par exemple, Google a développé un système appelé Google Flu Trends pour prédire les épidémies de grippe. Le système corrélait les recherches Google avec les épidémies de grippe - et il a mal performé. La distribution des requêtes de recherche a changé et Google Flu Trends a manqué de nombreuses épidémies de grippe. Les recherches Google ne causent pas la grippe. Lorsque les gens recherchent des symptômes comme “fièvre”, c’est simplement une corrélation avec les épidémies de grippe réelles. Idéalement, les modèles n’utiliseraient que des caractéristiques causales car elles ne seraient pas manipulables.",
    "crumbs": [
      "3 - Interprétabilité",
      "3.1 - Importance de l'Interprétabilité"
    ]
  },
  {
    "objectID": "03-interpretability/03.1-importance_of_interpretability.html#footnotes",
    "href": "03-interpretability/03.1-importance_of_interpretability.html#footnotes",
    "title": "Apprentissage automatique interprétable",
    "section": "Notes de bas de page",
    "text": "Notes de bas de page\n\n\nDoshi-Velez, Finale, and Been Kim. “Towards a rigorous science of interpretable machine learning,” no. Ml: 1–13. https://arxiv.org/abs/1702.08608 (2017).↩︎\nMiller, Tim. “Explanation in artificial intelligence: Insights from the social sciences.” arXiv Preprint arXiv:1706.07269. (2017).↩︎\nHeider, Fritz, and Marianne Simmel. “An experimental study of apparent behavior.” The American Journal of Psychology 57 (2). JSTOR: 243–59. (1944).↩︎",
    "crumbs": [
      "3 - Interprétabilité",
      "3.1 - Importance de l'Interprétabilité"
    ]
  },
  {
    "objectID": "02-introduction/02.3-terminology.html",
    "href": "02-introduction/02.3-terminology.html",
    "title": "Apprentissage automatique interprétable",
    "section": "",
    "text": "Version original (en)\n\n\n\nhttps://christophm.github.io/interpretable-ml-book/terminology.html",
    "crumbs": [
      "2 - Introduction",
      "2.3 - Terminologie"
    ]
  },
  {
    "objectID": "02-introduction/02.3-terminology.html#terminologie",
    "href": "02-introduction/02.3-terminology.html#terminologie",
    "title": "Apprentissage automatique interprétable",
    "section": "2.3 - Terminologie",
    "text": "2.3 - Terminologie\nPour éviter la confusion due à l’ambiguïté, voici quelques définitions des termes utilisés dans ce livre :\nUn algorithme est un ensemble de règles qu’une machine suit pour atteindre un objectif particulier1. Un algorithme peut être considéré comme une recette qui définit les entrées, la sortie et toutes les étapes nécessaires pour passer des entrées à la sortie. Les recettes de cuisine sont des algorithmes où les ingrédients sont les entrées, la nourriture cuisinée est la sortie, et les étapes de préparation et de cuisson sont les instructions de l’algorithme.\nL’apprentissage automatique (Machine learning) est un ensemble de méthodes qui permettent aux ordinateurs d’apprendre à partir de données pour effectuer et améliorer des prédictions (par exemple, le cancer, les ventes hebdomadaires, le défaut de crédit). L’apprentissage automatique représente un changement de paradigme par rapport à la “programmation normale” pour laquelle toutes les instructions doivent être explicitement fournies à l’ordinateur, vers une “programmation indirecte” qui se fait en fournissant des données.\n\n\n\nProgrammation contre Apprentissage automatique\n\n\nUn apprenant ou algorithme d’apprentissage automatique est un programme utilisé pour apprendre un modèle d’apprentissage automatique à partir de données. Un autre terme utilisé est “inducteur” (par exemple, “inducteur d’arbres”).\nUn modèle d’apprentissage automatique est le programme appris qui associe des entrées à des prédictions. Cela correspond à un ensemble de poids pour un modèle linéaire ou pour un réseau neuronal. D’autres termes pour le mot assez imprécis “modèle” sont “prédicteur” ou, selon la tâche, “classificateur” ou “modèle de régression”. Dans les formules mathématiques, le modèle d’apprentissage automatique formé est représenté par \\(\\hat{f}\\) ou \\(\\hat{f}(x)\\).\n\n\n\nSchéma de l’apprenant\n\n\nUn modèle “boîte noire” est un système qui ne révèle pas ses mécanismes internes. Dans l’apprentissage automatique, “boîte noire” décrit les modèles qui ne peuvent pas être compris en regardant leurs paramètres (par exemple, un réseau neuronal). L’opposé d’une boîte noire est parfois appelé “boîte blanche”, et ce terme désigne dans ce livre un modèle interprétable. Les méthodes agnostiques au modèle pour l’interprétabilité traitent les modèles d’apprentissage automatique comme des “boîtes noires”, même s’ils ne le sont pas.\n\n\n\nBoîte noire\n\n\nL’apprentissage automatique interprétable fait référence aux méthodes et modèles qui rendent le comportement et les prédictions des systèmes d’apprentissage automatique compréhensibles pour les humains.\nUn ensemble de données est un tableau contenant les données à partir desquelles la machine apprend. L’ensemble de données contient les caractéristiques et la cible à prédire. Lorsqu’il est utilisé pour induire un modèle, l’ensemble de données est appelé jeu de données d’entraînement.\n\nUne instance est une ligne dans l’ensemble de données. D’autres noms pour “instance” sont : point (de données), exemple, observation. Une instance se compose des valeurs des caractéristiques \\(x^{(i)}\\) et, s’il est connu, du résultat cible \\(y_i\\).\nLes caractéristiques sont les entrées utilisées pour la prédiction ou la classification. Une caractéristique est une colonne dans l’ensemble de données. Tout au long du livre, il est supposé que les caractéristiques sont interprétables, ce qui signifie qu’il est facile de comprendre ce qu’elles signifient, comme la température d’un jour donné ou la taille d’une personne. L’interprétabilité des caractéristiques est une supposition importante. Mais si les caractéristiques d’entrée peuvent être difficiles à comprendre, il est encore plus difficile de comprendre ce que fait le modèle. La matrice contenant toutes les caractéristiques est appelée \\(X\\), et \\(x^{(i)}\\) le vecteur représentant l’instance \\(i\\). Le vecteur d’une seule caractéristique sur toutes les instances est \\(x_j\\), et la valeur de la caractéristique \\(j\\) de l’instance \\(i\\) est \\(x^{(i)}_j\\).\nLa cible est l’information que la machine apprend à prédire. Dans les formules mathématiques, la cible est généralement représentée par \\(y\\), ou \\(y_i\\) pour l’instance \\(i\\).\nUne tâche d’apprentissage automatique est la combinaison d’un ensemble de données comprenant des caractéristiques et une cible. Selon le type de la cible, la tâche peut être, par exemple, la classification, la régression, l’analyse de survie, le regroupement ou la détection d’anomalies.\nLa prédiction est ce que le modèle d’apprentissage automatique “estime” comme devant être la valeur cible en fonction des caractéristiques données. Dans ce livre, la prédiction du modèle est notée \\(\\hat{f}(x^{(i)})\\), \\(\\hat{y}\\) ou \\(\\hat{y}_i\\).",
    "crumbs": [
      "2 - Introduction",
      "2.3 - Terminologie"
    ]
  },
  {
    "objectID": "02-introduction/02.3-terminology.html#footnotes",
    "href": "02-introduction/02.3-terminology.html#footnotes",
    "title": "Apprentissage automatique interprétable",
    "section": "Notes de bas de page",
    "text": "Notes de bas de page\n\n\n“Definition of Algorithm.” https://www.merriam-webster.com/dictionary/algorithm (2017).↩︎",
    "crumbs": [
      "2 - Introduction",
      "2.3 - Terminologie"
    ]
  },
  {
    "objectID": "References/index.html",
    "href": "References/index.html",
    "title": "Références",
    "section": "",
    "text": "Version original (en)\n\n\n\nhttps://christophm.github.io/interpretable-ml-book/references.html\n\n\n“Definition of Algorithm.” https://www.merriam-webster.com/dictionary/algorithm (2017).\nAamodt, Agnar, and Enric Plaza. “Case-based reasoning: Foundational issues, methodological variations, and system approaches.” AI communications 7.1 (1994): 39-59.\nAdebayo, Julius, Justin Gilmer, Michael Muelly, Ian Goodfellow, Moritz Hardt, and Been Kim. “Sanity checks for saliency maps.” arXiv preprint arXiv:1810.03292 (2018).\nAlain, Guillaume, and Yoshua Bengio. “Understanding intermediate layers using linear classifier probes.” arXiv preprint arXiv:1610.01644 (2016).\nAlberto, Túlio C, Johannes V Lochter, and Tiago A Almeida. “Tubespam: comment spam filtering on YouTube.” In Machine Learning and Applications (Icmla), Ieee 14th International Conference on, 138–43. IEEE. (2015).\nAlvarez-Melis, David, and Tommi S. Jaakkola. “On the robustness of interpretability methods.” arXiv preprint arXiv:1806.08049 (2018).\nApley, Daniel W., and Jingyu Zhu. “Visualizing the effects of predictor variables in black box supervised learning models.” Journal of the Royal Statistical Society: Series B (Statistical Methodology) 82.4 (2020): 1059-1086.\nAthalye, Anish, and Ilya Sutskever. “Synthesizing robust adversarial examples.” arXiv preprint arXiv:1707.07397 (2017).\nBau, David, Bolei Zhou, Aditya Khosla, Aude Oliva, and Antonio Torralba. “Network dissection: Quantifying interpretability of deep visual representations.” In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 6541-6549 (2017).\nBiggio, Battista, and Fabio Roli. “Wild Patterns: Ten years after the rise of adversarial machine learning.” Pattern Recognition 84 (2018): 317-331.\nBorgelt, C. “An implementation of the FP-growth algorithm.” Proceedings of the 1st International Workshop on Open Source Data Mining Frequent Pattern Mining Implementations - OSDM ’05, 1–5. http://doi.org/10.1145/1133905.1133907 (2005).\nBreiman, Leo.“Random Forests.” Machine Learning 45 (1). Springer: 5-32 (2001).\nBrown, Tom B., Dandelion Mané, Aurko Roy, Martín Abadi, and Justin Gilmer. “Adversarial patch.” arXiv preprint arXiv:1712.09665 (2017).\nCaruana, Rich, et al. “Intelligible models for healthcare: Predicting pneumonia risk and hospital 30-day readmission.” Proceedings of the 21th ACM SIGKDD international conference on knowledge discovery and data mining. (2015).\nChen, Zhi, Yijie Bei, and Cynthia Rudin. “Concept whitening for interpretable image recognition.” Nature Machine Intelligence 2, no. 12 (2020): 772-782.\nCohen, William W. “Fast effective rule induction.” Machine Learning Proceedings (1995). 115-123.\nCook, R. Dennis. “Detection of influential observation in linear regression.” Technometrics 19.1 (1977): 15-18.\nDandl, Susanne, Christoph Molnar, Martin Binder, Bernd Bischl. “Multi-objective counterfactual explanations”. In: Bäck T. et al. (eds) Parallel Problem Solving from Nature – PPSN XVI. PPSN 2020. Lecture Notes in Computer Science, vol 12269. Springer, Cham (2020).\nDeb, Kalyanmoy, Amrit Pratap, Sameer Agarwal and T. Meyarivan, “A fast and elitist multiobjective genetic algorithm: NSGA-II,” in IEEE Transactions on Evolutionary Computation, vol. 6, no. 2, pp. 182-197, (2002).\nDoshi-Velez, Finale, and Been Kim. “Towards a rigorous science of interpretable machine learning,” no. Ml: 1–13. https://arxiv.org/abs/1702.08608 (2017).\nEmilie Kaufmann and Shivaram Kalyanakrishnan. “Information complexity in bandit subset selection”. Proceedings of Machine Learning Research (2013).\nFürnkranz, Johannes, Dragan Gamberger, and Nada Lavrač. “Foundations of rule learning.” Springer Science & Business Media, (2012).\nFanaee-T, Hadi, and Joao Gama. “Event labeling combining ensemble detectors and background knowledge.” Progress in Artificial Intelligence. Springer Berlin Heidelberg, 1–15. https://doi.org/10.1007/s13748-013-0040-3. (2013).\nFernandes, Kelwin, Jaime S Cardoso, and Jessica Fernandes. “Transfer learning with partial observability applied to cervical cancer screening.” In Iberian Conference on Pattern Recognition and Image Analysis, 243–50. Springer. (2017).\nFisher, Aaron, Cynthia Rudin, and Francesca Dominici. “All models are wrong, but many are useful: Learning a variable’s importance by studying an entire class of prediction models simultaneously.” https://arxiv.org/abs/1801.01489 (2018).\nFokkema, Marjolein, and Benjamin Christoffersen. “Pre: Prediction rule ensembles”. https://CRAN.R-project.org/package=pre (2017).\nFriedman, Jerome H, and Bogdan E Popescu. “Predictive learning via rule ensembles.” The Annals of Applied Statistics. JSTOR, 916–54. (2008).\nFriedman, Jerome H. “Greedy function approximation: A gradient boosting machine.” Annals of statistics (2001): 1189-1232.\nFriedman, Jerome, Trevor Hastie, and Robert Tibshirani. “The elements of statistical learning”. hastie.su.domains/ElemStatLearn (2009).\nGhorbani, Amirata, Abubakar Abid, and James Zou. “Interpretation of neural networks is fragile.” Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 33. 2019.\nGhorbani, Amirata, James Wexler, James Zou and Been Kim. “Towards automatic concept-based explanations.” Advances in Neural Information Processing Systems 32 (2019).\nGoldstein, Alex, Adam Kapelner, Justin Bleich, and Emil Pitkin. “Peeking inside the black box: Visualizing statistical learning with plots of individual conditional expectation.” journal of Computational and Graphical Statistics 24, no. 1 (2015): 44-65.\nGoldstein, Alex, Adam Kapelner, Justin Bleich, and Maintainer Adam Kapelner. “Package ‘ICEbox’.” (2017).\nGoodfellow, Ian J., Jonathon Shlens, and Christian Szegedy. “Explaining and harnessing adversarial examples.” arXiv preprint arXiv:1412.6572 (2014).\nGrömping, Ulrike. “Model-Agnostic Effects Plots for Interpreting Machine Learning Models.” Reports in Mathematics, Physics and Chemistry: Department II, Beuth University of Applied Sciences Berlin. Report 1/2020 (2020)\nGreenwell, Brandon M., Bradley C. Boehmke, and Andrew J. McCarthy. “A simple and effective model-based variable importance measure.” arXiv preprint arXiv:1805.04755 (2018).\nHeider, Fritz, and Marianne Simmel. “An experimental study of apparent behavior.” The American Journal of Psychology 57 (2). JSTOR: 243–59. (1944).\nHolte, Robert C. “Very simple classification rules perform well on most commonly used datasets.” Machine learning 11.1 (1993): 63-90.\nHooker, Giles. “Discovering additive structure in black box functions.” Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining. (2004).\nHooker, Giles. “Generalized functional anova diagnostics for high-dimensional functions of dependent variables.” Journal of Computational and Graphical Statistics 16.3 (2007): 709-732.\nInglis, Alan, Andrew Parnell, and Catherine Hurley. “Visualizing Variable Importance and Variable Interaction Effects in Machine Learning Models.” arXiv preprint arXiv:2108.04310 (2021).\nJanzing, Dominik, Lenon Minorics, and Patrick Blöbaum. “Feature relevance quantification in explainable AI: A causal problem.” International Conference on Artificial Intelligence and Statistics. PMLR (2020).\nKahneman, Daniel, and Amos Tversky. “The simulation heuristic.” Stanford Univ CA Dept of Psychology. (1981).\nKarimi, Amir-Hossein, Gilles Barthe, Borja Balle and Isabel Valera. “Model-agnostic counterfactual explanations for consequential decisions.” AISTATS (2020).\nKarpathy, Andrej, Justin Johnson, and Li Fei-Fei. “Visualizing and understanding recurrent networks.” arXiv preprint arXiv:1506.02078 (2015).\nKaufman, Leonard, and Peter Rousseeuw. “Clustering by means of medoids”. North-Holland (1987).\nKim, Been, Martin Wattenberg, Justin Gilmer, Carrie Cai, James Wexler, and Fernanda Viegas. “Interpretability beyond feature attribution: Quantitative testing with concept activation vectors (tcav).” In International conference on machine learning, pp. 2668-2677. PMLR (2018).\nKim, Been, Rajiv Khanna, and Oluwasanmi O. Koyejo. “Examples are not enough, learn to criticize! Criticism for interpretability.” Advances in Neural Information Processing Systems (2016).\nKim, Been, Rajiv Khanna, and Oluwasanmi O. Koyejo. “Examples are not enough, learn to criticize! Criticism for interpretability.” Advances in Neural Information Processing Systems (2016).\nKindermans, Pieter-Jan, Sara Hooker, Julius Adebayo, Maximilian Alber, Kristof T. Schütt, Sven Dähne, Dumitru Erhan, and Been Kim. “The (un) reliability of saliency methods.” In Explainable AI: Interpreting, Explaining and Visualizing Deep Learning, pp. 267-280. Springer, Cham (2019).\nKoh, Pang Wei, and Percy Liang. “Understanding black-box predictions via influence functions.” arXiv preprint arXiv:1703.04730 (2017).\nKoh, Pang Wei, Kai-Siang Ang, Hubert HK Teo, and Percy Liang. “On the accuracy of influence functions for measuring group effects.” arXiv preprint arXiv:1905.13289 (2019).\nKoh, Pang Wei, Thao Nguyen, Yew Siang Tang, Stephen Mussmann, Emma Pierson, Been Kim, and Percy Liang. “Concept bottleneck models.” In International Conference on Machine Learning, pp. 5338-5348. PMLR (2020).\nLaugel, Thibault, Marie-Jeanne Lesot, Christophe Marsala, Xavier Renard, and Marcin Detyniecki. “Inverse classification for comparison-based interpretability in machine learning.” arXiv preprint arXiv:1712.08443 (2017).\nLetham, Benjamin, Cynthia Rudin, Tyler H. McCormick, and David Madigan. “Interpretable classifiers using rules and Bayesian analysis: Building a better stroke prediction model.” The Annals of Applied Statistics 9, no. 3 (2015): 1350-1371.\nLipton, Peter. “Contrastive explanation.” Royal Institute of Philosophy Supplements 27 (1990): 247-266.\nLipton, Zachary C. “The mythos of model interpretability.” arXiv preprint arXiv:1606.03490, (2016).\nLundberg, Scott M., and Su-In Lee. “A unified approach to interpreting model predictions.” Advances in Neural Information Processing Systems (2017).\nLundberg, Scott M., Gabriel G. Erion, and Su-In Lee. “Consistent individualized feature attribution for tree ensembles.” arXiv preprint arXiv:1802.03888 (2018).\nMarco Tulio Ribeiro, Sameer Singh and Carlos Guestrin. “Anchors: high-precision model-agnostic explanations”. AAAI Conference on Artificial Intelligence (AAAI), 2018\nMiller, Tim. “Explanation in artificial intelligence: Insights from the social sciences.” arXiv Preprint arXiv:1706.07269. (2017).\nMothilal, Ramaravind K., Amit Sharma, and Chenhao Tan. “Explaining machine learning classifiers through diverse counterfactual explanations.” Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency. (2020).\nMurdoch, W. J., Singh, C., Kumbier, K., Abbasi-Asl, R., & Yu, B. “Definitions, methods, and applications in interpretable machine learning.” Proceedings of the National Academy of Sciences, 116(44), 22071-22080. (2019).\nNguyen, Anh, Alexey Dosovitskiy, Jason Yosinski, Thomas Brox, and Jeff Clune. “Synthesizing the preferred inputs for neurons in neural networks via deep generator networks.” Advances in neural information processing systems 29 (2016): 3387-3395.\nNguyen, Anh, Jeff Clune, Yoshua Bengio, Alexey Dosovitskiy, and Jason Yosinski. “Plug & play generative networks: Conditional iterative generation of images in latent space.” In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 4467-4477. 2017.\nOlah, Chris, Alexander Mordvintsev, and Ludwig Schubert. “Feature visualization.” Distill 2, no. 11 (2017): e7.\nOlah, Chris, Arvind Satyanarayan, Ian Johnson, Shan Carter, Ludwig Schubert, Katherine Ye, and Alexander Mordvintsev. “The building blocks of interpretability.” Distill 3, no. 3 (2018): e10.\nOlga Russakovsky and Jia Deng (equal contribution), Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg and Li Fei-Fei. “ImageNet large scale visual recognition challenge”. IJCV (2015).\nPapernot, Nicolas, et al. “Practical black-box attacks against machine learning.” Proceedings of the 2017 ACM on Asia Conference on Computer and Communications Security. ACM (2017).\nRibeiro, Marco Tulio, Sameer Singh, and Carlos Guestrin. “Anchors: High-precision model-agnostic explanations.” AAAI Conference on Artificial Intelligence (2018).\nRibeiro, Marco Tulio, Sameer Singh, and Carlos Guestrin. “Model-agnostic interpretability of machine learning.” ICML Workshop on Human Interpretability in Machine Learning. (2016).\nRibeiro, Marco Tulio, Sameer Singh, and Carlos Guestrin. “Why should I trust you?: Explaining the predictions of any classifier.” Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining. ACM (2016).\nRobnik-Sikonja, Marko, and Marko Bohanec. “Perturbation-based explanations of prediction models.” Human and Machine Learning. Springer, Cham. 159-175. (2018).\nShapley, Lloyd S. “A value for n-person games.” Contributions to the Theory of Games 2.28 (1953): 307-317.\nShrikumar, Avanti, Peyton Greenside, and Anshul Kundaje. “Learning important features through propagating activation differences.” Proceedings of the 34th International Conference on Machine Learning-Volume 70. JMLR. org, (2017).\nSimonyan, Karen, and Andrew Zisserman. “Very deep convolutional networks for large-scale image recognition.” arXiv preprint arXiv:1409.1556 (2014).\nSimonyan, Karen, Andrea Vedaldi, and Andrew Zisserman. “Deep inside convolutional networks: Visualising image classification models and saliency maps.” arXiv preprint arXiv:1312.6034 (2013).\nSlack, Dylan, Sophie Hilgard, Emily Jia, Sameer Singh, and Himabindu Lakkaraju. “Fooling lime and shap: Adversarial attacks on post hoc explanation methods.” In Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society, pp. 180-186 (2020).\nSmilkov, Daniel, et al. “SmoothGrad: removing noise by adding noise.” arXiv preprint arXiv:1706.03825 (2017).\nStaniak, Mateusz, and Przemyslaw Biecek. “Explanations of model predictions with live and breakDown packages.” arXiv preprint arXiv:1804.01955 (2018).\nŠtrumbelj, Erik, and Igor Kononenko. “A general method for visualizing and explaining black-box regression models.” In International Conference on Adaptive and Natural Computing Algorithms, 21–30. Springer. (2011).\nŠtrumbelj, Erik, and Igor Kononenko. “Explaining prediction models and individual predictions with feature contributions.” Knowledge and information systems 41.3 (2014): 647-665.\nSu, Jiawei, Danilo Vasconcellos Vargas, and Kouichi Sakurai. “One pixel attack for fooling deep neural networks.” IEEE Transactions on Evolutionary Computation (2019).\nSundararajan, Mukund, and Amir Najmi. “The many Shapley values for model explanation.” arXiv preprint arXiv:1908.08474 (2019).\nSzegedy, Christian, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. “Rethinking the inception architecture for computer vision.” In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 2818-2826 (2016).\nSzegedy, Christian, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow, and Rob Fergus. “Intriguing properties of neural networks.” arXiv preprint arXiv:1312.6199 (2013).\nTomsett, Richard, Dan Harborne, Supriyo Chakraborty, Prudhvi Gurram, and Alun Preece. “Sanity checks for saliency metrics.” In Proceedings of the AAAI Conference on Artificial Intelligence, vol. 34, no. 04, pp. 6021-6029. 2020.\nVan Looveren, Arnaud, and Janis Klaise. “Interpretable counterfactual explanations guided by prototypes.” arXiv preprint arXiv:1907.02584 (2019).\nWachter, Sandra, Brent Mittelstadt, and Chris Russell. “Counterfactual explanations without opening the black box: Automated decisions and the GDPR.” (2017).\nWei, Pengfei, Zhenzhou Lu, and Jingwen Song. “Variable importance analysis: a comprehensive review.” Reliability Engineering & System Safety 142 (2015): 399-432.\nYang, Hongyu, Cynthia Rudin, and Margo Seltzer. “Scalable Bayesian rule lists.” Proceedings of the 34th International Conference on Machine Learning-Volume 70. JMLR. org, 2017.\nZeiler, Matthew D., and Rob Fergus. “Visualizing and understanding convolutional networks.” European conference on computer vision. Springer, Cham (2014).\nZhao, Qingyuan, and Trevor Hastie. “Causal interpretations of black-box models.” Journal of Business & Economic Statistics, to appear. (2017).\n\n\n\n Retour au sommet",
    "crumbs": [
      "Références"
    ]
  },
  {
    "objectID": "14-translations/index.html",
    "href": "14-translations/index.html",
    "title": "Apprentissage automatique interprétable",
    "section": "",
    "text": "Version original (en)\n\n\n\nhttps://christophm.github.io/interpretable-ml-book/translations.html",
    "crumbs": [
      "14 - Traductions"
    ]
  },
  {
    "objectID": "14-translations/index.html#traductions",
    "href": "14-translations/index.html#traductions",
    "title": "Apprentissage automatique interprétable",
    "section": "14 - Traductions",
    "text": "14 - Traductions\n\nVous souhaitez traduire le livre ?\nCe livre est sous licence internationale Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License. Cela signifie que vous êtes autorisé à le traduire et à le mettre en ligne. Vous devez me mentionner comme auteur original et vous n’êtes pas autorisé à vendre le livre.\nSi vous souhaitez traduire le livre, vous pouvez écrire un message et je peux lier votre traduction ici. Mon adresse est christoph.molnar.ai@gmail.com.\n\n\nListe des traductions\n\nBahasa indonésien\n\nUne traduction complète par Hatma Suryotrisongko et Smart City & Cybersecurity Laboratory, Information Technology, ITS.\n\n\n\nChinois\n\nTraduction complète de la 2e édition par Jiazhen de CSDN, une communauté en ligne de programmeurs.\nTraductions complètes by Mingchao Zhu par Mingchao Zhu. Des versions électroniques et imprimées de cette traduction sont disponibles.\nTraduction de la plupart des chapitres par CSDN.\nTraduction de certains chapitres. Le site Web comprend également des questions et réponses de divers utilisateurs.\n\n\n\nFrançais\n\nTraduction (en cours) par Nicolas Guillard.\n\n\n\nJaponais\n\nTraduction complète par Ryuji Masui et l’équipe HACARUS.\n\n\n\nCoréen\n\nTraduction complète en coréen par TooTouch.\nTraduction partielle en coréen par An Subin.\n\n\n\nEspagnol\n\nTraduction espagnole complète par Federico Fliguer.\n\n\n\nVietnamien\n\nUne traduction complète de Giang Nguyen, Duy-Tung Nguyen, Hung-Quang Nguyen, Tri Le et Hoang Nguyen.\n\nSi vous connaissez une autre traduction du livre ou de chapitres individuels, je serais reconnaissant d’en entendre parler et de la lister ici. Vous pouvez me joindre par e-mail : christoph.molnar.ai@gmail.com.\n\n\n\n\n\n\nNote du traducteur\n\n\n\nVous pouvez contacter le traducteur à cette adresse : nicolas.guillard.ai(AT)gmail.com",
    "crumbs": [
      "14 - Traductions"
    ]
  },
  {
    "objectID": "12-contribute/index.html",
    "href": "12-contribute/index.html",
    "title": "12 - Contribuer à ce livre",
    "section": "",
    "text": "Version original (en)\n\n\n\nhttps://christophm.github.io/interpretable-ml-book/contribute.html\n\n\n\n12 - Contribuer à ce livre\nMerci d’avoir lu mon livre sur l’apprentissage automatique interprétable. Le livre est en développement continu. Il sera amélioré au fil du temps et d’autres chapitres seront ajoutés. Très similaire à la façon dont les logiciels sont développés.\nTout le texte et le code du livre original sont open source et disponibles sur github.com. Sur la page GitHub, vous pouvez suggérer des correctifs en ouvrant des tickets si vous trouvez une erreur ou s’il manque quelque chose.\n\n\n\n\n\n\nNote du traducteur\n\n\n\nPour cette version française, le texte et les codes seront ouverts et disponibles sur github, et la suggestion de correctifs est possible en ouvrant un ticket.\n\n\n\n\n\n\n Retour au sommet",
    "crumbs": [
      "12 - Contribuer à ce livre"
    ]
  },
  {
    "objectID": "11-future/11.2-future-interpretability.html",
    "href": "11-future/11.2-future-interpretability.html",
    "title": "Apprentissage automatique interprétable",
    "section": "",
    "text": "Version original (en)\n\n\n\nhttps://christophm.github.io/interpretable-ml-book/the-future-of-interpretability.html",
    "crumbs": [
      "11 - Un regard dans une boule de cristal",
      "11.2 -  L'avenir de l'interprétabilité"
    ]
  },
  {
    "objectID": "11-future/11.2-future-interpretability.html#lavenir-de-linterprétabilité",
    "href": "11-future/11.2-future-interpretability.html#lavenir-de-linterprétabilité",
    "title": "Apprentissage automatique interprétable",
    "section": "11.2 - L’avenir de l’interprétabilité",
    "text": "11.2 - L’avenir de l’interprétabilité\nJetons un coup d’œil à l’avenir possible de l’interprétabilité de l’apprentissage automatique.\nL’accent sera mis sur les outils d’interprétabilité indépendants des modèles.\nIl est beaucoup plus facile d’automatiser l’interprétabilité lorsqu’elle est découplée du modèle d’apprentissage automatique sous-jacent. L’avantage de l’interprétabilité indépendante du modèle réside dans sa modularité. Nous pouvons facilement remplacer le modèle d’apprentissage automatique sous-jacent. On peut tout aussi bien remplacer la méthode d’interprétation. Pour ces raisons, les méthodes indépendantes du modèle évolueront bien mieux. C’est pourquoi je pense que les méthodes indépendantes des modèles deviendront plus dominantes à long terme. Mais les méthodes intrinsèquement interprétables auront également leur place.\nL’apprentissage automatique sera automatisé et, avec lui, l’interprétabilité.\nUne tendance déjà visible est l’automatisation de la formation des modèles. Cela comprend l’ingénierie automatisée et la sélection de caractéristiques, l’optimisation automatisée des hyperparamètres, la comparaison de différents modèles et l’assemblage ou l’empilement des modèles. Le résultat est le meilleur modèle de prédiction possible. Lorsque nous utilisons des méthodes d’interprétation indépendantes du modèle, nous pouvons les appliquer automatiquement à tout modèle émergeant du processus d’apprentissage automatique automatisé. D’une certaine manière, nous pouvons également automatiser cette deuxième étape : calculer automatiquement l’importance des caractéristiques, tracer la dépendance partielle, former un modèle de substitution, etc. Personne ne vous empêche de calculer automatiquement toutes ces interprétations de modèles. L’interprétation proprement dite nécessite encore des personnes. Imaginez : vous téléchargez un ensemble de données, spécifiez l’objectif de prédiction et, d’une simple pression sur un touche, le meilleur modèle de prédiction est formé et le programme crache toutes les interprétations du modèle. Il existe déjà des premiers produits et je soutiens que pour de nombreuses applications, il suffira d’utiliser ces services automatisés d’apprentissage automatique. Aujourd’hui, n’importe qui peut créer des sites Web sans connaître les HTML, CSS et Javascript, mais il existe encore de nombreux développeurs Web. De même, je pense que tout le monde pourra former des modèles d’apprentissage automatique sans savoir programmer, et qu’il y aura toujours besoin d’experts en apprentissage automatique.\nNous n’analysons pas des données, nous analysons des modèles.\nLes données brutes elles-mêmes sont toujours inutiles. (J’exagère volontairement. La réalité est que vous avez besoin d’une compréhension approfondie des données pour mener une analyse significative.) Je m’en fiche des données; Je me soucie des connaissances contenues dans les données. L’apprentissage automatique interprétable est un excellent moyen de distiller des connaissances à partir de données. Vous pouvez sonder le modèle de manière approfondie, il reconnaît automatiquement si et comment les caractéristiques sont pertinentes pour la prédiction (de nombreux modèles ont une sélection de caractéristiques intégrée), il peut détecter automatiquement comment les relations sont représentées et, s’il est correctement entraîné, le modèle final est une très bonne approximation de la réalité.\nDe nombreux outils analytiques sont déjà basés sur des modèles de données (car ils reposent sur des hypothèses de distribution) :\n\nTests d’hypothèse simples comme le test t de Student.\nTests d’hypothèses avec ajustements pour les facteurs de confusion (généralement des GLM)\nAnalyse de variance (ANOVA)\nLe coefficient de corrélation (le coefficient de régression linéaire standardisé est lié au coefficient de corrélation de Pearson)…\n\nCe que je vous dis ici n’est en réalité rien de nouveau. Alors pourquoi passer de l’analyse de modèles transparents fondés sur des hypothèses à l’analyse de modèles de boîte noire sans hypothèses ? Parce que faire toutes ces hypothèses est problématique : elles sont généralement fausses (à moins que vous ne pensiez que la majeure partie du monde suive une distribution gaussienne), difficiles à vérifier, très rigides et difficiles à automatiser. Dans de nombreux domaines, les modèles basés sur des hypothèses ont généralement de moins bonnes performances prédictives sur des données de test intactes que les modèles d’apprentissage automatique en boîte noire. Cela n’est vrai que pour les grands ensembles de données, car les modèles interprétables avec de bonnes hypothèses fonctionnent souvent mieux avec de petits ensembles de données que les modèles en boîte noire. L’approche d’apprentissage automatique en boîte noire nécessite beaucoup de données pour fonctionner correctement. Avec la numérisation de tout, nous disposerons d’ensembles de données toujours plus volumineux et l’approche de l’apprentissage automatique deviendra donc plus attrayante. Nous ne faisons pas d’hypothèses, nous nous rapprochons le plus possible de la réalité (tout en évitant le surajustement des données d’entraînement). Je soutiens que nous devrions développer tous les outils dont nous disposons en statistique pour répondre aux questions (tests d’hypothèses, mesures de corrélation, mesures d’interaction, outils de visualisation, intervalles de confiance, p-valeurs (probabilités critiques), intervalles de prédiction, distributions de probabilité) et les réécrire pour les modèles de boîte noire. D’une certaine manière, cela se produit déjà :\n\nPrenons un modèle linéaire classique : le coefficient de régression standardisé est déjà une mesure de l’importance des caractéristiques. Avec la mesure de l’importance des fonctionnalités de permutation, nous disposons d’un outil qui fonctionne avec n’importe quel modèle.\nDans un modèle linéaire, les coefficients mesurent l’effet d’une seule caractéristique sur le résultat prédit. La version généralisée de ceci est le diagramme de dépendance partielle.\nTestez si A ou B est meilleur : Pour cela, nous pouvons également utiliser des fonctions de dépendance partielle. Ce que nous n’avons pas encore (à ma connaissance), ce sont des tests statistiques pour des modèles arbitraires de boîtes noires.\n\nLes scientifiques des données s’automatiseront eux-mêmes.\nJe pense que les scientifiques des données finiront par s’automatiser pour de nombreuses tâches d’analyse et de prédiction. Pour que cela se produise, les tâches doivent être bien définies et il doit y avoir des processus et des routines autour d’elles. Aujourd’hui, ces routines et processus manquent, mais les scientifiques des données et leurs collègues y travaillent. À mesure que l’apprentissage automatique devient partie intégrante de nombreuses industries et institutions, de nombreuses tâches seront automatisées.\nLes robots et les programmes s’expliqueront.\nNous avons besoin d’interfaces plus intuitives pour les machines et les programmes qui font largement appel à l’apprentissage automatique. Quelques exemples : une voiture autonome qui indique pourquoi elle s’est arrêtée brusquement (“70 % de probabilité qu’un enfant traverse la route”); Un programme de défaut de crédit qui explique à un employé de banque pourquoi une demande de crédit a été rejetée (“Le demandeur a trop de cartes de crédit et occupe un emploi instable.”); Un bras robotique qui explique pourquoi il a déplacé l’article du tapis roulant vers la poubelle (“L’article a un engouement en bas.”).\nL’interprétabilité pourrait stimuler la recherche sur l’intelligence artificielle.\nJ’imagine qu’en effectuant davantage de recherches sur la façon dont les programmes et les machines peuvent s’expliquer, nous pouvons améliorer notre compréhension de l’intelligence et devenir meilleurs dans la création de machines intelligentes.\nEn fin de compte, toutes ces prédictions ne sont que des spéculations et nous devons voir ce que l’avenir nous réserve réellement. Faites-vous votre propre opinion et continuez à apprendre !",
    "crumbs": [
      "11 - Un regard dans une boule de cristal",
      "11.2 -  L'avenir de l'interprétabilité"
    ]
  },
  {
    "objectID": "09-local_model_agnostic_methods/index.html",
    "href": "09-local_model_agnostic_methods/index.html",
    "title": "9 - Méthodes locales indépendantes du modèle",
    "section": "",
    "text": "Avertissement\n\n\n\nEn cours de traduction.\n\n\n\n9 - Méthodes locales indépendantes du modèle\nLes méthodes d’interprétation locales expliquent les prédictions individuelles. Dans ce chapitre, vous découvrirez les méthodes d’explication locale suivantes :\n\nLes courbes d’espérance conditionnelle individuelles sont les éléments constitutifs des tracés de dépendance partielle et décrivent comment la modification d’une caractéristique modifie la prédiction.\nLes modèles de substitution locaux (LIME) expliquent une prédiction en remplaçant le modèle complexe par un modèle de substitution interprétable localement.\nLes règles de portée (ancres) sont des règles qui décrivent quelles valeurs de caractéristiques ancrent une prédiction, dans le sens où elles verrouillent la prédiction en place.\nLes explications contrefactuelles expliquent une prédiction en examinant quelles caractéristiques devraient être modifiées pour obtenir la prédiction souhaitée.\nLes valeurs Shapley sont une méthode d’attribution qui attribue équitablement la prédiction à des caractéristiques individuelles.\nSHAP est une autre méthode de calcul des valeurs de Shapley, mais propose également des méthodes d’interprétation globale basées sur des combinaisons de valeurs de Shapley à travers les données.\n\nLes valeurs LIME et Shapley sont des méthodes d’attribution, de sorte que la prédiction d’une seule instance est décrite comme la somme des effets de caractéristiques. D’autres méthodes, telles que les explications contrefactuelles, sont basées sur des exemples.\n\n\n\n\n Retour au sommet",
    "crumbs": [
      "9 - Méthodes locales indépendantes du modèle"
    ]
  },
  {
    "objectID": "08-global_model_agnostic_methods/08.6-global-surrogate.html",
    "href": "08-global_model_agnostic_methods/08.6-global-surrogate.html",
    "title": "Apprentissage automatique interprétable",
    "section": "",
    "text": "Avertissement\n\n\n\nEn cours de traduction.",
    "crumbs": [
      "8 - Méthodes globales indépendantes du modèle",
      "8.6 - Substitut global"
    ]
  },
  {
    "objectID": "08-global_model_agnostic_methods/08.6-global-surrogate.html#substitut-global",
    "href": "08-global_model_agnostic_methods/08.6-global-surrogate.html#substitut-global",
    "title": "Apprentissage automatique interprétable",
    "section": "8.6 - Substitut global",
    "text": "8.6 - Substitut global\nUn modèle de substitution global est un modèle interprétable qui est entraîné pour se rapprocher des prédictions d’un modèle de boîte noire. Nous pouvons tirer des conclusions sur le modèle de la boîte noire en interprétant le modèle de substitution. Résoudre l’interprétabilité du machine learning en utilisant davantage de machine learning !",
    "crumbs": [
      "8 - Méthodes globales indépendantes du modèle",
      "8.6 - Substitut global"
    ]
  },
  {
    "objectID": "08-global_model_agnostic_methods/08.6-global-surrogate.html#théorie",
    "href": "08-global_model_agnostic_methods/08.6-global-surrogate.html#théorie",
    "title": "Apprentissage automatique interprétable",
    "section": "8.6.1 - Théorie",
    "text": "8.6.1 - Théorie\nLes modèles de substitution sont également utilisés en ingénierie : si un résultat intéressant est coûteux, prend du temps ou est difficile à mesurer (par exemple parce qu’il provient d’une simulation informatique complexe), un modèle de substitution bon marché et rapide du résultat peut être utilisé à la place. La différence entre les modèles de substitution utilisés en ingénierie et dans l’apprentissage automatique interprétable réside dans le fait que le modèle sous-jacent est un modèle d’apprentissage automatique (et non une simulation) et que le modèle de substitution doit être interprétable. Le but des modèles de substitution (interprétables) est de se rapprocher le plus précisément possible des prédictions du modèle sous-jacent tout en étant interprétables. L’idée des modèles de substitution peut se retrouver sous différents noms : modèle d’approximation, métamodèle, modèle de surface de réponse, émulateur, …\nÀ propos de la théorie : il n’y a en fait pas besoin de beaucoup de théorie pour comprendre les modèles de substitution. Nous voulons nous rapprocher le plus possible de notre fonction de prédiction de boîte noire f avec la fonction de prédiction du modèle de substitution g, sous la contrainte que g soit interprétable. Pour la fonction g n’importe quel modèle interprétable – par exemple du chapitre sur les modèles interprétables – peut être utilisé.\nPar exemple un modèle linéaire : \\[g(x)=\\beta_0+\\beta_1{}x_1{}+\\ldots+\\beta_p{}x_p\\]\nOu un arbre de décision : \\[g(x)=\\sum_{m=1}^Mc_m{}I\\{x\\in{}R_m\\}\\]\nLa formation d’un modèle de substitution est une méthode indépendante du modèle, car elle ne nécessite aucune information sur le fonctionnement interne du modèle de boîte noire, seul l’accès aux données et à la fonction de prédiction est nécessaire. Si le modèle d’apprentissage automatique sous-jacent était remplacé par un autre, vous pouvez toujours utiliser la méthode de substitution. Le choix du type de modèle boîte noire et du type de modèle de substitution est découplé.\nEffectuez les étapes suivantes pour obtenir un modèle de substitution :\n\nSélectionnez un ensemble de données X. Il peut s’agir du même ensemble de données que celui utilisé pour entraîner le modèle de boîte noire ou d’un nouvel ensemble de données de la même distribution. Vous pouvez même sélectionner un sous-ensemble de données ou une grille de points, en fonction de votre application.\nPour l’ensemble de données X sélectionné, obtenez les prédictions du modèle de boîte noire.\nSélectionnez un type de modèle interprétable (modèle linéaire, arbre de décision, …)\nEntraînez le modèle interprétable sur l’ensemble de données X et ses prédictions.\nToutes nos félicitations! Vous disposez désormais d’un modèle de substitution.\nMesurez dans quelle mesure le modèle de substitution reproduit les prédictions du modèle de boîte noire.\nInterprétez le modèle de substitution.\n\nVous pouvez trouver des approches pour les modèles de substitution qui comportent des étapes supplémentaires ou diffèrent légèrement, mais l’idée générale est généralement celle décrite ici.\nUne façon de mesurer dans quelle mesure le substitut reproduit le modèle de la boîte noire est la mesure du R carré : \\[R^2=1-\\frac{SSE}{SST}=1-\\frac{\\sum_{i=1}^n(\\hat{y}_*^{(i)}-\\hat{y}^{(i)})^2}{\\sum_{i=1}^n(\\hat{y}^{(i)}-\\bar{\\hat{y}})^2}\\]\noù \\(\\hat{y}_*^{(i)}\\) est la prédiction pour la i-ième instance du modèle de substitution, \\(\\hat{y}^{(i)}\\) la prédiction du modèle de la boîte noire et \\(\\bar{\\hat{y}}\\) la moyenne des prédictions du modèle boîte noire. SSE signifie erreur de somme des carrés et SST pour somme des carrés total. La mesure R au carré peut être interprétée comme le pourcentage de variance capturé par le modèle de substitution. Si R-carré est proche de 1 (= SSE faible), alors le modèle interprétable se rapproche très bien du comportement du modèle boîte noire. Si le modèle interprétable est très proche, vous souhaiterez peut-être remplacer le modèle complexe par le modèle interprétable. Si le R au carré est proche de 0 (= SSE élevé), alors le modèle interprétable ne parvient pas à expliquer le modèle de la boîte noire.\nNotez que nous n’avons pas parlé des performances du modèle de boîte noire sous-jacent, c’est-à-dire de sa bonne ou de sa mauvaise performance dans la prédiction du résultat réel. Les performances du modèle boîte noire ne jouent aucun rôle dans la formation du modèle de substitution. L’interprétation du modèle de substitution est toujours valable car elle fait des déclarations sur le modèle et non sur le monde réel. Mais bien sûr, l’interprétation du modèle de substitution n’a plus d’importance si le modèle de la boîte noire est mauvais, car alors le modèle de la boîte noire lui-même n’est plus pertinent.\nNous pourrions également créer un modèle de substitution basé sur un sous-ensemble des données d’origine ou repondérer les instances. De cette façon, nous modifions la distribution des entrées du modèle de substitution, ce qui modifie l’orientation de l’interprétation (elle n’est alors plus vraiment globale). Si nous pondérons les données localement en fonction d’une instance spécifique des données (plus les instances sont proches de l’instance sélectionnée, plus leur poids est élevé), nous obtenons un modèle de substitution local qui peut expliquer la prédiction individuelle de l’instance. Apprenez-en davantage sur les modèles locaux dans le chapitre suivant.\n\n8.6.2 - Exemple\nPour démontrer les modèles de substitution, nous considérons un exemple de régression et de classification.\nTout d’abord, nous formons une machine à vecteurs de support pour prédire le nombre quotidien de vélos loués en fonction des informations météorologiques et du calendrier. La machine à vecteurs de support n’est pas très interprétable, nous formons donc un substitut avec un arbre de décision CART comme modèle interprétable pour approximer le comportement de la machine à vecteurs de support.\n\n\n\nLes noeuds terminaux d’un arbre de substitution qui se rapproche des prédictions d’une machine à vecteurs de support entraînée sur l’ensemble de données de location de vélos. Les distributions dans les nœuds montrent que l’arbre de substitution prédit un nombre plus élevé de vélos loués lorsque la température est supérieure à 13 degrés Celsius et lorsque le jour est plus tardif sur la période de 2 ans (point de coupure à 435 jours).\n\n\nLe modèle de substitution a un R au carré (variance expliquée) de 0,77, ce qui signifie qu’il se rapproche assez bien du comportement sous-jacent de la boîte noire, mais pas parfaitement. Si l’ajustement était parfait, nous pourrions jeter la machine à vecteurs de support et utiliser l’arbre à la place.\nDans notre deuxième exemple, nous prédisons la probabilité de cancer du col de l’utérus avec une forêt aléatoire. Encore une fois, nous formons un arbre de décision avec l’ensemble de données d’origine, mais avec la prédiction de la forêt aléatoire comme résultat, au lieu des classes réelles (saines ou cancer) à partir des données.\n\n\n\nLes nœuds terminaux d’un arbre de substitution qui se rapproche des prédictions d’une forêt aléatoire formée sur l’ensemble de données sur le cancer du col de l’utérus. Les décomptes dans les nœuds montrent la fréquence des classifications des modèles de boîte noire dans les nœuds.).\n\n\nLe modèle de substitution a un R au carré (variance expliquée) de 0,19, ce qui signifie qu’il ne se rapproche pas bien de la forêt aléatoire et que nous ne devons pas surinterpréter l’arbre lorsque nous tirons des conclusions sur le modèle complexe.\n\n\n8.6.3 - Avantages\nLa méthode du modèle de substitution est flexible : n’importe quel modèle du chapitre sur les modèles interprétables peut être utilisé. Cela signifie également que vous pouvez échanger non seulement le modèle interprétable, mais également le modèle de boîte noire sous-jacent. Supposons que vous créiez un modèle complexe et que vous l’expliquiez aux différentes équipes de votre entreprise. Une équipe est familiarisée avec les modèles linéaires, l’autre équipe peut comprendre les arbres de décision. Vous pouvez former deux modèles de substitution (modèle linéaire et arbre de décision) pour le modèle de boîte noire d’origine et proposer deux types d’explications. Si vous trouvez un modèle de boîte noire plus performant, vous n’êtes pas obligé de modifier votre méthode d’interprétation, car vous pouvez utiliser la même classe de modèles de substitution.\nJe dirais que l’approche est très intuitive et simple. Cela signifie qu’il est facile à mettre en œuvre, mais également facile à expliquer aux personnes non familiarisées avec la science des données ou l’apprentissage automatique.\nAvec la mesure R au carré, nous pouvons facilement mesurer la capacité de nos modèles de substitution à se rapprocher des prédictions de la boîte noire.\n\n\n8.6.4 - Inconvénients\nVous devez être conscient que vous tirez des conclusions sur le modèle et non sur les données, puisque le modèle de substitution ne voit jamais le résultat réel.\nIl n’est pas clair quel est le meilleur seuil pour le R-carré afin d’être sûr que le modèle de substitution est suffisamment proche du modèle de la boîte noire. \\(80%\\) de la variance expliquée ? \\(50%\\) ? \\(99%\\) ?\nNous pouvons mesurer à quel point le modèle de substitution est proche du modèle de la boîte noire. Supposons que nous ne soyons pas très proches, mais suffisamment proches. Il peut arriver que le modèle interprétable soit très proche pour un sous-ensemble de l’ensemble de données, mais très divergent pour un autre sous-ensemble. Dans ce cas, l’interprétation du modèle simple ne serait pas aussi bonne pour tous les points de données.\nLe modèle interprétable que vous choisissez comme substitut présente tous ses avantages et ses inconvénients.\nCertains soutiennent qu’il n’existe, en général, pas de modèles intrinsèquement interprétables (y compris même les modèles linéaires et les arbres de décision) et qu’il serait même dangereux d’avoir une illusion d’interprétabilité. Si vous partagez cet avis, alors bien entendu cette méthode n’est pas pour vous.\n\n\n8.6.5 - Logiciel\nJ’ai utilisé le iml package R pour les exemples. Si vous pouvez former un modèle d’apprentissage automatique, vous devriez alors pouvoir implémenter vous-même des modèles de substitution. Entraînez simplement un modèle interprétable pour prédire les prédictions du modèle boîte noire.",
    "crumbs": [
      "8 - Méthodes globales indépendantes du modèle",
      "8.6 - Substitut global"
    ]
  },
  {
    "objectID": "05-interpretable_models/05.3-glm-gam-more.html",
    "href": "05-interpretable_models/05.3-glm-gam-more.html",
    "title": "Apprentissage automatique interprétable",
    "section": "",
    "text": "Avertissement\n\n\n\nEn cours de traduction.",
    "crumbs": [
      "5 - Modèles interprétables",
      "5.3 - GLM, GAM et plus"
    ]
  },
  {
    "objectID": "05-interpretable_models/05.3-glm-gam-more.html#glm-gam-et-plus",
    "href": "05-interpretable_models/05.3-glm-gam-more.html#glm-gam-et-plus",
    "title": "Apprentissage automatique interprétable",
    "section": "5.3 - GLM, GAM et plus",
    "text": "5.3 - GLM, GAM et plus\nLa plus grande force mais aussi la plus grande faiblesse du modèle de régression linéaire est que la prédiction est modélisée comme une somme pondérée des caractéristiques. De plus, le modèle linéaire vient avec de nombreuses autres hypothèses. La mauvaise nouvelle est (bien, ce n’est pas vraiment une nouveauté) que toutes ces hypothèses sont souvent violées dans la réalité : le résultat étant donné les caractéristiques peut avoir une distribution non gaussienne, les caractéristiques peuvent interagir et la relation entre les caractéristiques et le résultat peut être non linéaire. La bonne nouvelle est que la communauté statistique a développé une variété de modifications qui transforment le modèle de régression linéaire d’une simple lame en un couteau suisse.\nCe chapitre n’est définitivement pas votre guide définitif pour étendre les modèles linéaires. Plutôt, il sert de vue d’ensemble des extensions telles que les Modèles Linéaires Généralisés (GLM) et les Modèles Additifs Généralisés (GAM) et vous donne un peu d’intuition. Après lecture, vous devriez avoir une vue d’ensemble solide de comment étendre les modèles linéaires. Si vous voulez en apprendre davantage sur le modèle de régression linéaire en premier, je vous suggère de lire le chapitre sur les modèles de régression linéaire, si vous ne l’avez pas déjà fait.\nRappelons-nous de la formule d’un modèle de régression linéaire :\n\\[y=\\beta_{0}+\\beta_{1}x_{1}+\\ldots+\\beta_{p}x_{p}+\\epsilon\\]\nLe modèle de régression linéaire suppose que le résultat ( y ) d’une instance peut être exprimé par une somme pondérée de ses ( p ) caractéristiques avec une erreur individuelle ( ) qui suit une distribution gaussienne. En forçant les données dans ce corset de formule, nous obtenons beaucoup d’interprétabilité du modèle. Les effets des caractéristiques sont additifs, ce qui signifie qu’il n’y a pas d’interactions, et la relation est linéaire, ce qui signifie qu’une augmentation d’une caractéristique d’une unité peut être directement traduite en une augmentation/diminution du résultat prédit. Le modèle linéaire nous permet de compresser la relation entre une caractéristique et le résultat attendu en un seul nombre, à savoir le poids estimé.\nMais une simple somme pondérée est trop restrictive pour de nombreux problèmes de prédiction du monde réel. Dans ce chapitre, nous allons apprendre trois problèmes du modèle classique de régression linéaire et comment les résoudre. Il existe de nombreux autres problèmes avec des hypothèses possiblement violées, mais nous nous concentrerons sur les trois présentés dans la figure suivante:\n\n\n\nThree assumptions of the linear model (left side): Gaussian distribution of the outcome given the features, additivity (= no interactions) and linear relationship. Reality usually does not adhere to those assumptions (right side): Outcomes might have non-Gaussian distributions, features might interact and the relationship might be nonlinear.\n\n\nIl existe une solution à tous ces problèmes :\nProblème : Le résultat cible \\(y\\) étant donné les caractéristiques ne suit pas une distribution gaussienne.\nExemple : Supposons que je veuille prédire combien de minutes je vais faire du vélo un jour donné. Comme caractéristiques, j’ai le type de journée, la météo, etc. Si j’utilise un modèle linéaire, il pourrait prédire des minutes négatives car il suppose une distribution gaussienne qui ne s’arrête pas à 0 minute. Aussi, si je veux prédire des probabilités avec un modèle linéaire, je peux obtenir des probabilités qui sont négatives ou supérieures à 1.\nSolution : Modèles Linéaires Généralisés (GLMs).\nProblème : Les caractéristiques interagissent.\nExemple : En moyenne, une légère pluie a un effet légèrement négatif sur mon envie de faire du vélo. Mais en été, pendant l’heure de pointe, j’apprécie la pluie, car alors tous les cyclistes du beau temps restent à la maison et j’ai les pistes cyclables pour moi ! C’est une interaction entre le temps et la météo que ne peut pas capturer un modèle purement additif.\nSolution : Ajout manuel d’interactions.\nProblème : La véritable relation entre les caractéristiques et ( y ) n’est pas linéaire.\nExemple : Entre 0 et 25 degrés Celsius, l’influence de la température sur mon envie de faire du vélo pourrait être linéaire, ce qui signifie qu’une augmentation de 0 à 1 degré provoque la même augmentation de l’envie de faire du vélo qu’une augmentation de 20 à 21. Mais à des températures plus élevées, ma motivation à faire du vélo se stabilise et diminue même - je n’aime pas faire du vélo quand il fait trop chaud.\nSolutions : Modèles Additifs Généralisés (GAMs) ; transformation des caractéristiques.\nLes solutions à ces trois problèmes sont présentées dans ce chapitre. De nombreuses autres extensions du modèle linéaire sont omises. Si je tentais de tout couvrir ici, le chapitre se transformerait rapidement en un livre dans un livre sur un sujet déjà couvert dans de nombreux autres livres. Mais puisque vous êtes déjà ici, j’ai fait un petit aperçu du problème plus solution pour les extensions de modèle linéaire, que vous pouvez trouver à la fin du chapitre. Le nom de la solution est destiné à servir de point de départ pour une recherche.\n\n5.3.1 - Non-Gaussian Outcomes - GLMs\nLe modèle de régression linéaire suppose que le résultat étant donné les caractéristiques d’entrée suit une distribution gaussienne. Cette hypothèse exclut de nombreux cas : le résultat peut également être une catégorie (cancer vs. sain), un décompte (nombre d’enfants), le temps jusqu’à la survenue d’un événement (temps jusqu’à la défaillance d’une machine) ou un résultat très biaisé avec quelques valeurs très élevées (revenu des ménages). Le modèle de régression linéaire peut être étendu pour modéliser tous ces types de résultats. Cette extension est appelée Modèles Linéaires Généralisés ou GLMs en abrégé. Tout au long de ce chapitre, j’utiliserai le nom GLM à la fois pour le cadre général et pour les modèles particuliers de ce cadre. Le concept central de tout GLM est : Conserver la somme pondérée des caractéristiques, mais autoriser des distributions de résultat non gaussiennes et connecter la moyenne attendue de cette distribution et la somme pondérée à travers une fonction éventuellement non linéaire. Par exemple, le modèle de régression logistique suppose une distribution de Bernoulli pour le résultat et relie la moyenne attendue et la somme pondérée en utilisant la fonction logistique.\nLe GLM lie mathématiquement la somme pondérée des caractéristiques avec la valeur moyenne de la distribution supposée en utilisant la fonction de lien ( g ), qui peut être choisie de manière flexible en fonction du type de résultat.\n\\[g(E_Y(y|x))=\\beta_0+\\beta_1{}x_{1}+\\ldots{}\\beta_p{}x_{p}\\]\nLes GLMs se composent de trois composants : La fonction de lien \\(g\\), la somme pondérée \\(X^T\\beta\\) (parfois appelée prédicteur linéaire) et une distribution de probabilité de la famille exponentielle qui définit \\(E_Y\\).\nLa famille exponentielle est un ensemble de distributions qui peuvent être écrites avec la même formule (paramétrée) incluant un exposant, la moyenne et la variance de la distribution et d’autres paramètres. Je ne vais pas entrer dans les détails mathématiques car c’est un très grand univers en soi que je ne souhaite pas aborder. Wikipedia a une liste ordonnée des distributions de la famille exponentielle. N’importe quelle distribution de cette liste peut être choisie pour votre GLM. En fonction du type de résultat que vous souhaitez prédire, choisissez une distribution adaptée. Le résultat est-il un décompte de quelque chose (par exemple, le nombre d’enfants vivant dans un ménage) ? Alors la distribution de Poisson pourrait être un bon choix. Le résultat est-il toujours positif (par exemple, le temps entre deux événements) ? Alors la distribution exponentielle pourrait être un bon choix.\nConsidérons le modèle linéaire classique comme un cas particulier d’un GLM. La fonction de lien pour la distribution gaussienne dans le modèle linéaire classique est simplement la fonction identité. La distribution gaussienne est paramétrée par les paramètres de moyenne et de variance. La moyenne décrit la valeur que nous attendons en moyenne et la variance décrit combien les valeurs varient autour de cette moyenne. Dans le modèle linéaire, la fonction de lien relie la somme pondérée des caractéristiques à la moyenne de la distribution gaussienne.\nDans le cadre du GLM, ce concept se généralise à toute distribution (de la famille exponentielle) et à des fonctions de lien arbitraires. Si \\(y\\) est un décompte de quelque chose, comme le nombre de cafés que quelqu’un boit un jour donné, nous pourrions le modéliser avec un GLM avec une distribution de Poisson et le logarithme naturel comme fonction de lien :\n\\[ln(E_Y(y|x))=x^{T}\\beta\\]\nLe modèle de régression logistique est également un GLM qui suppose une distribution de Bernoulli et utilise la fonction logit comme fonction de lien. La moyenne de la distribution binomiale utilisée dans la régression logistique est la probabilité que \\(y\\) soit 1.\n\\[x^{T}\\beta=ln\\left(\\frac{E_Y(y|x)}{1-E_Y(y|x)}\\right)=ln\\left(\\frac{P(y=1|x)}{1-P(y=1|x)}\\right)\\]\nEt si nous résolvons cette équation pour avoir \\(P(y=1)\\) d’un côté, nous obtenons la formule de la régression logistique :\n\\[P(y=1)=\\frac{1}{1+exp(-x^{T}\\beta)}\\]\nChaque distribution de la famille exponentielle a une fonction de lien canonique qui peut être dérivée mathématiquement de la distribution. Le cadre GLM permet de choisir la fonction de lien indépendamment de la distribution. Comment choisir la bonne fonction de lien ? Il n’y a pas de recette parfaite. Vous prenez en compte les connaissances sur la distribution de votre cible, mais aussi des considérations théoriques et la manière dont le modèle s’adapte à vos données réelles. Pour certaines distributions, la fonction de lien canonique peut conduire à des valeurs qui sont invalides pour cette distribution. Dans le cas de la distribution exponentielle, la fonction de lien canonique est l’inverse négatif, ce qui peut conduire à des prédictions négatives qui sont en dehors du domaine de la distribution exponentielle. Puisque vous pouvez choisir n’importe quelle fonction de lien, la solution simple est de choisir une autre fonction qui respecte le domaine de la distribution.\nExemples\nJ’ai simulé un ensemble de données sur le comportement de consommation de café pour souligner le besoin de GLMs. Supposons que vous ayez collecté des données sur votre comportement quotidien de consommation de café. Si vous n’aimez pas le café, prétendez qu’il s’agit de thé. En plus du nombre de tasses, vous enregistrez votre niveau de stress actuel sur une échelle de 1 à 10, comment vous avez bien dormi la nuit précédente sur une échelle de 1 à 10 et si vous avez dû travailler ce jour-là. L’objectif est de prédire le nombre de cafés étant donné les caractéristiques stress, sommeil et travail. J’ai simulé des données pour 200 jours. Le stress et le sommeil ont été tirés uniformément entre 1 et 10 et le travail oui/non a été tiré avec une chance de 50/50 (quelle vie !). Pour chaque jour, le nombre de cafés a ensuite été tiré d’une distribution de Poisson, modélisant l’intensité ( ) (qui est aussi la valeur attendue de la distribution de Poisson) comme une fonction des caractéristiques sommeil, stress et travail. Vous pouvez deviner où cette histoire va mener : “Hé, essayons de modéliser ces données avec un modèle linéaire … Oh ça ne fonctionne pas … Essayons un GLM avec une distribution de Poisson … SURPRISE ! Maintenant ça fonctionne !”. J’espère ne pas avoir trop gâché l’histoire pour vous.\nRegardons la distribution de la variable cible, le nombre de cafés pour un jour donné.\n\n\n\nSimulated distribution of number of daily coffees for 200 days.\n\n\nSur r tab[1,2] des r n jours, vous n’avez pas bu de café du tout et le jour le plus extrême, vous avez bu r tab[nrow(tab),1] cafés. Utilisons naïvement un modèle linéaire pour prédire le nombre de cafés en utilisant le niveau de sommeil, le niveau de stress et le travail oui/non comme caractéristiques. Qu’est-ce qui peut mal tourner lorsque nous supposons à tort une distribution gaussienne ? Une mauvaise hypothèse peut invalider les estimations, en particulier les intervalles de confiance des poids. Un problème plus évident est que les prédictions ne correspondent pas au domaine “autorisé” du résultat réel, comme le montre la figure suivante.\n\n\n\nPredicted number of coffees dependent on stress, sleep and work. The linear model predicts negative values.\n\n\nLe modèle linéaire n’a pas de sens, car il prédit un nombre négatif de cafés. Ce problème peut être résolu avec les Modèles Linéaires Généralisés (GLMs). Nous pouvons changer la fonction de lien et la distribution supposée. Une possibilité est de conserver la distribution gaussienne et d’utiliser une fonction de lien qui conduit toujours à des prédictions positives telles que le lien logarithmique (l’inverse est la fonction exp) au lieu de la fonction identité. Encore mieux : Nous choisissons une distribution qui correspond au processus de génération des données et une fonction de lien appropriée. Puisque le résultat est un décompte, la distribution de Poisson est un choix naturel, accompagnée du logarithme comme fonction de lien. Dans ce cas, les données ont même été générées avec la distribution de Poisson, donc le GLM de Poisson est le choix parfait. Le GLM de Poisson ajusté conduit à la distribution suivante des valeurs prédites :\n\n\n\nPredicted number of coffees dependent on stress, sleep and work. The GLM with Poisson assumption and log link is an appropriate model for this dataset.\n\n\nPlus aucun nombre négatif de café, cela semble bien meilleur.\nInterprétation des poids de GLM\nLa distribution supposée, combinée à la fonction de lien, détermine comment les poids des caractéristiques estimés sont interprétés. Dans l’exemple du comptage de cafés, j’ai utilisé un GLM avec une distribution de Poisson et un lien logarithmique, ce qui implique la relation suivante entre le résultat attendu et les caractéristiques stress (\\(str\\)), sommeil (\\(slp\\)) et travail (\\(wrk\\)).\n\\[ln(E(\\text{coffee}|\\text{str},\\text{slp},\\text{wrk}))=\\beta_0+\\beta_{\\text{str}}x_{\\text{str}}+\\beta_{\\text{slp}}x_{\\text{slp}}+\\beta_{\\text{wrk}}x_{\\text{wrk}}\\]\nPour interpréter les poids, nous inversons la fonction de lien afin de pouvoir interpréter l’effet des caractéristiques sur le résultat attendu et non sur le logarithme du résultat attendu.\n\\[E(\\text{coffee}|\\text{str},\\text{slp},\\text{wrk})=exp(\\beta_0+\\beta_{\\text{str}}x_{\\text{str}}+\\beta_{\\text{slp}}x_{\\text{slp}}+\\beta_{\\text{wrk}}x_{\\text{wrk}})\\]\nPuisque tous les poids sont dans la fonction exponentielle, l’interprétation de l’effet n’est pas additive, mais multiplicative, car exp(a + b) est exp(a) multiplié par exp(b). Le dernier élément pour l’interprétation est les poids réels de l’exemple jouet. Le tableau suivant répertorie les poids estimés et exp(poids) avec l’intervalle de confiance à 95 % :\n\n\n\nTable Weights in the Poisson model\n\n\nAugmenter le niveau de stress d’un point multiplie le nombre de cafés attendu par le facteur r round(cc[\"stress\", \"exp.beta\"], 2). Augmenter la qualité du sommeil d’un point multiplie le nombre de cafés attendu par le facteur r round(cc[\"sleep\", \"exp.beta\"], 2). Le nombre prévu de cafés lors d’un jour de travail est en moyenne r round(cc[\"workYES\", \"exp.beta\"], 2) fois le nombre de cafés lors d’un jour de repos. En résumé, plus le stress est grand, moins le sommeil est bon et plus le travail est abondant, plus la consommation de café est élevée.\nDans cette section, vous avez appris un peu sur les Modèles Linéaires Généralisés qui sont utiles lorsque la cible ne suit pas une distribution gaussienne. Ensuite, nous examinerons comment intégrer les interactions entre deux caractéristiques dans le modèle de régression linéaire.\n\n\n5.3.2 - Interactions\nLe modèle de régression linéaire suppose que l’effet d’une caractéristique est le même, indépendamment des valeurs des autres caractéristiques (= pas d’interactions). Mais souvent, il existe des interactions dans les données. Pour prédire le nombre de vélos loués, il peut y avoir une interaction entre la température et le fait qu’il s’agisse ou non d’un jour ouvrable. Peut-être que, lorsque les gens doivent travailler, la température n’influence pas beaucoup le nombre de vélos loués, car les gens vont faire du vélo pour se rendre au travail quoi qu’il arrive. Les jours de congé, beaucoup de gens font du vélo pour le plaisir, mais seulement quand il fait assez chaud. En ce qui concerne les vélos de location, on pourrait s’attendre à une interaction entre la température et les jours ouvrables.\nComment pouvons-nous amener le modèle linéaire à inclure des interactions ? Avant d’ajuster le modèle linéaire, ajoutez une colonne à la matrice de caractéristiques qui représente l’interaction entre les caractéristiques et ajustez le modèle comme d’habitude. La solution est élégante d’une certaine manière, car elle ne nécessite aucun changement du modèle linéaire, seulement des colonnes supplémentaires dans les données. Dans l’exemple du jour ouvrable et de la température, nous ajouterions une nouvelle caractéristique qui a des zéros pour les jours non ouvrables, sinon elle a la valeur de la caractéristique de température, en supposant que le jour ouvrable est la catégorie de référence.\nSupposons que nos données ressemble à ceci :\n\nLa matrice de données utilisée par le modèle linéaire semble légèrement différente. Le tableau suivant montre à quoi ressemblent les données préparées pour le modèle lorsque nous ne spécifions aucune interaction. Normalement, cette transformation est effectuée automatiquement par tout logiciel statistique.\n\nLa première colonne est le terme d’interception. La deuxième colonne code la caractéristique catégorielle, avec 0 pour la catégorie de référence et 1 pour l’autre. La troisième colonne contient la température.\nSi nous souhaitons que le modèle linéaire prenne en compte l’interaction entre la température et la caractéristique de jour ouvrable, nous devons ajouter une colonne pour cette interaction.\n\nLa nouvelle colonne “workY.temp” capture l’interaction entre les caractéristiques jour ouvrable (work) et température (temp). Cette nouvelle colonne de caractéristique est zéro pour une instance si la caractéristique travail est dans la catégorie de référence (“N” pour non jour ouvrable), sinon elle prend les valeurs de la caractéristique de température des instances. Avec ce type de codage, le modèle linéaire peut apprendre un effet linéaire différent de la température pour les deux types de jours. C’est l’effet d’interaction entre les deux caractéristiques. Sans un terme d’interaction, l’effet combiné d’une caractéristique catégorielle et d’une caractéristique numérique peut être décrit par une ligne qui est déplacée verticalement pour les différentes catégories. Si nous incluons l’interaction, nous permettons à l’effet des caractéristiques numériques (la pente) d’avoir une valeur différente dans chaque catégorie.\nL’interaction de deux caractéristiques catégorielles fonctionne de manière similaire. Nous créons des caractéristiques supplémentaires qui représentent des combinaisons de catégories. Voici des données artificielles contenant le jour ouvrable (work) et une caractéristique météorologique catégorielle (wthr) :\n\nEnsuite, nous incluons les termes d’interaction :\n\nLa première colonne sert à estimer l’interception. La deuxième colonne est la caractéristique de travail codée. Les colonnes trois et quatre sont pour la caractéristique météorologique, qui nécessite deux colonnes car vous avez besoin de deux poids pour capturer l’effet pour trois catégories, dont l’une est la catégorie de référence. Le reste des colonnes capture les interactions. Pour chaque catégorie des deux caractéristiques (sauf pour les catégories de référence), nous créons une nouvelle colonne de caractéristique qui est 1 si les deux caractéristiques ont une certaine catégorie, sinon 0.\nPour deux caractéristiques numériques, la colonne d’interaction est encore plus facile à construire : nous multiplions simplement les deux caractéristiques numériques.\nIl existe des approches pour détecter et ajouter automatiquement des termes d’interaction. L’une d’elles peut être trouvée dans le chapitre RuleFit. L’algorithme RuleFit extrait d’abord les termes d’interaction puis estime un modèle de régression linéaire incluant les interactions.\nExample\nRevenons à la tâche de prédiction de location de vélos que nous avons déjà modélisée dans le chapitre sur le modèle linéaire. Cette fois, nous considérons également une interaction entre la température et la caractéristique du jour ouvrable. Cela aboutit aux poids estimés et aux intervalles de confiance suivants.\n\nL’effet d’interaction supplémentaire est négatif (r round(lm_summary_print['workingdayWORKING DAY:temp','Estimate'], 1)) et diffère significativement de zéro, comme le montre l’intervalle de confiance à 95%, qui n’inclut pas zéro. D’ailleurs, les données ne sont pas iid, car les jours proches les uns des autres ne sont pas indépendants les uns des autres. Les intervalles de confiance pourraient être trompeurs, prenez-le donc avec des pincettes. Le terme d’interaction change l’interprétation des poids des caractéristiques impliquées. La température a-t-elle un effet négatif étant donné qu’il s’agit d’un jour ouvrable ? La réponse est non, même si le tableau le suggère à un utilisateur non formé. Nous ne pouvons pas interpréter le poids de l’interaction “workingdayWORKING DAY:temp” isolément, car l’interprétation serait : “Tout en laissant inchangées toutes les autres valeurs des caractéristiques, augmenter l’effet d’interaction de la température pour un jour ouvrable diminue le nombre prévu de vélos.” Mais l’effet d’interaction ne s’ajoute qu’à l’effet principal de la température. Supposons qu’il s’agisse d’un jour ouvrable et que nous voulions savoir ce qui se passerait si la température était plus chaude de 1 degré aujourd’hui. Alors nous devons additionner à la fois les poids pour “temp” et “workingdayWORKING DAY:temp” pour déterminer de combien l’estimation augmente.\n\n\n\nThe effect (including interaction) of temperature and working day on the predicted number of bikes for a linear model. Effectively, we get two slopes for the temperature, one for each category of the working day feature.\n\n\n\n\n5.3.3 - Nonlinear Effects - GAMs\nLe monde n’est pas linéaire\nLa linéarité dans les modèles linéaires signifie que, quelle que soit la valeur qu’une instance possède dans une caractéristique particulière, augmenter la valeur d’une unité a toujours le même effet sur le résultat prédit. Est-il raisonnable de supposer qu’augmenter la température d’un degré à 10 degrés Celsius a le même effet sur le nombre de vélos de location qu’augmenter la température quand elle atteint déjà 40 degrés ? Intuitivement, on s’attend à ce qu’augmenter la température de 10 à 11 degrés Celsius ait un effet positif sur les locations de vélos et de 40 à 41 un effet négatif, ce qui est également le cas, comme vous le verrez, dans de nombreux exemples tout au long du livre. La caractéristique de la température a un effet linéaire et positif sur le nombre de vélos de location, mais à un certain point, cet effet s’aplatit et a même un effet négatif à des températures élevées. Le modèle linéaire ne s’en préoccupe pas, il trouvera consciencieusement le meilleur plan linéaire (en minimisant la distance euclidienne).\nVous pouvez modéliser des relations non linéaires en utilisant l’une des techniques suivantes :\n\nTransformation simple de la caractéristique (par ex. logarithme)\nCatégorisation de la caractéristique\nModèles Additifs Généralisés (GAMs)\n\nAvant de détailler chaque méthode, commençons par un exemple qui illustre ces trois techniques. J’ai pris le jeu de données de location de vélos et entraîné un modèle linéaire avec uniquement la caractéristique de la température pour prédire le nombre de vélos de location. La figure suivante montre la pente estimée avec : le modèle linéaire standard, un modèle linéaire avec la température transformée (logarithme), un modèle linéaire traitant la température comme une caractéristique catégorielle et utilisant des splines de régression (GAM).\n\n\n\nPredicting the number of rented bicycles using only the temperature feature. A linear model (top left) does not fit the data well. One solution is to transform the feature with e.g. the logarithm (top right), categorize it (bottom left), which is usually a bad decision, or use Generalized Additive Models that can automatically fit a smooth curve for temperature (bottom right).\n\n\nTransformation de caractéristique\nSouvent, le logarithme de la caractéristique est utilisé comme transformation. Utiliser le logarithme indique que chaque augmentation de température de 10 fois a le même effet linéaire sur le nombre de vélos, donc passer de 1 degré Celsius à 10 degrés Celsius a le même effet que de passer de 0,1 à 1 (ce qui semble incorrect). D’autres exemples de transformations de caractéristiques sont la racine carrée, la fonction carrée et la fonction exponentielle. Utiliser une transformation de caractéristique signifie que vous remplacez la colonne de cette caractéristique dans les données par une fonction de la caractéristique, comme le logarithme, et ajustez le modèle linéaire comme d’habitude. Certains programmes statistiques vous permettent également de spécifier des transformations dans l’appel du modèle linéaire. Vous pouvez être créatif lorsque vous transformez la caractéristique. L’interprétation de la caractéristique change selon la transformation sélectionnée. Si vous utilisez une transformation logarithmique, l’interprétation dans un modèle linéaire devient : “Si le logarithme de la caractéristique est augmenté d’un, la prédiction est augmentée du poids correspondant.” Lorsque vous utilisez un GLM avec une fonction de lien qui n’est pas la fonction identité, alors l’interprétation devient plus compliquée, car vous devez incorporer les deux transformations dans l’interprétation (sauf quand elles se neutralisent mutuellement, comme log et exp, alors l’interprétation devient plus facile).\nCatégorisation des caractéristiques\nUne autre possibilité pour obtenir un effet non linéaire est de discrétiser la caractéristique ; la transformer en une caractéristique catégorielle. Par exemple, vous pourriez découper la caractéristique de température en 20 intervalles avec les niveaux \\([-10, -5)\\), \\([-5, 0)\\), … et ainsi de suite. Lorsque vous utilisez la température catégorisée au lieu de la température continue, le modèle linéaire estimerait une fonction en escalier car chaque niveau obtient sa propre estimation. Le problème avec cette approche est qu’elle nécessite plus de données, qu’elle est plus susceptible de surajuster et qu’il n’est pas clair comment discrétiser la caractéristique de manière significative (intervalle équidistant ou quantiles ? combien d’intervalles ?). Je n’utiliserais la discrétisation que s’il y a un cas très solide pour cela. Par exemple, pour rendre le modèle comparable à une autre étude.\nModèles Additifs Généralisés (GAMs : Generalized Additive Models)\nPourquoi ne pas ‘simplement’ permettre au modèle linéaire (généralisé) d’apprendre des relations non linéaires ? C’est la motivation derrière les GAMs. Les GAMs assouplissent la restriction selon laquelle la relation doit être une simple somme pondérée, et supposent à la place que le résultat peut être modélisé par une somme de fonctions arbitraires de chaque caractéristique. Mathématiquement, la relation dans un GAM se présente comme ceci :\n\\[g(E_Y(y|x))=\\beta_0+f_1(x_{1})+f_2(x_{2})+\\ldots+f_p(x_{p})\\]\nLa formule est similaire à celle du GLM avec la différence que le terme linéaire \\(\\beta_j{}x_{j}\\) est remplacé par une fonction plus flexible \\(f_j(x_{j})\\). Le cœur d’un GAM est toujours une somme des effets des caractéristiques, mais vous avez la possibilité de permettre des relations non linéaires entre certaines caractéristiques et la sortie. Les effets linéaires sont également couverts par le cadre, car pour les caractéristiques à traiter de manière linéaire, vous pouvez limiter leur \\(f_j(x_{j})\\) uniquement à prendre la forme de \\(x_{j}\\beta_j\\).\nLa grande question est comment apprendre des fonctions non linéaires. La réponse s’appelle “splines” ou “fonctions spline”. Les splines sont des fonctions qui sont construites à partir de fonctions de base plus simples. Les splines peuvent être utilisées pour approximer d’autres fonctions plus complexes. Un peu comme empiler des briques Lego pour construire quelque chose de plus complexe. Il existe un nombre déroutant de façons de définir ces fonctions de base spline. Si vous êtes intéressé à en savoir plus sur toutes les manières de définir les fonctions de base, je vous souhaite bonne chance dans votre voyage. Je ne vais pas entrer dans les détails ici, je vais juste construire une intuition. Ce qui m’a personnellement le plus aidé à comprendre les splines était de visualiser les fonctions de base individuelles et de regarder comment la matrice de données est modifiée. Par exemple, pour modéliser la température avec des splines, nous supprimons la caractéristique de température des données et la remplaçons par, disons, 4 colonnes, chacune représentant une fonction de base spline. Habituellement, vous auriez plus de fonctions de base spline, j’ai seulement réduit le nombre à des fins d’illustration. La valeur pour chaque instance de ces nouvelles caractéristiques de base spline dépend des valeurs de température des instances. Avec tous les effets linéaires, le GAM estime également ces poids spline. Les GAMs introduisent également un terme de pénalité pour les poids pour les garder proches de zéro. Cela réduit effectivement la flexibilité des splines et réduit le surajustement. Un paramètre de lissage couramment utilisé pour contrôler la flexibilité de la courbe est ensuite ajusté via la validation croisée. En ignorant le terme de pénalité, la modélisation non linéaire avec des splines est une ingénierie de caractéristiques sophistiquée.\nDans l’exemple où nous prédisons le nombre de vélos avec un GAM en utilisant uniquement la température, la matrice des caractéristiques du modèle ressemble à ceci :\n\nChaque ligne représente une instance individuelle des données (un jour). Chaque colonne de base spline contient la valeur de la fonction de base spline aux valeurs de température particulières. La figure suivante montre à quoi ressemblent ces fonctions de base spline :\n\n\n\nTo smoothly model the temperature effect, we use 4 spline basis functions. Each temperature value is mapped to (here) 4 spline basis values. If an instance has a temperature of 30 °C, the value for the first spline basis feature is -1, for the second 0.7, for the third -0.8 and for the 4th 1.7.\n\n\nLe modèle GAM attribue des poids à chaque caractéristique de base de spline de température :\n\nEt la courbe réelle, qui résulte de la somme des fonctions de base de spline pondérées avec les poids estimés, ressemble à ceci :\n\n\n\nGAM feature effect of the temperature for predicting the number of rented bikes (temperature used as the only feature).\n\n\nL’interprétation des effets lissés nécessite une vérification visuelle de la courbe ajustée. Les splines sont généralement centrées autour de la prédiction moyenne, de sorte qu’un point sur la courbe représente la différence par rapport à la prédiction moyenne. Par exemple, à 0 degré Celsius, le nombre prévu de vélos est inférieur de 3000 à la prédiction moyenne.\n\n\n5.3.4 - Avantages\nToutes ces extensions du modèle linéaire constituent en elles-mêmes une sorte d’univers. Quels que soient les problèmes que vous rencontrez avec les modèles linéaires, vous trouverez probablement une extension qui les résout.\nLa plupart des méthodes sont utilisées depuis des décennies. Par exemple, les GAMs ont presque 30 ans. De nombreux chercheurs et praticiens de l’industrie sont très expérimentés avec les modèles linéaires et les méthodes sont acceptées dans de nombreuses communautés comme statu quo pour la modélisation.\nEn plus de faire des prédictions, vous pouvez utiliser les modèles pour faire de l’inférence, tirer des conclusions sur les données – à condition que les hypothèses du modèle ne soient pas violées. Vous obtenez des intervalles de confiance pour les poids, des tests de significativité, des intervalles de prédiction et bien plus encore.\nLes logiciels statistiques ont généralement de très bonnes interfaces pour ajuster les GLMs, les GAMs et d’autres modèles linéaires plus spéciaux.\nL’opacité de nombreux modèles d’apprentissage automatique provient de 1) un manque de parcimonie, ce qui signifie que de nombreuses caractéristiques sont utilisées, 2) des caractéristiques qui sont traitées de manière non linéaire, ce qui signifie que vous avez besoin de plus qu’un seul poids pour décrire l’effet, et 3) la modélisation des interactions entre les caractéristiques. En supposant que les modèles linéaires sont hautement interprétables mais souvent sous-ajustés à la réalité, les extensions décrites dans ce chapitre offrent un bon moyen d’atteindre une transition en douceur vers des modèles plus flexibles, tout en préservant une partie de l’interprétabilité.\n\n\n5.3.5 - Inconvénients\nComme avantage, j’ai mentionné que les modèles linéaires vivent dans leur propre univers. Le nombre de façons dont vous pouvez étendre le simple modèle linéaire est écrasant, pas seulement pour les débutants. En réalité, il existe plusieurs univers parallèles, car de nombreuses communautés de chercheurs et de praticiens ont leurs propres noms pour des méthodes qui font plus ou moins la même chose, ce qui peut être très déroutant.\nLa plupart des modifications du modèle linéaire rendent le modèle moins interprétable. Toute fonction de lien (dans un GLM) qui n’est pas la fonction identité complique l’interprétation ; les interactions compliquent également l’interprétation ; les effets non linéaires des caractéristiques sont soit moins intuitifs (comme la transformation logarithmique) soit ne peuvent plus être résumés par un seul nombre (par exemple, les fonctions spline).\nLes GLMs, les GAMs, etc. reposent sur des hypothèses concernant le processus de génération des données. Si celles-ci sont violées, l’interprétation des poids n’est plus valide.\nLa performance des ensembles basés sur des arbres comme la forêt aléatoire ou le boosting d’arbres par gradient est dans de nombreux cas meilleure que celle des modèles linéaires les plus sophistiqués. Cela repose en partie sur ma propre expérience et en partie sur des observations des modèles gagnants sur des plateformes comme kaggle.com.\n\n\n5.3.6 - Logiciel\nTous les exemples de ce chapitre ont été créés en utilisant le langage R. Pour les GAMs, le package gam a été utilisé, mais il en existe beaucoup d’autres. R possède un nombre incroyable de packages pour étendre les modèles de régression linéaire. Inégalé par aucun autre langage d’analyse, R est le foyer de toutes les extensions concevables de l’extension du modèle de régression linéaire. Vous trouverez des implémentations de GAMs en Python (comme pyGAM), mais ces implémentations ne sont pas aussi matures.\n\n\n5.3.7 - D’autres extensions\nComme promis, voici une liste de problèmes que vous pourriez rencontrer avec les modèles linéaires, accompagnée du nom d’une solution à ce problème que vous pouvez copier et coller dans votre moteur de recherche préféré.\nMes données violent l’hypothèse d’être indépendantes et identiquement distribuées (iid).\nPar exemple, des mesures répétées sur le même patient.\nRecherchez modèles mixtes ou équations d’estimation généralisées.\nMon modèle a des erreurs hétéroscédastiques.\nPar exemple, lors de la prédiction de la valeur d’une maison, les erreurs du modèle sont généralement plus élevées dans les maisons coûteuses, ce qui viole l’homoscédasticité du modèle linéaire.\nRecherchez régression robuste.\nJ’ai des valeurs aberrantes qui influencent fortement mon modèle.\nRecherchez régression robuste.\nJe veux prédire le temps jusqu’à la survenue d’un événement.\nLes données de temps jusqu’à l’événement viennent généralement avec des mesures censurées, ce qui signifie que pour certaines instances, il n’y avait pas assez de temps pour observer l’événement. Par exemple, une entreprise veut prédire la défaillance de ses machines à glaçons, mais n’a des données que pour deux ans. Certaines machines sont encore intactes après deux ans, mais pourraient tomber en panne plus tard.\nRecherchez modèles de survie paramétriques, régression de Cox, analyse de survie.\nMon résultat à prédire est une catégorie.\nSi le résultat a deux catégories, utilisez un modèle de régression logistique, qui modélise la probabilité pour les catégories.\nSi vous avez plus de catégories, recherchez régression multinomiale.\nLa régression logistique et la régression multinomiale sont toutes deux des GLMs.\nJe veux prédire des catégories ordonnées.\nPar exemple, les notes scolaires.\nRecherchez modèle à cotes proportionnelles.\nMon résultat est un décompte (comme le nombre d’enfants dans une famille).\nRecherchez régression de Poisson.\nLe modèle de Poisson est également un GLM.\nVous pourriez également avoir le problème que la valeur du décompte 0 est très fréquente.\nRecherchez régression de Poisson à inflation de zéros, modèle à obstacle.\nJe ne suis pas sûr des caractéristiques à inclure dans le modèle pour tirer des conclusions causales correctes.\nPar exemple, je veux connaître l’effet d’un médicament sur la pression artérielle. Le médicament a un effet direct sur certaines valeurs sanguines et cette valeur sanguine affecte le résultat.\nDois-je inclure la valeur sanguine dans le modèle de régression ?\nRecherchez inférence causale, analyse de médiation.\nJ’ai des données manquantes.\nRecherchez imputation multiple.\nJe veux intégrer des connaissances préalables dans mes modèles.\nRecherchez inférence bayésienne.\nJe me sens un peu déprimé dernièrement.\nRecherchez “Amazon Alexa Gone Wild!!! Version complète du début à la fin”.",
    "crumbs": [
      "5 - Modèles interprétables",
      "5.3 - GLM, GAM et plus"
    ]
  },
  {
    "objectID": "04-datasets/index.html",
    "href": "04-datasets/index.html",
    "title": "4 - Jeux de données",
    "section": "",
    "text": "Avertissement\n\n\n\nEn cours de traduction.\n\n\n\n4 - Jeux de données\nTout au long du livre, des ensembles de données réels disponibles gratuitement en ligne seront exploités par les différentes modèles et techniques présentés. Nous utiliserons différents ensembles de données pour différentes tâches : Classification, régression et classification de textes.\n\n\n\n\n Retour au sommet",
    "crumbs": [
      "4 - Jeux de données"
    ]
  },
  {
    "objectID": "02-introduction/02.2-ml_definitions.html",
    "href": "02-introduction/02.2-ml_definitions.html",
    "title": "Apprentissage automatique interprétable",
    "section": "",
    "text": "Version original (en)\n\n\n\nhttps://christophm.github.io/interpretable-ml-book/what-is-machine-learning.html",
    "crumbs": [
      "2 - Introduction",
      "2.2 - Qu'est-ce que l'apprentissage automatique ?"
    ]
  },
  {
    "objectID": "02-introduction/02.2-ml_definitions.html#quest-ce-que-lapprentissage-automatique",
    "href": "02-introduction/02.2-ml_definitions.html#quest-ce-que-lapprentissage-automatique",
    "title": "Apprentissage automatique interprétable",
    "section": "2.2 - Qu’est-ce que l’apprentissage automatique ?",
    "text": "2.2 - Qu’est-ce que l’apprentissage automatique ?\nL’apprentissage automatique est un ensemble de méthodes que les ordinateurs utilisent pour produire et améliorer des prédictions ou des comportements basés sur des données.\nPar exemple, pour prédire la valeur d’une maison, l’ordinateur peut apprendre des motifs à partir de ventes de maisons passées. Le livre se concentre sur l’apprentissage automatique supervisé, qui couvre tous les problèmes de prédiction où nous disposons d’un ensemble de données pour lequel nous connaissons déjà le résultat qui nous intéresse (par exemple, les prix des maisons passées) et souhaitons apprendre à prédire le résultat pour de nouvelles données. Sont excluses de l’apprentissage supervisé, par exemple, les tâches de regroupement (exemple d’apprentissage non supervisé) où nous n’avons pas de résultat spécifique cible, mais nous voulons trouver des regroupements de points de données. Sont également excluses des techniques comme l’apprentissage par renforcement, où un agent apprend à optimiser une certaine récompense en agissant dans un environnement (par exemple, un ordinateur jouant à Tetris). Le but de l’apprentissage supervisé est d’apprendre un modèle prédictif qui relie les caractéristiques des données (par exemple, la taille de la maison, son emplacement, le type de sol, …) à une sortie (par exemple, le prix de la maison). Si la sortie est la prédiction d’une catégorie, la tâche est appelée classification, et si elle est numérique, elle est appelée régression. L’algorithme d’apprentissage automatique apprend un modèle en estimant des paramètres (comme les poids) ou en apprenant des structures (comme les arbres). L’algorithme est guidé par une fonction de score ou de perte qui est minimisée. Dans l’exemple de la valeur de la maison, l’algorithme minimise la différence entre le prix estimé de la maison et le prix qu’il prédit. Un modèle d’apprentissage automatique entièrement formé peut alors être utilisé pour faire des prédictions avec de nouvelles observations. \nL’estimation des prix des maisons, les recommandations de produits, la détection des panneaux de signalisation, la prédiction de défaut de crédit et la détection de fraude : tous ces problèmes ont en commun qu’ils peuvent être résolus par l’apprentissage automatique. Les tâches sont différentes, mais l’approche est la même :\n\nÉtape 1 : Collecter de données. Plus il y en a, mieux c’est. Les données doivent contenir le résultat que vous souhaitez prédire et des informations supplémentaires à partir desquelles effectuer la prédiction. Pour un détecteur de panneaux de signalisation (“Y a-t-il un panneau de signalisation dans l’image ?”), vous collecteriez des images de rues et indiqueriez si un panneau de signalisation est visible ou non. Pour un prédicteur de défaut de crédit, vous auriez besoin de données passées sur des prêts réels, d’informations sur le fait que les clients étaient en défaut de paiement avec leurs prêts, et de données qui vous aideraient à faire des prédictions, telles que le revenu, les défauts de crédit passés, et ainsi de suite. Pour un programme automatique d’estimation de la valeur des maisons, vous pourriez collecter des données de ventes de maisons passées et des informations sur l’immobilier telles que la taille, l’emplacement, etc.\nÉtape 2 : Entrer ces informations dans un algorithme d’apprentissage automatique qui génère un modèle de détecteur de panneaux, un modèle d’évaluation de demande de crédit ou un estimateur de valeur de maison.\nÉtape 3 : Utiliser le modèle avec de nouvelles données. Intégrer le modèle dans un produit ou un processus, tel qu’une voiture autonome, un processus de demande de crédit ou un site Web de marché immobilier.\n\nLes machines surpassent les humains dans de nombreuses tâches, comme jouer aux échecs (ou plus récemment au Go) ou prédire la météo. Même si la machine est aussi bonne qu’un humain ou un peu moins bonne dans une tâche, elle dispose de grands avantages en termes de vitesse, de reproductibilité et d’échelle. Un modèle d’apprentissage automatique une fois mis en oeuvre peut accomplir une tâche beaucoup plus rapidement que les humains, fournir de manière fiable des résultats cohérents et peut être copié à l’infini. Répliquer un modèle d’apprentissage automatique sur une autre machine est rapide et bon marché. La formation d’un humain pour une tâche peut prendre des décennies (surtout lorsqu’ils sont jeunes) et est très coûteuse.\nUn inconvénient majeur de l’utilisation de l’apprentissage automatique est que les informations sur les données et la tâche que la machine résout sont cachées dans des modèles de plus en plus complexes. Vous avez besoin de millions de nombres pour décrire un réseau neuronal profond, et il n’y a aucun moyen de comprendre le modèle dans son intégralité. D’autres modèles, comme la forêt aléatoire, se composent de centaines d’arbres de décision qui “votent” pour les prédictions. Pour comprendre comment la décision a été prise, vous devez regarder dans les votes et les structures de chacun des centaines d’arbres. Cela ne fonctionne tout simplement pas, peu importe votre intelligence ou la qualité de votre mémoire de travail. Les modèles les plus performants sont souvent des mélanges de plusieurs modèles (également appelés ensembles) qui ne peuvent pas être interprétés, même si chaque modèle individuel peut l’être. Si vous vous concentrez uniquement sur la performance, vous obtiendrez automatiquement des modèles de plus en plus opaques. Les modèles gagnants dans les compétitions d’apprentissage automatique sont souvent des ensembles de modèles ou des modèles très complexes tels que les arbres boostés ou les réseaux neuronaux profonds.",
    "crumbs": [
      "2 - Introduction",
      "2.2 - Qu'est-ce que l'apprentissage automatique ?"
    ]
  },
  {
    "objectID": "01-preface/index.html",
    "href": "01-preface/index.html",
    "title": "1 - Préface de l’auteur",
    "section": "",
    "text": "Version original (en)\n\n\n\nhttps://christophm.github.io/interpretable-ml-book/preface-by-the-author.html\n\n\n\n1 - Préface de l’auteur\nCe livre a commencé comme un projet secondaire lorsque je travaillais comme statisticien dans la recherche clinique. Je travaillais quatre jours par semaine, et pendant mon “jour de congé”, je travaillais sur des projets annexes. Finalement, l’apprentissage automatique interprétable est devenu l’un de mes projets annexes. Au début, je n’avais pas l’intention d’écrire un livre. J’étais simplement intéressé d’en savoir plus sur l’apprentissage automatique interprétable et je cherchais de bonnes ressources pour apprendre. Étant donné le succès de l’apprentissage automatique et l’importance de l’interprétabilité, je m’attendais à ce qu’il y ait des tonnes de livres et de tutoriels sur ce sujet. Mais je n’ai trouvé que les articles de recherche pertinents et quelques publications de blogs éparpillés sur Internet, et rien qui n’offre une bonne vue d’ensemble. Pas de livre, pas de tutoriel, pas d’article de synthèse, rien. Cette carence m’a inspiré à commencer à écrire ce livre. J’ai fini par écrire le livre que j’aurais aimé avoir à disposition lorsque j’ai commencé mes études sur l’apprentissage automatique interprétable. Mon intention avec ce livre était double : apprendre pour moi-même et partager ces nouvelles connaissances avec les autres.\n\nJ’ai obtenu mon baccalauréat et ma maîtrise en statistiques à l’Université Ludwig-Maximilian de Munich, en Allemagne. La plupart de mes connaissances en apprentissage automatique ont été acquises en autodidacte à travers des cours en ligne, des compétitions, des projets annexes et des activités professionnelles. Mon parcours en statistiques a été une excellente base pour me lancer dans l’apprentissage automatique, et surtout dans l’interprétabilité. En statistiques, l’accent est mis sur la création de modèles de régression interprétables. Après avoir terminé ma maîtrise en statistiques, j’ai décidé de ne pas poursuivre un doctorat, car je n’avais pas apprécié la phase de rédaction de mon mémoire de maîtrise. Écrire me stressait tout simplement trop. J’ai donc pris des emplois de scientifique des données dans une start-up de Fintech et de statisticien dans la recherche clinique. Après ces trois années dans l’industrie, j’ai commencé à écrire ce livre et quelques mois plus tard, j’ai commencé un doctorat en apprentissage automatique interprétable. En travaillant sur ce livre, j’ai redécouvert le plaisir d’écrire et cela m’a aidé à développer une passion pour la recherche.\nCe livre couvre de nombreuses techniques d’apprentissage automatique interprétable. Dans les premiers chapitres, j’introduis le concept d’interprétabilité et j’explique pourquoi l’interprétabilité est nécessaire. Il y a même quelques courtes histoires ! Le livre discute des différentes propriétés d’une explication et de ce que les humains considèrent être une bonne explication. Nous discuterons ensuite des modèles d’apprentissage automatique qui sont intrinsèquement interprétables, par exemple les modèles de régression et les arbres de décision. Le principal sujet de ce livre porte sur les méthodes d’interprétabilité agnostiques au modèle. Agnostique au modèle signifie que ces méthodes peuvent être appliquées à n’importe quel modèle d’apprentissage automatique et qu’elles ne sont appliquées qu’après que le modèle ait été entraîné. Cette indépendance vis-à-vis du modèle rend les méthodes agnostiques très flexibles et puissantes. Certaines techniques expliquent comment les prédictions individuelles ont été produites, comme les explications locales interprétables agnostiques au modèle (LIME) et les valeurs de Shapley. D’autres techniques décrivent le comportement moyen du modèle à travers un ensemble de données. Nous apprendrons dans ce livre le graphique de dépendance partielle, les effets locaux cumulés, l’importance des caractéristiques par permutation et de nombreuses autres méthodes. Une catégorie spéciale concerne les méthodes basées sur des exemples qui produisent des points de données comme explications. Les explications contrefactuelles, les prototypes, les instances influentes et les exemples adverses sont des méthodes basées sur des exemples, qui sont discutées dans ce livre. Celui-ci se conclut par quelques réflexions sur ce que pourrait être l’avenir de l’apprentissage automatique interprétable.\nVous n’avez pas à lire le livre du début à la fin, vous pouvez aller et venir et vous concentrer sur les techniques qui vous intéressent le plus. Je recommande seulement que vous commenciez par l’introduction suivi du chapitre sur l’interprétabilité. La plupart des chapitres suivent une structure similaire et se concentrent sur une méthode d’interprétation. Le premier paragraphe résume la méthode. Ensuite, j’essaie de l’expliquer de manière intuitive sans me fier aux formules mathématiques. Puis nous examinons sa théorie afin d’obtenir une compréhension approfondie de son fonctionnement. Vous ne serez pas épargné durant cette étape, car la théorie contiendra des formules. Je crois qu’une nouvelle méthode est mieux comprise à l’aide d’exemples. Par conséquent, chaque méthode est appliquée à des données réelles. Certaines personnes disent que les statisticiens sont des gens très critiques. Pour moi, c’est vrai, car chaque chapitre contient des discussions critiques sur les avantages et les inconvénients de la méthode d’interprétation présentée. Ce livre n’est pas une publicité pour telle ou telle méthode, mais il devrait vous aider à décider si une méthode fonctionne bien pour votre application ou non. Dans la dernière section de chaque chapitre, des implémentations logicielles disponibles sont décrites.\nL’apprentissage automatique a reçu une grande attention de la part de nombreuses personnes dans la recherche et l’industrie. Parfois, l’apprentissage automatique est surestimé dans les médias, mais il existe de nombreuses applications réelles et impactantes. L’apprentissage automatique est une technologie puissante pour les produits, la recherche et l’automatisation. Aujourd’hui, l’apprentissage automatique est utilisé, par exemple, pour détecter des transactions financières frauduleuses, recommander des films et classifier des images. Il est souvent crucial que les modèles d’apprentissage automatique soient interprétables. L’interprétabilité aide les développeurs dans les phases de débogage et d’amélioration, construit la confiance dans le modèle, justifie les prédictions de ce dernier et mène à de nouvelles connaissances. Le besoin croissant d’interprétabilité de l’apprentissage automatique est une conséquence naturelle d’une utilisation accrue de l’apprentissage automatique. Ce livre est devenu une ressource précieuse pour de nombreuses personnes. Les instructeurs l’utilisent pour introduire auprès des étudiants les concepts de l’apprentissage automatique interprétable. J’ai reçu des e-mails de plusieurs étudiants de maîtrise et de doctorat qui m’ont dit que ce livre était le point de départ et la référence la plus importante pour leurs thèses. Il a aidé les chercheurs impliqués dans les domaines de l’écologie, de la finance, de la psychologie, etc., qui utilisent l’apprentissage automatique pour comprendre leurs données. Des scientifiques des données de l’industrie m’ont dit qu’ils utilisaient le livre “Apprentissage Machine Interprétable” pour leur travail et le recommandaient à leurs collègues. Je suis heureux que de nombreuses personnes en aient bénéficié et soient devenues des experts en interprétation de modèles.\nJe recommanderais ce livre aux praticiens qui souhaitent un aperçu des techniques leur permettant de rendre leurs modèles d’apprentissage automatique plus interprétables. Il pourrait être également bénéfique aux étudiants et aux chercheurs (et toute autre personne) qui s’intéressent à ce sujet. Afin de tirer le meilleur parti de ce livre, il est recommandé de disposer d’une compréhension de base de l’apprentissage automatique, et également d’avoir des connaissances en mathématiques de niveau universitaire pour pouvoir interpréter la théorie et les formules présentées dans ce livre. Il devrait être possible, cependant, de comprendre la description intuitive de la méthode au début de chaque chapitre sans les mathématiques.\nJ’espère que vous apprécierez de livre!\n\n\n\n\n Retour au sommet",
    "crumbs": [
      "1 - Préface de l'auteur"
    ]
  },
  {
    "objectID": "Formulaire/index.html",
    "href": "Formulaire/index.html",
    "title": "Des remarques ?",
    "section": "",
    "text": "Loading…\n\n\n\n\n Retour au sommet",
    "crumbs": [
      "Des remarques ?"
    ]
  },
  {
    "objectID": "05-interpretable_models/index.html",
    "href": "05-interpretable_models/index.html",
    "title": "5 - Modèles interprétables",
    "section": "",
    "text": "5 - Modèles interprétables\nLa façon la plus simple d’atteindre l’interprétabilité est d’utiliser uniquement un sous-ensemble d’algorithmes qui créent des modèles interprétables. La régression linéaire, la régression logistique et l’arbre de décision sont des modèles couramment utilisés pour leur interprétabilité.\nDans les chapitres suivants, nous parlerons de ces modèles. Pas en détail, seulement les bases, car il existe déjà une tonne de livres, vidéos, tutoriels, articles et d’autres matériaux disponibles. Nous nous concentrerons sur comment interpréter les modèles. Le livre discute en détail de la régression linéaire, de la régression logistique, d’autres extensions de la régression linéaire, des arbres de décision, des règles de décision et de l’algorithme RuleFit. Il liste également d’autres modèles interprétables.\nTous les modèles interprétables présentés dans ce livre sont interprétables à un niveau modulaire, à l’exception de la méthode des k plus proches voisins. Le tableau suivant donne un aperçu des types de modèles interprétables et de leurs propriétés. Un modèle est linéaire si l’association entre les caractéristiques et la cible est modélisée de manière linéaire. Un modèle avec des contraintes de monotonie assure que la relation entre une caractéristique et le résultat cible va toujours dans la même direction sur toute la gamme de la caractéristique : une augmentation de la valeur de la caractéristique entraîne toujours une augmentation ou toujours une diminution du résultat cible. La monotonie est utile pour l’interprétation d’un modèle car elle rend une relation plus facile à comprendre. Certains modèles peuvent inclure automatiquement des interactions entre les caractéristiques pour prédire le résultat cible. Vous pouvez inclure des interactions dans n’importe quel type de modèle en créant manuellement des caractéristiques d’interaction. Les interactions peuvent améliorer la performance prédictive, mais trop d’interactions ou des interactions trop complexes peuvent nuire à l’interprétabilité. Certains modèles gèrent uniquement la régression, d’autres uniquement la classification, et encore d’autres les deux.\nÀ partir de ce tableau, vous pouvez sélectionner un modèle interprétable adapté à votre tâche, que ce soit pour de la régression (regr) ou de la classification (class) :\n\n\n\nAlgorithme\nLinéaire\nMonotone\nInteraction\nTâche\n\n\n\n\nRégression linéaire\nOui\nOui\nNon\nregr\n\n\nRégression logistique\nNon\nOui\nNon\nclass\n\n\nArbres de décision\nNon\nQuelques-uns\nOui\nclass, regr\n\n\nRuleFit\nOui\nNon\nOui\nclass, regr\n\n\nNaive Bayes\nNon\nOui\nNon\nclass\n\n\nk plus proches voisins\nNon\nNon\nNon\nclass, regr\n\n\n\nOn pourrait arguer que la régression logistique et le Naive Bayes permettent des explications linéaires. Cependant, cela est vrai uniquement pour le logarithme de la cible : augmenter une caractéristique d’un point augmente le logarithme de la probabilité de la cible d’une certaine quantité, en supposant que toutes les autres caractéristiques restent identiques.\n\n\n\n\n Retour au sommet",
    "crumbs": [
      "5 - Modèles interprétables"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Résumé",
    "section": "",
    "text": "Traduction du livre en ligne https://christophm.github.io/interpretable-ml-book/index.html Dernière mise à jour de la version originale : 31/07/2024\n\n\n\n\nCouverture\n\n\nL’apprentissage automatique représente un grand potentiel pour améliorer les produits, les processus et la recherche. Mais les ordinateurs n’expliquent généralement pas leurs prédictions, ce qui est un obstacle à l’adoption de l’apprentissage automatique. Ce livre traite de ce qui rend les modèles d’apprentissage automatique et leurs prédictions interprétables.\nAprès avoir exploré les concepts d’interprétabilité, vous découvrirez des modèles simples et interprétables tels que les arbres de décision, les règles de décision et la régression linéaire. Le livre met l’accent sur les méthodes agnostiques aux modèles pour interpréter les boîtes noires que sont ces modèles, tels que l’importance des caractéristiques et les effets locaux accumulés, et pour expliquer les prédictions individuelles avec les valeurs de Shapley et LIME. De plus, le livre présente des méthodes spécifiques aux réseaux de neurones profonds.\nToutes les méthodes d’interprétation sont expliquées en profondeur et discutées de manière critique. Comment fonctionnent-elles sous le capot ? Quelles sont leurs forces et leurs faiblesses ? Comment leurs résultats peuvent-ils être interprétés ? Ce livre vous permettra de sélectionner et d’appliquer correctement la méthode d’interprétation la plus adaptée à votre projet d’apprentissage automatique. La lecture du livre est recommandée aux praticiens de l’apprentissage automatique, aux experts en science des données, aux statisticiens et à toute autre personne intéressée à rendre des modèles d’apprentissage automatique interprétables.\nVous pouvez acheter les versions PDF ou e-book (epub, mobi) sur leanpub.com.\nVous pouvez acheter la version imprimée sur Amazon.\nAu sujet de l’auteur: Il s’appelle Christoph Molnar, il est statisticien et spécialiste en apprentissage machine. Son but est de rendre l’apprentissage machine interprétable.\nSuivez l’auteur sur X(ex-Twitter)! @ChristophMolnar\nCouverture de @YvonneDoinel\n\n\n\nlicence BY-NC-SA\n\n\nConsultez également Modeling Mindsets, son second livre.\nCe livre est publié sous la licence Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License.\n\n\n\n\n\n\nNote du traducteur\n\n\n\nAu sujet du traducteur (assisté de ChatGPT-4) : Mon nom est Nicolas Guillard. De formation initiale Ingénieur en informatique, préalablement Analytiste Digital, je suis maintenant Ingénieur en Intelligence Artificielle. Dernière traduction réalisée entre les 27/12/2023 et 15/05/2024.\n\n\n\n\n\n Retour au sommet"
  },
  {
    "objectID": "02-introduction/02.1-short_stories.html",
    "href": "02-introduction/02.1-short_stories.html",
    "title": "Apprentissage automatique interprétable",
    "section": "",
    "text": "Version original (en)\n\n\n\nhttps://christophm.github.io/interpretable-ml-book/storytime.html",
    "crumbs": [
      "2 - Introduction",
      "2.1 - Quelques histoires"
    ]
  },
  {
    "objectID": "02-introduction/02.1-short_stories.html#sec-short_stories",
    "href": "02-introduction/02.1-short_stories.html#sec-short_stories",
    "title": "Apprentissage automatique interprétable",
    "section": "2.1 - Quelques histoires",
    "text": "2.1 - Quelques histoires\nNous commencerons avec quelques courtes histoires. Chaque histoire est un appel volontairement exagéré à un apprentissage automatique interprétable. Si vous êtes pressé, vous pouvez les passer. Si vous voulez être diverti et (dé)motivé, poursuivez leur lecture !\nLe format est inspiré par les “Tech Tales” de Jack Clark dans sa Newsletter Import AI. Si vous aimez ce genre d’histoires ou si vous vous intéressez à l’IA, je vous recommande de vous y inscrire.\n\n2.1.1 - La foudre ne frappe jamais deux fois\n2030 : Un laboratoire médical en Suisse\n\n\n\nhôpital\n\n\n– “Il n’y a certainement pas pire façon de mourir !”, résuma Tom, essayant de trouver quelque chose de positif dans la tragédie. Il retira la pompe de la perfusion. – “Il est juste mort pour de mauvaises raisons,” ajouta Lena. – “Et certainement avec la mauvaise pompe à morphine ! Cela crée juste plus de travail pour nous !” se plaignit Tom en dévissant le panneau arrière de la pompe. Après avoir retiré toutes les vis, il souleva le panneau et le mit de côté. Il brancha un câble sur le port de diagnostic. – “Tu ne te plaignais pas d’avoir un travail, n’est-ce pas ?” Lena lui lança un sourire moqueur. – “Bien sûr que non. Jamais !” s’exclama-t-il avec un ton sarcastique.\nIl démarra l’ordinateur de la pompe. Lena brancha l’autre extrémité du câble à sa tablette. – “Bon, le diagnostic est en cours,” annonça-t-elle. “Je suis vraiment curieuse de savoir ce qui a mal tourné.” – “Ça a certainement envoyé notre John Doe au Nirvana. Cette haute concentration de ce truc à la morphine. Mec. Je veux dire… c’est une première, non ? Normalement, une pompe cassée donne trop peu de la douce substance ou rien du tout. Mais jamais, tu sais, comme cette folle injection”, expliqua Tom. – “Je sais. Tu n’as pas à me convaincre… Hey, regarde ça.” Lena leva sa tablette. “Tu vois ce pic ici ? C’est la puissance du mélange d’antidouleurs. Regarde ! Cette ligne montre le niveau de référence. Le pauvre gars avait un mélange d’antidouleurs dans son système sanguin qui aurait pu le tuer 17 fois. Injecté par notre pompe ici. Et ici…” Elle fit défiler, “ici tu peux voir le moment de la mort du patient.” – “Alors, une idée de ce qui s’est passé, chef ?” demanda Tom à son superviseur. – “Hm… Les capteurs semblent être bons. Rythme cardiaque, niveaux d’oxygène, glucose,… Les données ont été collectées comme prévu. Quelques valeurs manquantes dans les données d’oxygène sanguin, mais ce n’est pas inhabituel. Regarde ici. Les capteurs ont également détecté le ralentissement du rythme cardiaque du patient et des niveaux extrêmement bas de cortisol causés par le dérivé de morphine et d’autres agents bloquant la douleur.” Elle continua de faire défiler le rapport de diagnostic. Tom était captivé par l’écran. C’était sa première enquête sur une véritable défaillance d’appareil.\n– “Ok, voici notre premier élément du puzzle. Le système a échoué à envoyer un avertissement au canal de communication de l’hôpital. L’avertissement a été déclenché, mais rejeté au niveau du protocole. Cela pourrait être de notre faute, mais cela pourrait aussi être la faute de l’hôpital. S’il te plaît, envoie les journaux à l’équipe informatique,” dit Lena à Tom. Tom acquiesça, les yeux toujours fixés sur l’écran. Lena continua :  – “C’est étrange. L’avertissement aurait également dû provoquer l’arrêt de la pompe. Mais il a manifestement échoué à le faire. Cela doit être un bug. Quelque chose que l’équipe qualité a manqué. Quelque chose de vraiment mauvais. Peut-être que c’est lié au problème de protocole.” – “Donc, le système d’urgence de la pompe est d’une manière ou d’une autre tombé en panne, mais pourquoi la pompe s’est-elle emballée et a injecté autant d’analgésiques dans John Doe ?” se demanda Tom. – “Bonne question. Tu as raison. Même en cas de défaillance de l’urgence du protocole, la pompe n’aurait pas dû administrer autant de médicaments. L’algorithme aurait dû s’arrêter bien plus tôt de lui-même, étant donné le faible niveau de cortisol et d’autres signes d’alerte,” expliqua Lena. – “Peut-être un coup de malchance, comme une chose sur un million, comme être frappé par la foudre ?” demanda Tom. – “Non, Tom. Si tu avais lu la documentation que je t’ai envoyée, tu aurais su que la pompe a d’abord été entraînée dans des expériences sur des animaux, puis plus tard sur des humains, pour apprendre à injecter la quantité parfaite d’antidouleurs en fonction des entrées sensorielles. L’algorithme de la pompe peut être opaque et complexe, mais il n’est pas aléatoire. Cela signifie que dans la même situation, la pompe se comporterait exactement de la même manière à nouveau. Notre patient mourrait à nouveau. Une combinaison ou une interaction indésirable des entrées sensorielles doit avoir déclenché le comportement erroné de la pompe. C’est pourquoi nous devons creuser plus profondément et découvrir ce qui s’est passé ici”, expliqua Lena.\n– “Je vois…”, répondit Tom, perdu dans ses pensées. “Le patient n’allait-il pas mourir bientôt de toute façon ? À cause du cancer ou de quelque chose ?” Lena hocha la tête en lisant le rapport d’analyse. Tom se leva et alla à la fenêtre. Il regarda dehors, les yeux fixés sur un point au loin. – “Peut-être que la machine lui a rendu service, tu sais, en le libérant de la douleur. Plus de souffrance. Peut-être qu’elle a juste fait ce qu’il fallait. Comme la foudre, mais, tu sais, une bonne. Je veux dire comme la loterie, mais pas aléatoire. Mais pour une raison. Si j’étais la pompe, j’aurais fait la même chose.” Elle leva finalement la tête et le regarda. Il continuait de observer quelque chose à l’extérieur. Tous deux restèrent silencieux quelques instants. Lena baissa à nouveau la tête et continua l’analyse. – “Non, Tom. C’est un bug… Juste un sacré bug.”\n\n\n2.1.2 - Perte de confiance\n2050 : Une station de métro à Singapour\n\n\n\nAccès refusé\n\n\nElle se précipita vers la station de métro Bishan. Ses pensées étaient déjà au travail. Les tests pour la nouvelle architecture neuronale devraient être terminés maintenant. Elle dirigeait la refonte du “Système de Prédiction de l’Affinité Fiscale pour les Entités Individuelles” du gouvernement, qui prédit si une personne cachera de l’argent au bureau des impôts. Son équipe a conçu une pièce d’ingénierie élégante. S’il est un succès, le système servirait non seulement à l’administration fiscale, mais il alimenterait également d’autres systèmes tels que le système d’alerte antiterroriste et le registre commercial. Un jour, le gouvernement pourrait même intégrer les prédictions dans le Score de Confiance Civique. Le Score de Confiance Civique estime la fiabilité d’une personne. Cette estimation affecte chaque aspect de votre vie quotidienne, comme obtenir un prêt ou le délai d’attente pour un nouveau passeport. Alors qu’elle descendait l’escalator, elle imaginait à quoi pourrait ressembler l’intégration du système de son équipe dans le Système de Score de Confiance Civique.\nElle passa machinalement sa main sur le lecteur RFID sans réduire sa vitesse de marche. Son esprit était occupé, mais une dissonance entre ses prédictions sensorielles et la réalité tira l’alarme dans son cerveau.\nTrop tard.\nElle se cogna le nez contre le portillon d’entrée du métro et tomba sur les fesses au sol. La porte aurait dû s’ouvrir, … mais elle ne l’a pas fait. Abasourdie, elle se leva et regarda l’écran à côté du portillon. – “Veuillez réessayer plus tard,” suggéra un smiley à l’air sympathique sur l’écran. Une personne passa à côté d’elle et, sans lui prêter attention, passa sa main sur le lecteur. La porte s’ouvrit et elle passa. La porte se referma. Elle essuya son nez. Ça faisait mal, mais au moins ça ne saignait pas. Elle tenta d’ouvrir la porte, mais fut à nouveau rejetée. C’était étrange. Peut-être que son compte de transport public n’avait pas assez de jetons. Elle regarda sa montre intelligente pour vérifier le solde du compte.\n– “Connexion refusée. Veuillez contacter votre Bureau de Conseil aux Citoyens !” l’informa sa montre.\nUn sentiment de nausée la frappa comme un coup de poing dans l’estomac. Elle soupçonnait ce qui s’était passé. Pour confirmer sa théorie, elle lança le jeu mobile “Guilde des tireurs d’élite”, un jeu de tir à la première personne. L’application se ferma automatiquement, ce qui confirma sa théorie. Elle devint étourdie et s’assit à nouveau sur le sol.\nIl n’y avait qu’une seule explication possible : son Score de Confiance Civique avait chuté. Substantiellement. Une petite baisse signifiait des désagréments mineurs, comme ne pas obtenir de vols en première classe ou devoir attendre un peu plus longtemps pour les documents officiels. Un score de confiance faible était rare et signifiait que vous étiez classé comme une menace pour la société. Une mesure pour traiter ces personnes était de les éloigner des lieux publics tels que le métro. Le gouvernement restreignait les transactions financières des sujets avec de faibles Scores de Confiance Civique. Ils commençaient également à surveiller activement votre comportement sur les réseaux sociaux et allaient même jusqu’à restreindre certains contenus, comme les jeux violents. Il devenait exponentiellement plus difficile d’augmenter votre Score de Confiance Civique qu’il était bas. Les personnes avec un score très faible ne se rétablissaient généralement jamais.\nElle ne pouvait imaginer aucune raison pour laquelle son score avait dû chuter. Le score était basé sur l’apprentissage automatique. Le Système de Score de Confiance Civique fonctionnait comme un moteur bien huilé qui réglait la société. La performance du Système de Score de Confiance était toujours étroitement surveillée. L’apprentissage automatique était devenu bien meilleur depuis le début du siècle. Il était devenu si efficace que les décisions prises par le Système de Score de Confiance ne pouvaient plus être contestées. Un système infaillible.\nElle rit dans le désespoir. Système infaillible. Si seulement. Le système a rarement échoué. Mais il a échoué. Elle doit être l’un de ces cas spéciaux; une erreur du système; désormais une paria. Personne n’osait remettre en question le système. Il était trop intégré dans le gouvernement, dans la société elle-même, pour être remis en question. Dans les quelques pays démocratiques restants, il était interdit de former des mouvements antidémocratiques, non parce qu’ils étaient intrinsèquement malveillants, mais parce qu’ils déstabiliseraient le système actuel. La même logique s’appliquait aux algocraties désormais plus courantes. La critique des algorithmes était interdite en raison du danger pour le statu quo.\nLa confiance algorithmique était le tissu de l’ordre social. Pour le bien commun, les fausses notations de confiance rares étaient tacitement acceptées. Des centaines d’autres systèmes de prédiction et de bases de données alimentaient le score, rendant impossible de savoir ce qui avait causé la chute de son score. Elle se sentait comme si un grand trou noir s’ouvrait en elle et sous elle. Avec horreur, elle regarda dans le vide.\nSon système d’affinité fiscale a finalement été intégré dans le Système de Score de Confiance Civique, mais elle n’a jamais pu le savoir.\n\n\n2.1.3 - Les trombones de Fermi\nAnnée 612 ACM (Après Colonisation de Mars) : Un musée sur Mars\n\n\n\nTerre brûlée\n\n\n– “L’histoire, c’est ennuyeux,” murmura Xola à son amie. Xola, une fille aux cheveux bleus, chassait paresseusement l’un des drones projecteurs bourdonnant dans la salle avec sa main gauche. – “L’histoire est importante,” dit le professeur d’une voix contrariée en regardant les filles. Xola rougit. Elle ne s’attendait pas à ce que son professeur l’entende.\n– “Xola, qu’as-tu appris ?” demanda le professeur. – “Que les anciens ont épuisé toutes les ressources de la planète Terrienne et alors sont morts ?” demanda-t-elle prudemment. – “Non. Ils ont rendu le climat chaud et ce n’étaient pas les gens, c’étaient les ordinateurs et les machines. Et c’est la planète Terre, pas la planète Terrienne”, ajouta une autre fille nommée Lin. Xola acquiesça. Avec un soupçon de fierté, le professeur sourit et acquiesça. – “Vous avez toutes les deux raison. Savez-vous pourquoi cela s’est passé?” – “Parce que les gens étaient à courte vue et avides ?” demanda Xola. – “Les gens ne pouvaient pas arrêter leurs machines !” s’exclama Lin.\n– “Encore une fois, vous avez toutes les deux raison,” décida le professeur, “mais c’est bien plus compliqué que cela. La plupart des gens à l’époque n’étaient pas conscients de ce qui se passait. Certains ont vu les changements drastiques, mais ne pouvaient pas les inverser. Le morceau le plus célèbre de cette période est un poème d’un auteur anonyme. Il capture le mieux ce qui s’est passé à cette époque. Écoutez bien !”\nLe professeur débuta le poème. Une douzaine de petits drones se reposionnèrent devant les enfants et projetèrent la vidéo directement dans leurs yeux. Elle montrait une personne en costume debout dans une forêt constituée seulement de souches d’arbres. Il commença à parler :\nLes machines calculent; les machines prédisent.\nNous marchons comme si nous en faisions partie.\nNous cherchons un optimum comme entrainement.\nL’optimum est unidimensionnel, local et non contraint.\nSilicium et chair, à la poursuite de l’exponentialité.\nLa croissance est notre mentalité.\nQuand toutes les récompenses sont collectées,\net les effets secondaires négligés ;\nQuand toutes les pièces sont minées,\net que la nature est laissée pour compte;\nNous serons en difficulté,\nAprès tout, la croissance exponentielle est une bulle.\nLa tragédie des communs se déroule,\nExplose,\nDevant nos yeux.\nCalculs froids et cupidité glaciale,\nRemplissent la terre de chaleur.\nTout meurt,\nEt nous nous conformons.\nComme des chevaux avec des oeillères, nous courons la course de notre propre création,\nVers le Grand Filtre de la civilisation.\nEt ainsi nous marchons sans relâche.\nComme si nous faisions partie de la machine.\nEmbrassant l’entropie.\n– “Un sombre souvenir”, dit le professeur pour briser le silence dans la salle. “Il sera téléchargé dans votre bibliothèque. Votre devoir est de le mémoriser jusqu’à la semaine prochaine.” Xola soupira. Elle réussit à attraper l’un des petits drones. Le drone était chaud à cause du CPU et des moteurs. Xola aimait la façon dont il réchauffait ses mains.",
    "crumbs": [
      "2 - Introduction",
      "2.1 - Quelques histoires"
    ]
  },
  {
    "objectID": "03-interpretability/03.2-taxonomy_of_interpretability_methods.html",
    "href": "03-interpretability/03.2-taxonomy_of_interpretability_methods.html",
    "title": "Apprentissage automatique interprétable",
    "section": "",
    "text": "Avertissement\n\n\n\nEn cours de traduction.",
    "crumbs": [
      "3 - Interprétabilité",
      "3.2 - Taxonomie des Méthodes d'Interprétabilité"
    ]
  },
  {
    "objectID": "03-interpretability/03.2-taxonomy_of_interpretability_methods.html#taxonomie-des-méthodes-dinterprétabilité",
    "href": "03-interpretability/03.2-taxonomy_of_interpretability_methods.html#taxonomie-des-méthodes-dinterprétabilité",
    "title": "Apprentissage automatique interprétable",
    "section": "3.2 - Taxonomie des Méthodes d’Interprétabilité",
    "text": "3.2 - Taxonomie des Méthodes d’Interprétabilité\nLes méthodes pour l’interprétabilité de l’apprentissage automatique peuvent être classées selon différents critères.\nIntrinsèque ou a posteriori ? Ce critère distingue si l’interprétabilité est obtenue en limitant la complexité du modèle d’apprentissage automatique (intrinsèque) ou en appliquant des méthodes qui analysent le modèle après l’entraînement (a posteriori). L’interprétabilité intrinsèque fait référence aux modèles d’apprentissage automatique qui sont considérés comme interprétables en raison de leur structure simple, tels que les arbres de décision courts ou les modèles linéaires éparse. L’interprétabilité a posteriori fait référence à l’application de méthodes d’interprétation après l’entraînement du modèle. L’importance des caractéristiques par permutation est, par exemple, une méthode d’interprétation a posteriori. Les méthodes a posteriori peuvent également être appliquées à des modèles intrinsèquement interprétables. Par exemple, l’importance des caractéristiques par permutation peut être calculée pour les arbres de décision. L’organisation des chapitres de ce livre est déterminée par la distinction entre les modèles intrinsèquement interprétables et les méthodes d’interprétation a posteriori (et agnostiques au modèle).\nRésultat de la méthode d’interprétation Les différentes méthodes d’interprétation peuvent être grossièrement différenciées selon leurs résultats :\n\nStatistique résumée des caractéristiques : De nombreuses méthodes d’interprétation fournissent des statistiques résumées pour chaque caractéristique. Certaines méthodes renvoient un seul nombre par caractéristique, tel que l’importance des caractéristiques, ou un résultat plus complexe, tel que les forces d’interaction entre paires de caractéristiques, qui consistent en un nombre pour chaque paire de caractéristiques.\nVisualisation résumée des caractéristiques : La plupart des statistiques résumées des caractéristiques peuvent également être visualisées. Certaines synthèses de caractéristiques sont en fait uniquement significatives si elles sont visualisées et un tableau serait un mauvais choix. La dépendance partielle d’une caractéristique est un tel cas. Les graphiques de dépendance partielle sont des courbes qui montrent une caractéristique et le résultat moyen prédit. La meilleure façon de présenter les dépendances partielles est de dessiner réellement la courbe au lieu d’imprimer les coordonnées.\nInternes du modèle (par exemple, les poids appris) : L’interprétation des modèles intrinsèquement interprétables relève de cette catégorie. Des exemples en sont les poids dans les modèles linéaires ou la structure d’arbre apprise (les caractéristiques et les seuils utilisés pour les divisions) des arbres de décision. Les frontières sont floues entre les internes du modèle et la statistique résumée des caractéristiques, par exemple, dans les modèles linéaires, car les poids sont à la fois les internes du modèle et des statistiques résumées pour les caractéristiques en même temps. Une autre méthode qui produit des internes de modèle est la visualisation des détecteurs de caractéristiques appris dans les réseaux neuronaux convolutionnels. Les méthodes d’interprétabilité qui produisent des internes de modèle sont par définition spécifiques au modèle (voir le critère suivant).\nPoint de données : Cette catégorie comprend toutes les méthodes qui retournent des points de données (déjà existants ou nouvellement créés) pour rendre un modèle interprétable. Une méthode s’appelle les explications contrefactuelles. Pour expliquer la prédiction d’une instance de données, la méthode trouve un point de données similaire en changeant certaines des caractéristiques pour lesquelles le résultat prédit change de manière pertinente (par exemple, un changement dans la classe prédite). Un autre exemple est l’identification de prototypes des classes prédites. Pour être utiles, les méthodes d’interprétation qui produisent de nouveaux points de données nécessitent que les points de données eux-mêmes puissent être interprétés. Cela fonctionne bien pour les images et les textes, mais est moins utile pour les données tabulaires avec des centaines de caractéristiques.\nModèle intrinsèquement interprétable : Une solution pour interpréter les modèles boîte noire consiste à les approximer (soit globalement, soit localement) avec un modèle interprétable. Le modèle interprétable lui-même est interprété en examinant les paramètres internes du modèle ou les statistiques résumées des caractéristiques.\n\nSpécifique au modèle ou agnostique au modèle ? Les outils d’interprétation spécifiques à un modèle sont limités à des classes de modèles spécifiques. L’interprétation des poids de régression dans un modèle linéaire est une interprétation spécifique au modèle, puisque – par définition – l’interprétation des modèles intrinsèquement interprétables est toujours spécifique au modèle. Les outils qui fonctionnent uniquement pour l’interprétation, par exemple, des réseaux neuronaux sont spécifiques au modèle. Les outils agnostiques au modèle peuvent être utilisés sur n’importe quel modèle d’apprentissage automatique et sont appliqués après que le modèle ait été entraîné (a posteriori). Ces méthodes agnostiques fonctionnent généralement en analysant les paires d’entrée et de sortie de caractéristiques. Par définition, ces méthodes ne peuvent pas avoir accès aux internes du modèle tels que les poids ou les informations structurelles.\nLocal ou global ? La méthode d’interprétation explique-t-elle une prédiction individuelle ou le comportement global du modèle ? Ou est-ce que la portée se situe quelque part entre les deux ? Lisez plus sur le critère de portée dans la section suivante.",
    "crumbs": [
      "3 - Interprétabilité",
      "3.2 - Taxonomie des Méthodes d'Interprétabilité"
    ]
  },
  {
    "objectID": "05-interpretable_models/05.2-logistic-regression.html",
    "href": "05-interpretable_models/05.2-logistic-regression.html",
    "title": "Apprentissage automatique interprétable",
    "section": "",
    "text": "Avertissement\n\n\n\nEn cours de traduction.",
    "crumbs": [
      "5 - Modèles interprétables",
      "5.2 - Régéression logistique"
    ]
  },
  {
    "objectID": "05-interpretable_models/05.2-logistic-regression.html#régéression-logistique",
    "href": "05-interpretable_models/05.2-logistic-regression.html#régéression-logistique",
    "title": "Apprentissage automatique interprétable",
    "section": "5.2 - Régéression logistique",
    "text": "5.2 - Régéression logistique\nLa régression logistique modélise les probabilités pour les problèmes de classification avec deux issues possibles. C’est une extension du modèle de régression linéaire pour les problèmes de classification.\n\nVous cherchez simplement l’interprétation correcte des modèles de régression logistique ? Économisez-vous du temps et des maux de tête (les log odds, ça vous dit quelque chose ?) et consultez ma feuille de triche pour l’interprétation de la régression logistique.\n\n\n5.2.1 - Qu’est-ce qui ne va pas avec la régression linéaire pour la classification ?\nLe modèle de régression linéaire peut bien fonctionner pour la régression, mais échoue pour la classification. Pourquoi est-ce le cas ? Dans le cas de deux classes, vous pourriez étiqueter l’une des classes avec 0 et l’autre avec 1 et utiliser la régression linéaire. Techniquement, cela fonctionne et la plupart des programmes de modèle linéaire vous donneront des poids. Mais il y a quelques problèmes avec cette approche :\nUn modèle linéaire ne produit pas de probabilités, mais il traite les classes comme des nombres (0 et 1) et ajuste le meilleur hyperplan (pour une seule caractéristique, c’est une ligne) qui minimise les distances entre les points et l’hyperplan. Il interpole donc simplement entre les points, et vous ne pouvez pas l’interpréter comme des probabilités.\nUn modèle linéaire extrapole également et vous donne des valeurs en dessous de zéro et au-dessus de un. C’est un bon signe qu’il pourrait y avoir une approche plus intelligente pour la classification.\nÉtant donné que le résultat prédit n’est pas une probabilité, mais une interpolation linéaire entre les points, il n’y a pas de seuil significatif auquel vous pouvez distinguer une classe de l’autre. Une bonne illustration de ce problème a été donnée sur Stackoverflow.\nLes modèles linéaires ne s’étendent pas aux problèmes de classification avec plusieurs classes. Vous devriez commencer à étiqueter la classe suivante avec 2, puis 3, et ainsi de suite. Les classes pourraient ne pas avoir d’ordre significatif, mais le modèle linéaire forcerait une structure étrange sur la relation entre les caractéristiques et vos prédictions de classe. Plus la valeur d’une caractéristique avec un poids positif est élevée, plus elle contribue à la prédiction d’une classe avec un numéro plus élevé, même si les classes qui se trouvent avoir un numéro similaire ne sont pas plus proches que d’autres classes.\n\n\n\nA linear model classifies tumors as malignant (1) or benign (0) given their size. The lines show the prediction of the linear model. For the data on the left, we can use 0.5 as classification threshold. After introducing a few more malignant tumor cases, the regression line shifts and a threshold of 0.5 no longer separates the classes. Points are slightly jittered to reduce over-plotting.\n\n\n\n\n5.2.2 - Théorie\nUne solution pour la classification est la régression logistique. Au lieu d’ajuster une ligne droite ou un hyperplan, le modèle de régression logistique utilise la fonction logistique pour compresser la sortie d’une équation linéaire entre 0 et 1. La fonction logistique est définie comme :\n\\[\\text{logistic}(\\eta)=\\frac{1}{1+exp(-\\eta)}\\]\nEt cela ressemble à :\n\n\n\nThe logistic function. It outputs numbers between 0 and 1. At input 0, it outputs 0.5.\n\n\nLe passage de la régression linéaire à la régression logistique est assez direct. Dans le modèle de régression linéaire, nous avons modélisé la relation entre le résultat et les caractéristiques par une équation linéaire :\n\\[\\hat{y}^{(i)} = \\beta_{0} + \\beta_{1}x^{(i)}_{1} + \\ldots + \\beta_{p}x^{(i)}_{p}\\]\nPour la classification, nous préférons des probabilités entre 0 et 1, donc nous enveloppons le côté droit de l’équation dans la fonction logistique. Cela force la sortie à n’assumer que des valeurs entre 0 et 1.\n\\[P(y^{(i)} = 1) = \\frac{1}{1 + \\exp(-(\\beta_{0} + \\beta_{1}x^{(i)}_{1} + \\ldots + \\beta_{p}x^{(i)}_{p}))}\\]\nRevenons à l’exemple de la taille de la tumeur. Mais au lieu du modèle de régression linéaire, utilisons le modèle de régression logistique :\n\n\n\nThe logistic regression model finds the correct decision boundary between malignant and benign depending on tumor size. The line is the logistic function shifted and squeezed to fit the data.\n\n\nLa classification fonctionne mieux avec la régression logistique et nous pouvons utiliser 0,5 comme seuil dans les deux cas. L’ajout de points supplémentaires n’affecte pas vraiment la courbe estimée de manière significative.\n\n\n5.2.3 - Interpretation\nL’interprétation des poids dans la régression logistique diffère de celle des poids dans la régression linéaire, car le résultat dans la régression logistique est une probabilité comprise entre 0 et 1. Les poids n’influencent plus la probabilité de manière linéaire. La somme pondérée est transformée par la fonction logistique en une probabilité. Par conséquent, nous devons reformuler l’équation pour l’interprétation de sorte que seul le terme linéaire se trouve du côté droit de la formule.\n\\[ln\\left(\\frac{P(y=1)}{1-P(y=1)}\\right)=log\\left(\\frac{P(y=1)}{P(y=0)}\\right)=\\beta_{0}+\\beta_{1}x_{1}+\\ldots+\\beta_{p}x_{p}\\]\nNous appelons le terme dans la fonction ln() les “odds” (probabilité de l’événement divisée par la probabilité de non-événement) et enveloppé dans le logarithme, cela s’appelle log odds (logarithme des odds).\nCette formule montre que le modèle de régression logistique est un modèle linéaire pour les log odds. Super ! Cela ne semble pas utile ! Avec un peu de réarrangement des termes, vous pouvez déterminer comment la prédiction change lorsqu’une des caractéristiques (x_j) est modifiée d’une unité. Pour ce faire, nous pouvons d’abord appliquer la fonction exp() des deux côtés de l’équation :\n\\[\\frac{P(y=1)}{1-P(y=1)}=odds=exp\\left(\\beta_{0}+\\beta_{1}x_{1}+\\ldots+\\beta_{p}x_{p}\\right)\\]\nEnsuite, nous comparons ce qui se passe lorsque nous augmentons l’une des valeurs des caractéristiques de 1. Mais au lieu de regarder la différence, nous examinons le rapport des deux prédictions :\n\\[\\frac{odds_{x_j+1}}{odds{x_j}}=\\frac{exp\\left(\\beta_{0}+\\beta_{1}x_{1}+\\ldots+\\beta_{j}(x_{j}+1)+\\ldots+\\beta_{p}x_{p}\\right)}{exp\\left(\\beta_{0}+\\beta_{1}x_{1}+\\ldots+\\beta_{j}x_{j}+\\ldots+\\beta_{p}x_{p}\\right)}\\]\nNous appliquons la règle suivante :\n\\[\\frac{exp(a)}{exp(b)}=exp(a-b)\\]\nEt nous supprimons quelques termes :\n\\[\\frac{odds_{x_j+1}}{odds{x_j}}=exp\\left(\\beta_{j}(x_{j}+1)-\\beta_{j}x_{j}\\right)=exp\\left(\\beta_j\\right)\\]\n\n\n\n\n\n\nNote du traducteur\n\n\n\nRevoir les formules\n\n\nEn fin de compte, nous avons quelque chose d’aussi simple que \\(\\exp()\\) d’un poids de caractéristique. Un changement dans une caractéristique d’une unité change le rapport des odds (de manière multiplicative) par un facteur de \\(\\exp(\\beta_j)\\). Nous pourrions également l’interpréter de cette manière : Un changement dans \\(x_j\\) d’une unité augmente le rapport des log odds de la valeur du poids correspondant. La plupart des gens interprètent le rapport des odds parce que penser au ln() de quelque chose est connu pour être difficile pour le cerveau. Interpréter le rapport des odds nécessite déjà une certaine habitude. Par exemple, si vous avez des odds de 2, cela signifie que la probabilité pour y=1 est deux fois plus élevée que y=0. Si vous avez un poids (= rapport des log odds) de 0,7, alors augmenter la caractéristique respective d’une unité multiplie les odds par \\(\\exp(0.7)\\) (environ 2) et les odds changent à 4. Mais généralement, vous ne traitez pas avec les odds et interprétez les poids uniquement comme les rapports des odds. Car pour calculer réellement les odds, vous auriez besoin de définir une valeur pour chaque caractéristique, ce qui n’a de sens que si vous voulez regarder une instance spécifique de votre jeu de données.\nVoici les interprétations pour le modèle de régression logistique avec différents types de caractéristiques :\n\nCaractéristique numérique : Si vous augmentez la valeur de la caractéristique \\(x_{j}\\) d’une unité, les odds estimés changent d’un facteur de \\(\\exp(\\beta_{j})\\)\nCaractéristique catégorielle binaire : L’une des deux valeurs de la caractéristique est la catégorie de référence (dans certains langages, celle codée en 0). Changer la caractéristique \\(x_{j}\\) de la catégorie de référence à l’autre catégorie change les odds estimés d’un facteur de \\(\\exp(\\beta_{j})\\).\nCaractéristique catégorielle avec plus de deux catégories : Une solution pour gérer plusieurs catégories est le one-hot-encoding, signifiant que chaque catégorie a sa propre colonne. Vous avez besoin de seulement L-1 colonnes pour une caractéristique catégorielle avec L catégories, sinon c’est sur-paramétré. La L-ème catégorie est alors la catégorie de référence. Vous pouvez utiliser n’importe quel autre codage qui peut être utilisé dans la régression linéaire. L’interprétation pour chaque catégorie est alors équivalente à l’interprétation des caractéristiques binaires.\nIntercept \\(\\beta_{0}\\) : Lorsque toutes les caractéristiques numériques sont à zéro et les caractéristiques catégorielles sont à la catégorie de référence, les odds estimés sont \\(\\exp(\\beta_{0})\\). L’interprétation du poids de l’intercept n’est généralement pas pertinente.\n\n\n\n5.2.4 - Exemple\nNous utilisons le modèle de régression logistique pour prédire le cancer du col de l’utérus en fonction de certains facteurs de risque. Le tableau suivant montre les poids estimés, les rapports des odds associés et l’erreur standard des estimations.\n\n\n\nThe results of fitting a logistic regression model on the cervical cancer dataset. Shown are the features used in the model, their estimated weights and corresponding odds ratios, and the standard errors of the estimated weights.\n\n\nInterprétation d’une caractéristique numérique (« Nombre de MST diagnostiquées ») : Une augmentation du nombre de MST (maladies sexuellement transmissibles) diagnostiquées change (augmente) les odds de cancer par rapport à l’absence de cancer par un facteur de r sprintf('%.2f', coef.table['Num. of diagnosed STDs', 'Odds ratio']), lorsque toutes les autres caractéristiques restent les mêmes. Gardez à l’esprit que la corrélation n’implique pas la causalité.\nInterprétation d’une caractéristique catégorielle (« Contraceptifs hormonaux oui/non ») : Pour les femmes utilisant des contraceptifs hormonaux, les odds pour le cancer par rapport à l’absence de cancer sont inférieurs d’un facteur de r sprintf('%.2f', coef.table['Hormonal contraceptives y/n', 'Odds ratio']) par rapport aux femmes sans contraceptifs hormonaux, à condition que toutes les autres caractéristiques restent les mêmes.\nComme dans le modèle linéaire, les interprétations viennent toujours avec la clause que ‘toutes les autres caractéristiques restent les mêmes’.\n\n\n5.2.5 - Avantages et inconvénients\nMany of the pros and cons of the linear regression model also apply to the logistic regression model. Logistic regression has been widely used by many different people, but it struggles with its restrictive expressiveness (e.g. interactions must be added manually) and other models may have better predictive performance.\nAnother disadvantage of the logistic regression model is that the interpretation is more difficult because the interpretation of the weights is multiplicative and not additive.\nLogistic regression can suffer from complete separation. If there is a feature that would perfectly separate the two classes, the logistic regression model can no longer be trained. This is because the weight for that feature would not converge, because the optimal weight would be infinite. This is really a bit unfortunate, because such a feature is really useful. But you do not need machine learning if you have a simple rule that separates both classes. The problem of complete separation can be solved by introducing penalization of the weights or defining a prior probability distribution of weights.\nOn the good side, the logistic regression model is not only a classification model, but also gives you probabilities. This is a big advantage over models that can only provide the final classification. Knowing that an instance has a 99% probability for a class compared to 51% makes a big difference.\nLogistic regression can also be extended from binary classification to multi-class classification. Then it is called Multinomial Regression.\n\n\n5.2.6 - Logiciel\nJ’ai utilisé la fonction glm dans R pour tous les exemples. Vous pouvez trouver la régression logistique dans n’importe quel langage de programmation qui peut être utilisé pour effectuer des analyses de données, tel que Python, Java, Stata, Matlab, …",
    "crumbs": [
      "5 - Modèles interprétables",
      "5.2 - Régéression logistique"
    ]
  },
  {
    "objectID": "05-interpretable_models/05.7-other-interpretable-models.html",
    "href": "05-interpretable_models/05.7-other-interpretable-models.html",
    "title": "Apprentissage automatique interprétable",
    "section": "",
    "text": "Avertissement\n\n\n\nEn cours de traduction.",
    "crumbs": [
      "5 - Modèles interprétables",
      "5.7 - Autres modèles interprétables"
    ]
  },
  {
    "objectID": "05-interpretable_models/05.7-other-interpretable-models.html#autres-modèles-interprétables",
    "href": "05-interpretable_models/05.7-other-interpretable-models.html#autres-modèles-interprétables",
    "title": "Apprentissage automatique interprétable",
    "section": "5.7 - Autres modèles interprétables",
    "text": "5.7 - Autres modèles interprétables\nLa liste des modèles interprétables ne cesse de s’allonger et est de taille inconnue. Il comprend des modèles simples tels que des modèles linéaires, des arbres de décision et des Bayes naïfs, mais aussi des modèles plus complexes qui combinent ou modifient des modèles d’apprentissage automatique non interprétables pour les rendre plus interprétables. En particulier, les publications sur ce dernier type de modèles sont actuellement produites à une fréquence élevée et il est difficile de suivre les évolutions. Le livre ne présente que le classificateur Naive Bayes et les k-voisins les plus proches dans ce chapitre.\n\n5.7.1 - Classificateur naïf de Bayes\nLe classificateur Naive Bayes utilise le théorème des probabilités conditionnelles de Bayes. Pour chaque fonctionnalité, il calcule la probabilité d’une classe en fonction de la valeur de la fonctionnalité. Le classificateur Naive Bayes calcule indépendamment les probabilités de classe pour chaque caractéristique, ce qui équivaut à une hypothèse forte (= naïve) d’indépendance conditionnelle des caractéristiques. Naive Bayes est un modèle de probabilité conditionnelle et modélise la probabilité d’une classe \\(C_k\\) comme suit:\n\\[P(C_k|x)=\\frac{1}{Z}P(C_k)\\prod_{i=1}^n{}P(x_i|C_k)\\]\nLe terme Z est un paramètre d’échelle qui garantit que la somme des probabilités pour toutes les classes est égale à 1 (sinon, ce ne seraient pas des probabilités). La probabilité conditionnelle d’une classe est la probabilité de classe multipliée par la probabilité de chaque caractéristique étant donné la classe, normalisée par Z. Cette formule peut être dérivée en utilisant le théorème de Bayes.\nNaive Bayes est un modèle interprétable en raison de l’hypothèse d’indépendance. Il peut être interprété au niveau modulaire. Il est très clair pour chaque caractéristique dans quelle mesure elle contribue à une certaine prédiction de classe, puisque nous pouvons interpréter la probabilité conditionnelle.\n\n\n5.7.2 - K-Nearest Neighbors\nLa méthode des k-voisins les plus proches peut être utilisée pour la régression et la classification et utilise les voisins les plus proches d’un point de données à des fins de prédiction. Pour la classification, la méthode des k-voisins les plus proches attribue la classe la plus courante des voisins les plus proches d’une instance. Pour la régression, on prend la moyenne des résultats des voisins. Les parties délicates consistent à trouver le bon k et à décider comment mesurer la distance entre les instances, ce qui définit finalement le quartier.\nLe modèle du k-voisin le plus proche diffère des autres modèles interprétables présentés dans ce livre car il s’agit d’un algorithme d’apprentissage basé sur les instances. Comment interpréter les k-voisins les plus proches ? Tout d’abord, il n’y a pas de paramètres à apprendre, donc il n’y a pas d’interprétabilité au niveau modulaire. De plus, il existe un manque d’interprétabilité du modèle global car le modèle est intrinsèquement local et aucun poids ou structure global n’est explicitement appris. Peut-être est-ce interprétable au niveau local ? Pour expliquer une prédiction, vous pouvez toujours récupérer les k voisins qui ont été utilisés pour la prédiction. La question de savoir si le modèle est interprétable dépend uniquement de la question de savoir si vous pouvez « interpréter » une seule instance de l’ensemble de données. Si une instance comprend des centaines ou des milliers de fonctionnalités, elle n’est alors pas interprétable, dirais-je. Mais si vous avez peu de fonctionnalités ou un moyen de réduire votre instance aux fonctionnalités les plus importantes, présenter les k voisins les plus proches peut vous donner de bonnes explications.",
    "crumbs": [
      "5 - Modèles interprétables",
      "5.7 - Autres modèles interprétables"
    ]
  },
  {
    "objectID": "08-global_model_agnostic_methods/index.html",
    "href": "08-global_model_agnostic_methods/index.html",
    "title": "8 - Méthodes globales indépendantes du modèle",
    "section": "",
    "text": "Avertissement\n\n\n\nEn cours de traduction.\n\n\n\n8 - Méthodes globales indépendantes du modèle\nLes méthodes globales décrivent le comportement moyen d’un modèle d’apprentissage automatique. La contrepartie des méthodes globales sont les méthodes locales. Les méthodes globales sont souvent exprimées sous forme de valeurs attendues basées sur la distribution des données. Par exemple, le tracé de dépendance partielle, un tracé d’effet de caractéristique, est la prédiction attendue lorsque toutes les autres caractéristiques sont marginalisées. Puisque les méthodes d’interprétation globale décrivent un comportement moyen, elles sont particulièrement utiles lorsque le modélisateur souhaite comprendre les mécanismes généraux des données ou déboguer un modèle.\nDans ce livre, vous découvrirez les techniques d’interprétation globale indépendantes du modèle suivantes :\n\nLe tracé de dépendance partielle est une méthode d’effet de caractéristique.\nLes tracés d’effets locaux accumulés sont une autre méthode d’effet de caractéristique qui fonctionne lorsque les caractéristiques sont dépendantes.\nL’interaction des fonctionnalités (statistique H) quantifie dans quelle mesure la prédiction est le résultat d’effets conjoints des fonctionnalités.\nLa décomposition fonctionnelle est une idée centrale de l’interprétabilité et une technique qui décompose la fonction de prédiction complexe en parties plus petites.\nL’importance des fonctionnalités de permutation mesure l’importance d’une fonctionnalité sous la forme d’une augmentation de la perte lorsque la fonctionnalité est permutée.\nLes modèles de substitution globaux remplacent le modèle d’origine par un modèle d’interprétation plus simple.\nLes prototypes et les critiques sont des données représentatives d’une distribution et peuvent être utilisées pour améliorer l’interprétabilité.\n\n\n\n\n\n Retour au sommet",
    "crumbs": [
      "8 - Méthodes globales indépendantes du modèle"
    ]
  },
  {
    "objectID": "11-future/11.1-future-ml.html",
    "href": "11-future/11.1-future-ml.html",
    "title": "Apprentissage automatique interprétable",
    "section": "",
    "text": "Version original (en)\n\n\n\nhttps://christophm.github.io/interpretable-ml-book/the-future-of-machine-learning.html",
    "crumbs": [
      "11 - Un regard dans une boule de cristal",
      "11.1 - L'avenir de l'apprentissage automatique"
    ]
  },
  {
    "objectID": "11-future/11.1-future-ml.html#lavenir-de-lapprentissage-automatique",
    "href": "11-future/11.1-future-ml.html#lavenir-de-lapprentissage-automatique",
    "title": "Apprentissage automatique interprétable",
    "section": "11.1 - L’avenir de l’apprentissage automatique",
    "text": "11.1 - L’avenir de l’apprentissage automatique\nSans apprentissage automatique, il ne peut y avoir d’apprentissage automatique interprétable. Nous devons donc deviner où va l’apprentissage automatique avant de pouvoir parler d’interprétabilité.\nL’apprentissage automatique (ou « IA ») est associé à de nombreuses promesses et attentes. Mais commençons par une observation moins optimiste : alors que la science développe de nombreux outils sophistiqués d’apprentissage automatique, d’après mon expérience il est assez difficile de les intégrer dans des processus et des produits existants. Non pas parce que ce n’est pas possible, mais simplement parce qu’il faut du temps aux entreprises et aux institutions pour rattraper leur retard. Dans la ruée vers l’or du battage médiatique actuel sur l’IA, les entreprises ouvrent des « laboratoires d’IA », des « unités d’apprentissage automatique » et embauchent des « scientifiques des données », des « experts en apprentissage automatique », des « ingénieurs en IA », etc., mais la réalité est la suivante : d’après mon expérience, plutôt frustrante. Souvent, les entreprises ne disposent même pas de données sous la forme requise et les scientifiques des données restent inactifs pendant des mois. Parfois, les entreprises ont des attentes tellement élevées à l’égard de l’IA et de la science des données en raison des médias que les scientifiques des données ne pourront jamais les satisfaire. Et souvent, personne ne sait comment intégrer les scientifiques des données dans les structures existantes et dans bien d’autres problèmes. Cela m’amène à ma première prédiction.\nL’apprentissage automatique se développera lentement mais sûrement.\nLa numérisation progresse et la tentation d’automatiser ne cesse de se faire sentir. Même si le chemin vers l’adoption de l’apprentissage automatique est lent et semé d’embûches, l’apprentissage automatique passe constamment de la science aux processus métier, aux produits et aux applications du monde réel.\nJe pense que nous devons mieux expliquer aux non-experts quels types de problèmes peuvent être formulés comme des problèmes d’apprentissage automatique. Je connais de nombreux scientifiques des données hautement rémunérés qui effectuent des calculs Excel ou de l’analyse économique classique avec des rapports et des requêtes SQL au lieu d’appliquer l’apprentissage automatique. Mais quelques entreprises utilisent déjà avec succès l’apprentissage automatique, les grandes sociétés Internet étant en première ligne. Nous devons trouver de meilleurs moyens d’intégrer l’apprentissage automatique dans les processus et les produits, former les personnes et développer des outils d’apprentissage automatique faciles à utiliser. Je pense que l’apprentissage automatique deviendra beaucoup plus facile à utiliser : nous pouvons déjà constater que l’apprentissage automatique devient plus accessible, par exemple via les services « dans le nuage » (« Machine Learning as a service » – juste pour lancer quelques mots à la mode). Une fois que l’apprentissage automatique aura mûri – et que ce nouveau-né aura déjà fait ses premiers pas – ma prochaine prédiction est la suivante :\nL’apprentissage automatique alimentera beaucoup de choses.\nBasé sur le principe « Tout ce qui peut être automatisé le sera », je conclus que, dans la mesure du possible, les tâches seront formulées sous forme de problèmes de prédiction et résolues grâce à l’apprentissage automatique. L’apprentissage automatique est une forme d’automatisation ou peut du moins en faire partie. De nombreuses tâches actuellement effectuées par les humains sont remplacées par l’apprentissage automatique. Voici quelques exemples de tâches pour lesquelles l’apprentissage automatique est utilisé pour en automatiser certaines parties :\n\nTri / prise de décision / complétion de documents (par exemple dans les compagnies d’assurance, le secteur juridique ou les cabinets de conseil)\nDécisions basées sur des données telles que les demandes de crédit\nDécouverte de médicament\nContrôles qualité dans les chaînes de montage\nVoitures autonomes\nDiagnostic des maladies\nTraduction. Pour ce livre, j’ai utilisé un service de traduction appelé (DeepL) alimenté par des réseaux de neurones profonds pour améliorer mes phrases en les traduisant de l’anglais vers l’allemand et de nouveau vers l’anglais…\n\nLa percée de l’apprentissage automatique n’est pas seulement réalisée grâce à de meilleurs ordinateurs/plus de données/de meilleurs logiciels, mais aussi car :\nLes outils d’interprétabilité catalysent l’adoption de l’apprentissage automatique.\nPartant du principe que l’objectif d’un modèle d’apprentissage automatique ne peut jamais être parfaitement spécifié, il s’ensuit qu’un apprentissage automatique interprétable est nécessaire pour combler l’écart entre l’objectif mal spécifié et l’objectif réel. Dans de nombreux domaines et secteurs, l’interprétabilité sera le catalyseur de l’adoption de l’apprentissage automatique. Quelques preuves anecdotiques : de nombreuses personnes à qui j’ai parlé n’utilisent pas l’apprentissage automatique parce qu’elles ne peuvent pas expliquer les modèles aux autres. Je pense que l’interprétabilité résoudra ce problème et rendra l’apprentissage automatique attrayant pour les organisations et les personnes qui exigent une certaine transparence. Outre la mauvaise spécification du problème, de nombreux secteurs nécessitent une interprétabilité, que ce soit pour des raisons juridiques, par aversion au risque ou pour mieux comprendre la tâche sous-jacente. L’apprentissage automatique automatise le processus de modélisation et éloigne un peu plus l’humain des données et de la tâche sous-jacente : cela augmente le risque de problèmes avec la conception expérimentale, le choix de la distribution de la formation, l’échantillonnage, le codage des données, l’ingénierie des fonctionnalités, etc. Les outils d’interprétation facilitent l’identification de ces problèmes.",
    "crumbs": [
      "11 - Un regard dans une boule de cristal",
      "11.1 - L'avenir de l'apprentissage automatique"
    ]
  },
  {
    "objectID": "11-future/index.html",
    "href": "11-future/index.html",
    "title": "11 - Un regard dans une boule de cristal",
    "section": "",
    "text": "Version original (en)\n\n\n\nhttps://christophm.github.io/interpretable-ml-book/future.html\n\n\n\n11 - Un regard dans une boule de cristal\nQuel est l’avenir de l’apprentissage automatique interprétable ? Ce chapitre est un exercice mental spéculatif et une estimation subjective de la façon dont l’apprentissage automatique interprétable se développera. J’ai ouvert le livre sur des nouvelles plutôt pessimistes et j’aimerais conclure sur une perspective plus optimiste.\nJ’ai basé mes « prédictions » sur trois prémisses :\n\nNumérisation : Toute information (intéressante) sera numérisée. Pensez à l’argent dématérialisé et aux transactions en ligne. Pensez aux livres électroniques, à la musique et aux vidéos. Pensez à toutes les données sensorielles sur notre environnement, notre comportement humain, les processus de production industrielle, etc. Les moteurs de la numérisation de tout sont les suivants : ordinateurs/capteurs/stockage bon marché, effets d’échelle (le gagnant remporte tout), nouveaux modèles commerciaux, chaînes de valeur modulaires, pression sur les coûts et bien plus encore.\nAutomatisation : lorsqu’une tâche peut être automatisée et que le coût de l’automatisation est inférieur au coût d’exécution de la tâche au fil du temps, la tâche sera automatisée. Même avant l’introduction de l’ordinateur, nous avions un certain degré d’automatisation. Par exemple, la machine à tisser a automatisé le tissage ou la machine à vapeur a automatisé la puissance de travail. Mais les ordinateurs et la numérisation font passer l’automatisation à un niveau supérieur. Le simple fait que vous puissiez programmer des boucles for, écrire des macros Excel, automatiser les réponses aux e-mails, etc., montre à quel point un individu peut automatiser. Les guichets de billets automatisent l’achat des billets de train (plus besoin de caissier), les machines à laver automatisent la lessive, les ordres permanents automatisent les transactions de paiement, etc. L’automatisation des tâches libère du temps et de l’argent, il existe donc une énorme incitation économique et personnelle à automatiser les choses. Nous observons actuellement l’automatisation de la traduction linguistique, de la conduite automobile et, dans une moindre mesure, même de la découverte scientifique.\nMauvaise spécification : nous ne sommes pas en mesure de spécifier parfaitement un objectif avec toutes ses contraintes. Pensez au génie dans une bouteille qui prend toujours vos souhaits au pied de la lettre :  « Je veux être la personne la plus riche du monde ! » -&gt; Vous devenez la personne la plus riche, mais comme effet secondaire, la monnaie que vous détenez s’effondre à cause de l’inflation. « Je veux être heureux pour le reste de ma vie ! » -&gt; Les 5 minutes suivantes vous vous sentez très heureux, puis le génie vous tue. « Je souhaite la paix dans le monde ! » -&gt; Le génie tue tous les humains. Nous spécifions mal les objectifs, soit parce que nous ne connaissons pas toutes les contraintes, soit parce que nous ne pouvons pas les mesurer. Prenons les entreprises comme exemple de spécification d’objectifs imparfaite. Une société a pour objectif simple de gagner de l’argent pour ses actionnaires. Mais cette spécification ne rend pas compte du véritable objectif, avec toutes ses contraintes, que nous recherchons réellement : par exemple, nous n’apprécions pas qu’une entreprise tue des gens pour gagner de l’argent, empoisonne des rivières ou imprime simplement sa propre monnaie. Nous avons inventé des lois, des réglementations, des sanctions, des procédures de conformité, des syndicats et bien plus encore pour rafistoler la spécification imparfaite des objectifs. Un autre exemple que vous pouvez expérimenter par vous-même est Paperclips, un jeu dans lequel vous jouez à une machine dans le but de produire autant de trombones que possible. AVERTISSEMENT : on en devient accro. Je ne veux pas trop divulgâcher, mais disons que les choses deviennent très vite incontrôlables. En apprentissage automatique, les imperfections dans la spécification des objectifs proviennent d’abstractions de données imparfaites (populations biaisées, erreurs de mesure,…), de fonctions de perte non contraintes, d’un manque de connaissance des contraintes, d’un déplacement de la répartition entre les données de formation et d’application et bien plus encore.\n\nLa numérisation entraîne l’automatisation. La spécification imparfaite des objectifs entre en conflit avec l’automatisation. Je prétends que ce conflit sera en partie tempéré par des méthodes d’interprétation.\nLe décor de nos pronostics est planté, la boule de cristal est prête, regardons maintenant où pourrait aller le terrain !\n\n\n\n\n Retour au sommet",
    "crumbs": [
      "11 - Un regard dans une boule de cristal"
    ]
  },
  {
    "objectID": "13-citation/index.html",
    "href": "13-citation/index.html",
    "title": "13 - Citer ce livre",
    "section": "",
    "text": "Version original (en)\n\n\n\nhttps://christophm.github.io/interpretable-ml-book/cite.html\n\n\n\n13 - Citer ce livre\nSi vous avez trouvé ce livre utile pour votre article de blog, article de recherche ou produit, je vous serais reconnaissant de le citer. Vous pouvez citer le livre comme ceci :\nMolnar, C. (2022). Interpretable Machine Learning:\nA Guide for Making Black Box Models Explainable (2nd ed.).\nchristophm.github.io/interpretable-ml-book/\nOu utilisez l’entrée Bibtex suivante :\n@book{molnar2022,\n  title      = {Interpretable Machine Learning},\n  author     = {Christoph Molnar},\n  year       = {2022},\n  subtitle   = {A Guide for Making Black Box Models Explainable},\n  edition    = {2},\n  url        = {https://christophm.github.io/interpretable-ml-book}\n}\nJe suis toujours curieux de savoir où et comment les méthodes d’interprétation sont utilisées dans l’industrie et la recherche. Si vous utilisez le livre comme référence, ce serait formidable de m’écrire une ligne et me dire pourquoi l’avoir utilisé. Ceci est bien entendu facultatif et ne sert qu’à satisfaire ma propre curiosité et à stimuler des échanges intéressants. Mon mail est christoph.molnar.ai@gmail.com.\n\n\n\n\n\n\nNote du traducteur\n\n\n\nPour une citation de la version française :\nMolnar, C. (2024). Apprentissage automatique interprétable:\nUn guide pour rendre les boîtes noires exploitables (2nd ed.).\nTraduction : Nicolas Guillard\nnicolasguillard.github.io/interpretable-ml-book-fr/\net avec cette entrée Bibtex :\n@book{molnar2024,\n  title      = {Interpretable Machine Learning},\n  author     = {Christoph Molnar},\n  translator = {Nicolas Guillard},\n  language   = {french},\n  year       = {2024},\n  subtitle   = {Un guide pour rendre les boîtes noires exploitables},\n  edition    = {2},\n  url        = {https://nicolasguillard.github.io/interpretable-ml-book-fr/}\n}\nVous pouvez contacter le traducteur à cette adresse : nicolas.guillard.ai(AT)gmail.com\n\n\n\n\n\n\n Retour au sommet",
    "crumbs": [
      "13 - Citer ce livre"
    ]
  },
  {
    "objectID": "15-acknowledgements/index.html",
    "href": "15-acknowledgements/index.html",
    "title": "15 - Remerciements",
    "section": "",
    "text": "Version original (en)\n\n\n\nhttps://christophm.github.io/interpretable-ml-book/acknowledgements.html\n\n\n\n15 - Remerciements\nÉcrire ce livre était (et continue à être) très amusant. Mais c’est aussi beaucoup de travail et je suis très heureux du soutien que j’ai reçu.\nMon plus grand merci va à Katrin qui a fait le travail le plus dur en termes d’heures et d’efforts : elle a relu le livre du début à la fin et a découvert de nombreuses fautes d’orthographe et incohérences que je n’aurais jamais trouvées. Je lui suis très reconnaissant pour son soutien.\nUn grand merci à tous les auteurs invités. J’ai été vraiment surpris d’apprendre que les gens étaient intéressés à contribuer au livre. Et grâce à leurs efforts, le contenu du livre a pu être amélioré ! Tobias Goerke et Magdalena Lang ont écrit le chapitre sur les règles de portée (ancres). Fangzhou Li a contribué au chapitre sur la détection des concepts. Et Susanne Dandl a grandement amélioré le chapitre sur les exemples contrefactuels. Enfin, Verena Haunschmid a écrit la section sur les explications LIME pour les images. Je tiens également à remercier tous les lecteurs qui ont fourni des commentaires et contribué aux corrections directement sur GitHub!\nDe plus, je tiens à remercier tous ceux qui ont créé les illustrations : La couverture a été conçue par mon amie @YvonneDoinel. Les graphiques du chapitre sur les valeurs de Shapley ont été créés par Heidi Seibold, ainsi que l’exemple de la tortue dans le chapitre sur les exemples contradictoires. Verena Haunschmid a créé le graphique dans le chapitre Algorithme RuleFit .\nJe tiens également à remercier mon épouse et ma famille qui m’ont toujours soutenu. Ma femme, en particulier, a dû m’écouter beaucoup parler du livre. Elle m’a aidé à prendre de nombreuses décisions concernant l’écriture du livre.\nLa façon dont j’ai publié ce livre est un peu non conventionnelle. Premièrement, il n’est pas seulement disponible sous forme de livre de poche et d’ebook, mais également sous forme de site Web, entièrement gratuit. Le logiciel que j’ai utilisé pour créer ce livre s’appelle bookdown, écrit par Yihui Xie, qui a créé de nombreux packages R facilitant la combinaison de code R et de texte. Merci beaucoup! J’ai publié le livre en tant que travail en cours, ce qui m’a énormément aidé à obtenir des retours et à le monétiser au fur et à mesure. Je voudrais également vous remercier, cher lecteur, d’avoir lu ce livre sans un grand nom d’éditeur derrière lui.\nJe suis reconnaissant pour le financement de mes recherches sur l’apprentissage automatique interprétable par le ministère bavarois des Sciences et des Arts dans le cadre du Centre Digitisation.Bavaria (ZD.B) et par l’Institut bavarois de recherche pour la transformation numérique (bidt).\n\n\n\n\n\n\nNote du traducteur\n\n\n\nDifféremment à l’auteur, le traducteur a utilisé l’outil quarto pour composer les pages dont les parties dynamiques ont été réécrites en Python à partir du code R original.\n\n\n\n\n\n\n Retour au sommet",
    "crumbs": [
      "15 - Remerciements"
    ]
  },
  {
    "objectID": "00-summary/index.html",
    "href": "00-summary/index.html",
    "title": "Résumé",
    "section": "",
    "text": "Version original (en)\n\n\n\nhttps://christophm.github.io/interpretable-ml-book/index.html\n\n\n\nRésumé\n\nTraduction du livre en ligne https://christophm.github.io/interpretable-ml-book/\n\n\n\n\n \n\n\nL’apprentissage automatique représente un grand potentiel pour améliorer les produits, les processus et la recherche. Mais les ordinateurs n’expliquent généralement pas leurs prédictions, ce qui est un obstacle à l’adoption de l’apprentissage automatique. Ce livre traite de ce qui rend les modèles d’apprentissage automatique et leurs prédictions interprétables.\nAprès avoir exploré les concepts d’interprétabilité, vous découvrirez des modèles simples et interprétables tels que les arbres de décision, les règles de décision et la régression linéaire. Le livre met l’accent sur les méthodes agnostiques aux modèles pour interpréter les boîtes noires que sont ces modèles, tels que l’importance des caractéristiques et les effets locaux cumulés, et pour expliquer les prédictions individuelles avec les valeurs de Shapley et LIME. De plus, ce livre présente des méthodes spécifiques aux réseaux de neurones profonds.\nToutes les méthodes d’interprétation sont expliquées en profondeur et discutées de manière critique. Comment fonctionnent-elles sous le capot ? Quelles sont leurs forces et leurs faiblesses ? Comment leurs résultats peuvent-ils être interprétés ? Ce livre vous permettra de sélectionner et d’appliquer correctement la méthode d’interprétation la plus adaptée à votre projet d’apprentissage automatique. La lecture du livre est recommandée aux praticiens de l’apprentissage automatique, aux experts en science des données, aux statisticiens et à toute autre personne intéressée à rendre des modèles d’apprentissage automatique interprétables.\nVous pouvez acheter les versions PDF ou e-book (epub, mobi) en langue anglaise sur leanpub.com.\nVous pouvez acheter la version imprimée en langue anglaise sur Amazon.\nAu sujet de l’auteur: Il s’appelle Christoph Molnar, il est statisticien et spécialiste en apprentissage machine. Son but est de rendre l’apprentissage profond interprétable.\nSuivez-le sur X(ex-Twitter)! @ChristophMolnar\nCouverture réalisée par @YvonneDoinel\n\n\n\nlicence BY-NC-SA\n\n\nConsultez également Modeling Mindsets, son second livre.\nCe livre est publié sous la licence Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License.\n\n\n\n\n\n\nNote du traducteur\n\n\n\nAu sujet du traducteur (assisté d’IAs) : Nicolas Guillard. De formation initiale d’ingénieur en informatique, en passant par les sciences de l’information géographique et l’analyse e-marketing, il est maintenant plongé dans l’Intelligence Artificielle, l’interprétabilité et l’évaluation des grands modèles.\n\nDans la mesure du possible, la majorité du contenu du livre (dont les illustrations, sauf la couverture du livre en version anglaise) a été traduite. Toutefois, la plupart des références et leurs titres respectifs ont été laissés en anglais.\nLe lecteur peut communiquer toute remarque concernant cette traduction via ce formulaire.\nDernière traduction réalisée le 19 février 2025.\n\n\n\n\n\n\n Retour au sommet",
    "crumbs": [
      "Résumé"
    ]
  },
  {
    "objectID": "02-introduction/index.html",
    "href": "02-introduction/index.html",
    "title": "2 - Introduction",
    "section": "",
    "text": "Version original (en)\n\n\n\nhttps://christophm.github.io/interpretable-ml-book/intro.html",
    "crumbs": [
      "2 - Introduction"
    ]
  },
  {
    "objectID": "02-introduction/index.html#footnotes",
    "href": "02-introduction/index.html#footnotes",
    "title": "2 - Introduction",
    "section": "Notes de bas de page",
    "text": "Notes de bas de page\n\n\nFriedman, Jerome, Trevor Hastie, and Robert Tibshirani. “The elements of statistical learning”. hastie.su.domains/ElemStatLearn (2009).↩︎",
    "crumbs": [
      "2 - Introduction"
    ]
  },
  {
    "objectID": "03-interpretability/03.3-scope_of_interpretability.html",
    "href": "03-interpretability/03.3-scope_of_interpretability.html",
    "title": "Apprentissage automatique interprétable",
    "section": "",
    "text": "Avertissement\n\n\n\nEn cours de traduction.",
    "crumbs": [
      "3 - Interprétabilité",
      "3.3 - Portée de l'Interprétabilité"
    ]
  },
  {
    "objectID": "03-interpretability/03.3-scope_of_interpretability.html#portée-de-linterprétabilité",
    "href": "03-interpretability/03.3-scope_of_interpretability.html#portée-de-linterprétabilité",
    "title": "Apprentissage automatique interprétable",
    "section": "3.3 - Portée de l’Interprétabilité",
    "text": "3.3 - Portée de l’Interprétabilité\nUn algorithme entraîne un modèle qui produit les prédictions. Chaque étape peut être évaluée en termes de transparence ou d’interprétabilité.\n\n3.3.1 - Transparence d’un algorithme\nComment l’algorithme crée-t-il le modèle ?\nLa transparence de l’algorithme concerne la manière dont l’algorithme apprend un modèle à partir des données et le type de relations qu’il peut apprendre. Si vous utilisez des réseaux neuronaux convolutionnels pour classer des images, vous pouvez expliquer que l’algorithme apprend des détecteurs de bords et des filtres sur les couches les plus basses. Cela représente une compréhension de la façon dont l’algorithme fonctionne, mais pas pour le modèle spécifique qui est appris à la fin, ni pour la manière dont les prédictions individuelles sont faites. La transparence de l’algorithme nécessite uniquement la connaissance de l’algorithme et non des données ou du modèle appris. Ce livre se concentre sur l’interprétabilité du modèle et non sur la transparence de l’algorithme. Des algorithmes tels que la méthode des moindres carrés pour les modèles linéaires sont bien étudiés et compris. Ils se caractérisent par une grande transparence. Les approches d’apprentissage profond (poussant un gradient à travers un réseau avec des millions de poids) sont moins bien comprises et leur fonctionnement interne est l’objet de recherches en cours. Ils sont considérés comme moins transparents.\n\n\n3.3.2 - Interprétabilité Globale et Holistique du Modèle\nComment le modèle entraîné fait-il des prédictions ?\nOn pourrait décrire un modèle comme interprétable si vous pouvez comprendre l’ensemble du modèle d’un seul coup (Lipton 20161). Pour expliquer la sortie globale du modèle, vous avez besoin du modèle entraîné, de la connaissance de l’algorithme et des données. Ce niveau d’interprétabilité concerne la compréhension de la manière dont le modèle prend des décisions, basée sur une vue holistique de ses caractéristiques et de chacun des composants appris tels que les poids, d’autres paramètres et structures. Quelles caractéristiques sont importantes et quel type d’interactions se produisent entre elles ? L’interprétabilité globale du modèle aide à comprendre la distribution de votre résultat cible basé sur les caractéristiques. L’interprétabilité globale du modèle est très difficile à atteindre en pratique. Tout modèle qui dépasse quelques paramètres ou poids est peu susceptible de s’intégrer dans la mémoire à court terme de l’humain moyen. Je soutiens que vous ne pouvez pas vraiment imaginer un modèle linéaire avec 5 caractéristiques, car cela signifierait dessiner mentalement l’hyperplan estimé dans un espace à 5 dimensions. Tout espace de caractéristiques avec plus de 3 dimensions est tout simplement inconcevable pour les humains. Habituellement, lorsque les gens essaient de comprendre un modèle, ils ne considèrent que des parties de celui-ci, comme les poids dans les modèles linéaires.\n\n\n3.3.3 - Interprétabilité Globale du Modèle à un Niveau Modulaire\nComment les parties du modèle affectent-elles les prédictions ?\nUn modèle Naive Bayes avec des centaines de caractéristiques serait trop volumineux pour que vous et moi le gardions dans notre mémoire de travail. Et même si nous parvenions à mémoriser tous les poids, nous ne serions pas capables de faire rapidement des prédictions pour de nouveaux points de données. De plus, vous devez avoir la distribution conjointe de toutes les caractéristiques en tête pour estimer l’importance de chaque caractéristique et comment les caractéristiques affectent les prédictions en moyenne. Une tâche impossible. Mais vous pouvez facilement comprendre un seul poids. Bien que l’interprétabilité globale du modèle soit généralement hors de portée, il y a de bonnes chances de comprendre au moins certains modèles à un niveau modulaire. Tous les modèles ne sont pas interprétables au niveau des paramètres. Pour les modèles linéaires, les parties interprétables sont les poids, pour les arbres, ce seraient les divisions (caractéristiques sélectionnées plus points de coupure) et les prédictions des nœuds feuilles. Les modèles linéaires, par exemple, semblent comme s’ils pouvaient être parfaitement interprétés à un niveau modulaire, mais l’interprétation d’un seul poids est interdépendante de tous les autres poids. L’interprétation d’un seul poids est toujours accompagnée de la note de bas de page que les autres caractéristiques d’entrée restent à la même valeur, ce qui n’est pas le cas dans de nombreuses applications réelles. Un modèle linéaire qui prédit la valeur d’une maison, qui prend en compte à la fois la taille de la maison et le nombre de pièces, peut avoir un poids négatif pour la caractéristique des pièces. Cela peut arriver parce qu’il y a déjà la caractéristique de la taille de la maison, fortement corrélée. Sur un marché où les gens préfèrent des pièces plus grandes, une maison avec moins de pièces pourrait valoir plus qu’une maison avec plus de pièces si les deux ont la même taille. Les poids n’ont de sens que dans le contexte des autres caractéristiques du modèle. Mais les poids dans un modèle linéaire peuvent encore être mieux interprétés que les poids d’un réseau neuronal profond.\n\n\n3.3.4 - Interprétabilité Locale pour une Prédiction Unique\nPourquoi le modèle a-t-il fait une certaine prédiction pour une instance ?\nVous pouvez vous concentrer sur une seule instance et examiner ce que le modèle prédit pour cette entrée, et expliquer pourquoi. Si vous regardez une prédiction individuelle, le comportement du modèle par ailleurs complexe peut se comporter de manière plus agréable. Localement, la prédiction peut dépendre uniquement de manière linéaire ou monotone de certaines caractéristiques, plutôt que d’avoir une dépendance complexe envers elles. Par exemple, la valeur d’une maison peut dépendre de manière non linéaire de sa taille. Mais si vous ne regardez qu’une maison particulière de 100 mètres carrés, il est possible que pour ce sous-ensemble de données, votre prédiction de modèle dépende linéairement de la taille. Vous pouvez le découvrir en simulant comment le prix prédit change lorsque vous augmentez ou diminuez la taille de 10 mètres carrés. Les explications locales peuvent donc être plus précises que les explications globales. Ce livre présente des méthodes qui peuvent rendre les prédictions individuelles plus interprétables dans la section sur les méthodes agnostiques au modèle.\n\n\n3.3.5 - Interprétabilité Locale pour un Groupe de Prédictions\nPourquoi le modèle a-t-il fait des prédictions spécifiques pour un groupe d’instances ?\nLes prédictions du modèle pour plusieurs instances peuvent être expliquées soit avec des méthodes d’interprétation globale du modèle (à un niveau modulaire), soit avec des explications d’instances individuelles. Les méthodes globales peuvent être appliquées en prenant le groupe d’instances, en les traitant comme si le groupe était l’ensemble de données complet, et en utilisant les méthodes globales avec ce sous-ensemble. Les méthodes d’explication individuelle peuvent être utilisées sur chaque instance puis listées ou agrégées pour l’ensemble du groupe.",
    "crumbs": [
      "3 - Interprétabilité",
      "3.3 - Portée de l'Interprétabilité"
    ]
  },
  {
    "objectID": "03-interpretability/03.3-scope_of_interpretability.html#footnotes",
    "href": "03-interpretability/03.3-scope_of_interpretability.html#footnotes",
    "title": "Apprentissage automatique interprétable",
    "section": "Notes de bas de page",
    "text": "Notes de bas de page\n\n\nLipton, Zachary C. “The mythos of model interpretability.” arXiv preprint arXiv:1606.03490, (2016).↩︎",
    "crumbs": [
      "3 - Interprétabilité",
      "3.3 - Portée de l'Interprétabilité"
    ]
  },
  {
    "objectID": "03-interpretability/03.5-properties_of_explanations.html",
    "href": "03-interpretability/03.5-properties_of_explanations.html",
    "title": "Apprentissage automatique interprétable",
    "section": "",
    "text": "Avertissement\n\n\n\nEn cours de traduction.",
    "crumbs": [
      "3 - Interprétabilité",
      "3.5 - Propriétés des Explications"
    ]
  },
  {
    "objectID": "03-interpretability/03.5-properties_of_explanations.html#propriétés-des-explications",
    "href": "03-interpretability/03.5-properties_of_explanations.html#propriétés-des-explications",
    "title": "Apprentissage automatique interprétable",
    "section": "3.5 - Propriétés des Explications",
    "text": "3.5 - Propriétés des Explications\nNous voulons expliquer les prédictions d’un modèle d’apprentissage automatique. Pour cela, nous nous appuyons sur une méthode d’explication, qui est un algorithme générant des explications. Une explication relie généralement les valeurs des caractéristiques d’une instance à sa prédiction par le modèle d’une manière compréhensible par l’humain. D’autres types d’explications consistent en un ensemble d’instances de données (par exemple dans le cas du modèle des k-plus proches voisins). Par exemple, nous pourrions prédire le risque de cancer en utilisant une machine à vecteurs de support et expliquer les prédictions en utilisant la méthode des modèles locaux substituts, qui génère des arbres de décision comme explications. Ou nous pourrions utiliser un modèle de régression linéaire au lieu d’une machine à vecteurs de support. Le modèle de régression linéaire est déjà équipé d’une méthode d’explication (interprétation des poids).\nNous examinons de plus près les propriétés des méthodes d’explication et des explications (Robnik-Sikonja et Bohanec, 20181). Ces propriétés peuvent être utilisées pour juger de la qualité d’une méthode d’explication ou d’une explication. Il n’est pas clair pour toutes ces propriétés comment les mesurer correctement, l’un des défis étant de formaliser comment elles pourraient être calculées.\nPropriétés des Méthodes d’Explication\n\nPuissance expressive est le “langage” ou la structure des explications que la méthode est capable de générer. Une méthode d’explication pourrait générer des règles SI-ALORS, des arbres de décision, une somme pondérée, un langage naturel ou autre chose.\nTranslucidité décrit à quel point la méthode d’explication dépend de l’examen du modèle d’apprentissage automatique, comme ses paramètres. Par exemple, les méthodes d’explication reposant sur des modèles intrinsèquement interprétables comme le modèle de régression linéaire (spécifiques au modèle) sont très translucides. Les méthodes ne reposant que sur la manipulation des entrées et l’observation des prédictions ont une translucidité nulle. Selon le scénario, différents niveaux de translucidité peuvent être souhaitables. L’avantage d’une forte translucidité est que la méthode peut s’appuyer sur plus d’informations pour générer des explications. L’avantage d’une faible translucidité est que la méthode d’explication est plus portable.\nPortabilité décrit la gamme de modèles d’apprentissage automatique avec lesquels la méthode d’explication peut être utilisée. Les méthodes ayant une faible translucidité ont une plus grande portabilité car elles traitent le modèle d’apprentissage automatique comme une boîte noire. Les modèles substituts pourraient être la méthode d’explication avec la plus grande portabilité. Les méthodes qui ne fonctionnent que pour, par exemple, les réseaux neuronaux récurrents ont une faible portabilité.\nComplexité algorithmique décrit la complexité computationnelle de la méthode qui génère l’explication. Cette propriété est importante à considérer lorsque le temps de calcul est un goulot d’étranglement dans la génération d’explications.\n\nPropriétés des Explications Individuelles\n\nPrécision : À quel point une explication prédit-elle bien des données non vues ? Une haute précision est particulièrement importante si l’explication est utilisée pour des prédictions à la place du modèle d’apprentissage automatique. Une faible précision peut être acceptable si la précision du modèle d’apprentissage automatique est également faible, et si l’objectif est d’expliquer ce que fait le modèle boîte noire. Dans ce cas, seule la fidélité est importante.\nFidélité : À quel point l’explication approxime-t-elle la prédiction du modèle boîte noire ? Une haute fidélité est l’une des propriétés les plus importantes d’une explication, car une explication avec une faible fidélité est inutile pour expliquer le modèle d’apprentissage automatique. Précision et fidélité sont étroitement liées. Si le modèle boîte noire a une haute précision et que l’explication a une haute fidélité, l’explication a également une haute précision. Certaines explications offrent seulement une fidélité locale, ce qui signifie que l’explication n’approxime bien la prédiction du modèle que pour un sous-ensemble des données (p. ex. les modèles substituts locaux) ou même pour une seule instance de données (par exemple, Valeurs de Shapley).\nConsistance : À quel point une explication diffère-t-elle entre des modèles qui ont été entraînés sur la même tâche et qui produisent des prédictions similaires ? Par exemple, j’entraîne une machine à vecteurs de support et un modèle de régression linéaire sur la même tâche et les deux produisent des prédictions très similaires. Je calcule des explications en utilisant une méthode de mon choix et analyse à quel point les explications sont différentes. Si les explications sont très similaires, les explications sont très cohérentes. Je trouve cette propriété quelque peu délicate, car les deux modèles pourraient utiliser différentes caractéristiques, mais obtenir des prédictions similaires (également appelé “Effet Rashomon”). Dans ce cas, une haute consistance n’est pas souhaitable car les explications doivent être très différentes. Une haute consistance est souhaitable si les modèles se basent vraiment sur des relations similaires.\nStabilité : À quel point les explications pour des instances similaires sont-elles similaires ? Alors que la consistance compare les explications entre modèles, la stabilité compare les explications entre des instances similaires pour un modèle fixe. Une haute stabilité signifie que de légères variations dans les caractéristiques d’une instance ne changent pas substantiellement l’explication (à moins que ces légères variations ne changent également fortement la prédiction). Un manque de stabilité peut être le résultat d’une haute variance de la méthode d’explication. En d’autres termes, la méthode d’explication est fortement affectée par de légers changements des valeurs des caractéristiques de l’instance à expliquer. Un manque de stabilité peut également être causé par des composants non déterministes de la méthode d’explication, comme une étape d’échantillonnage des données, comme la méthode des modèles substituts locaux l’utilise. Une haute stabilité est toujours souhaitable.\nCompréhensibilité : Dans quelle mesure les humains comprennent-ils les explications ? Cela ressemble à une propriété parmi tant d’autres, mais c’est l’éléphant dans la pièce. Difficile à définir et à mesurer, mais extrêmement important à bien faire. Beaucoup de gens s’accordent à dire que la compréhensibilité dépend du public. Des idées pour mesurer la compréhensibilité incluent la mesure de la taille de l’explication (nombre de caractéristiques avec un poids non nul dans un modèle linéaire, nombre de règles de décision, …) ou tester à quel point les gens peuvent prédire le comportement du modèle d’apprentissage automatique à partir des explications. La compréhensibilité des caractéristiques utilisées dans l’explication devrait également être considérée. Une transformation complexe des caractéristiques peut être moins compréhensible que les caractéristiques originales.\nCertitude : L’explication reflète-t-elle la certitude du modèle d’apprentissage automatique ? De nombreux modèles d’apprentissage automatique ne donnent que des prédictions sans déclaration sur la confiance du modèle que la prédiction est correcte. Si le modèle prédit une probabilité de \\(4\\%\\) de cancer pour un patient, est-il aussi certain que la probabilité de \\(4\\%\\) qu’un autre patient, avec des valeurs de caractéristiques différentes, a reçue ? Une explication qui inclut la certitude du modèle est très utile.\nDegré d’Importance : Dans quelle mesure l’explication reflète-t-elle l’importance des caractéristiques ou des parties de l’explication ? Par exemple, si une règle de décision est générée comme explication pour une prédiction individuelle, est-il clair quelle condition de la règle était la plus importante ?\nNouveauté : L’explication reflète-t-elle si une instance de données à expliquer provient d’une région “nouvelle” éloignée de la distribution des données d’entraînement ? Dans de tels cas, le modèle peut être inexact et l’explication peut être inutile. Le concept de nouveauté est lié au concept de certitude. Plus la nouveauté est élevée, plus il est probable que le modèle aura une faible certitude en raison du manque de données.\nReprésentativité : Combien d’instances une explication couvre-t-elle ? Les explications peuvent couvrir l’ensemble du modèle (par exemple, l’interprétation des poids dans un modèle de régression linéaire) ou ne représenter qu’une prédiction individuelle (par exemple, Valeurs de Shapley).",
    "crumbs": [
      "3 - Interprétabilité",
      "3.5 - Propriétés des Explications"
    ]
  },
  {
    "objectID": "03-interpretability/03.5-properties_of_explanations.html#footnotes",
    "href": "03-interpretability/03.5-properties_of_explanations.html#footnotes",
    "title": "Apprentissage automatique interprétable",
    "section": "Notes de bas de page",
    "text": "Notes de bas de page\n\n\nRobnik-Sikonja, Marko, and Marko Bohanec. “Perturbation-based explanations of prediction models.” Human and Machine Learning. Springer, Cham. 159-175. (2018).↩︎",
    "crumbs": [
      "3 - Interprétabilité",
      "3.5 - Propriétés des Explications"
    ]
  },
  {
    "objectID": "03-interpretability/index.html",
    "href": "03-interpretability/index.html",
    "title": "3 - Interprétabilité",
    "section": "",
    "text": "Avertissement\n\n\n\nEn cours de traduction.",
    "crumbs": [
      "3 - Interprétabilité"
    ]
  },
  {
    "objectID": "03-interpretability/index.html#footnotes",
    "href": "03-interpretability/index.html#footnotes",
    "title": "3 - Interprétabilité",
    "section": "Notes de bas de page",
    "text": "Notes de bas de page\n\n\nMiller, Tim. “Explanation in artificial intelligence: Insights from the social sciences.” arXiv Preprint arXiv:1706.07269. (2017).↩︎\nKim, Been, Rajiv Khanna, and Oluwasanmi O. Koyejo. “Examples are not enough, learn to criticize! Criticism for interpretability.” Advances in Neural Information Processing Systems (2016).↩︎\nMurdoch, W. J., Singh, C., Kumbier, K., Abbasi-Asl, R., & Yu, B. “Definitions, methods, and applications in interpretable machine learning.” Proceedings of the National Academy of Sciences, 116(44), 22071-22080. (2019).↩︎",
    "crumbs": [
      "3 - Interprétabilité"
    ]
  },
  {
    "objectID": "04-datasets/04.2-datasets.html",
    "href": "04-datasets/04.2-datasets.html",
    "title": "Apprentissage automatique interprétable",
    "section": "",
    "text": "Avertissement\n\n\n\nEn cours de traduction.",
    "crumbs": [
      "4 - Jeux de données",
      "4.2 - Commentaires indésirables sur YouTube (Classification de Texte)"
    ]
  },
  {
    "objectID": "04-datasets/04.2-datasets.html#commentaires-indésirables-sur-youtube-classification-de-texte",
    "href": "04-datasets/04.2-datasets.html#commentaires-indésirables-sur-youtube-classification-de-texte",
    "title": "Apprentissage automatique interprétable",
    "section": "4.2 - Commentaires indésirables sur YouTube (Classification de Texte)",
    "text": "4.2 - Commentaires indésirables sur YouTube (Classification de Texte)\nEn tant qu’exemple de classification de texte, nous travaillons avec 1956 commentaires provenant de 5 vidéos YouTube différentes. Heureusement, les auteurs qui ont utilisé cet ensemble de données dans un article sur la classification du spam ont rendu les données librement disponibles (Alberto, Lochter et Almeida (2015)1).\nLes commentaires ont été collectés via l’API YouTube à partir de cinq des dix vidéos les plus visionnées sur YouTube au premier semestre 2015. Toutes les 5 sont des clips musicaux. L’une d’entre elles est “Gangnam Style” de l’artiste coréen Psy. Les autres artistes étaient Katy Perry, LMFAO, Eminem et Shakira.\nVoyez quelques-uns des commentaires. Les commentaires ont été manuellement étiquetés comme spam ou légitimes. Le spam a été codé avec un “1” et les commentaires légitimes avec un “0”.\n\n\n\n\n\n\nNote du traducteur\n\n\n\nTABLE 4.1: Sample of comments from the YouTube Spam dataset\n\n\nVous pouvez également aller sur YouTube et jeter un œil à la section des commentaires. Mais s’il vous plaît, ne vous faites pas prendre dans l’enfer de YouTube et finissez par regarder des vidéos de singes volant et buvant des cocktails à des touristes sur la plage. Le détecteur de spam de Google a probablement aussi beaucoup changé depuis 2015.\nRegardez la vidéo “Gangnam Style” qui a battu des records de visionnage ici.\nSi vous voulez jouer avec les données, vous pouvez trouver le fichier RData ainsi que le script R avec quelques fonctions de commodité dans le dépôt GitHub du livre.\n\n\n\n\n\n\nNote du traducteur\n\n\n\nVersion python + FR",
    "crumbs": [
      "4 - Jeux de données",
      "4.2 - Commentaires indésirables sur YouTube (Classification de Texte)"
    ]
  },
  {
    "objectID": "04-datasets/04.2-datasets.html#footnotes",
    "href": "04-datasets/04.2-datasets.html#footnotes",
    "title": "Apprentissage automatique interprétable",
    "section": "Notes de bas de page",
    "text": "Notes de bas de page\n\n\nAlberto, Túlio C, Johannes V Lochter, and Tiago A Almeida. “Tubespam: comment spam filtering on YouTube.” In Machine Learning and Applications (Icmla), Ieee 14th International Conference on, 138–43. IEEE. (2015).↩︎",
    "crumbs": [
      "4 - Jeux de données",
      "4.2 - Commentaires indésirables sur YouTube (Classification de Texte)"
    ]
  },
  {
    "objectID": "05-interpretable_models/05.1-linear-regression.html",
    "href": "05-interpretable_models/05.1-linear-regression.html",
    "title": "Apprentissage automatique interprétable",
    "section": "",
    "text": "Avertissement\n\n\n\nEn cours de traduction.",
    "crumbs": [
      "5 - Modèles interprétables",
      "5.1 - Régéression linéaire"
    ]
  },
  {
    "objectID": "05-interpretable_models/05.1-linear-regression.html#régéression-linéaire",
    "href": "05-interpretable_models/05.1-linear-regression.html#régéression-linéaire",
    "title": "Apprentissage automatique interprétable",
    "section": "5.1 - Régéression linéaire",
    "text": "5.1 - Régéression linéaire\nUn modèle de régression linéaire prédit la cible comme une somme pondérée des entrées de caractéristiques. La linéarité de la relation apprise facilite l’interprétation. Les modèles de régression linéaire sont utilisés depuis longtemps par les statisticiens, les informaticiens et d’autres personnes qui abordent des problèmes quantitatifs.\nLes modèles linéaires peuvent être utilisés pour modéliser la dépendance d’une cible de régression \\(y\\) par rapport à certaines caractéristiques \\(x\\). Les relations apprises sont linéaires et peuvent être écrites pour une instance unique \\(i\\) comme suit :\n\\[y=\\beta_{0}+\\beta_{1}x_{1}+\\ldots+\\beta_{p}x_{p}+\\epsilon\\]\nLe résultat prédit d’une instance est une somme pondérée de ses \\(p\\) caractéristiques. Les bêtas (\\(\\beta_{j}\\)) représentent les poids ou coefficients des caractéristiques appris. Le premier poids dans la somme (\\(\\beta_0\\)) est appelé l’ordonnée à l’origine (ou intercept) et n’est pas multiplié par une caractéristique. L’epsilon (\\(\\epsilon\\)) représente l’erreur que nous commettons encore, c’est-à-dire la différence entre la prédiction et le résultat réel. Ces erreurs sont supposées suivre une distribution gaussienne, ce qui signifie que nous commettons des erreurs dans les deux directions, négative et positive, et que nous commettons de nombreuses petites erreurs et peu de grandes erreurs.\nDiverses méthodes peuvent être utilisées pour estimer le poids optimal. La méthode des moindres carrés ordinaires est généralement utilisée pour trouver les poids qui minimisent les différences au carré entre les résultats réels et les résultats estimés :\n\\[\\hat{\\boldsymbol{\\beta}}=\\arg\\!\\min_{\\beta_0,\\ldots,\\beta_p}\\sum_{i=1}^n\\left(y^{(i)}-\\left(\\beta_0+\\sum_{j=1}^p\\beta_jx^{(i)}_{j}\\right)\\right)^{2}\\]\nNous n’entrerons pas dans le détail de la manière dont les poids optimaux peuvent être trouvés, mais si cela vous intéresse, vous pouvez lire le chapitre 3.2 du livre “The Elements of Statistical Learning” (Friedman, Hastie et Tibshirani, 2009)1 ou l’une des autres ressources en ligne sur les modèles de régression linéaire.\nLe plus grand avantage des modèles de régression linéaire est leur linéarité : cela rend la procédure d’estimation simple et, surtout, ces équations linéaires ont une interprétation facile à comprendre à un niveau modulaire (c’est-à-dire les poids). C’est l’une des principales raisons pour lesquelles le modèle linéaire et tous les modèles similaires sont si répandus dans les domaines académiques tels que la médecine, la sociologie, la psychologie et de nombreux autres domaines de recherche quantitative. Par exemple, dans le domaine médical, il est non seulement important de prédire le résultat clinique d’un patient, mais aussi de quantifier l’influence du médicament et en même temps de prendre en compte le sexe, l’âge et d’autres caractéristiques de manière interprétable.\nLes poids estimés sont accompagnés d’intervalles de confiance. Un intervalle de confiance est une plage pour l’estimation du poids qui couvre le poids “réel” avec une certaine confiance. Par exemple, un intervalle de confiance de 95 % pour un poids de 2 pourrait aller de 1 à 3. L’interprétation de cet intervalle serait : si nous répétions l’estimation 100 fois avec des données nouvellement échantillonnées, l’intervalle de confiance inclurait le vrai poids dans 95 cas sur 100, à condition que le modèle de régression linéaire soit le modèle correct pour les données.\nQue le modèle soit le “bon” modèle dépend de si les relations dans les données respectent certaines hypothèses, qui sont la linéarité, la normalité, l’homoscédasticité, l’indépendance, la fixité des caractéristiques et l’absence de multicollinéarité.\nLinéarité\nLe modèle de régression linéaire contraint la prédiction à être une combinaison linéaire des caractéristiques, ce qui est à la fois sa plus grande force et sa plus grande limitation. La linéarité conduit à des modèles interprétables. Les effets linéaires sont faciles à quantifier et à décrire. Ils sont additifs, donc il est facile de séparer les effets. Si vous suspectez des interactions entre caractéristiques ou une association non linéaire d’une caractéristique avec la valeur cible, vous pouvez ajouter des termes d’interaction ou utiliser des splines de régression.\nNormalité\nOn suppose que le résultat cible, étant donné les caractéristiques, suit une distribution normale. Si cette hypothèse est violée, les intervalles de confiance estimés des poids des caractéristiques sont invalides.\nHomoscédasticité (variance constante)\nOn suppose que la variance des termes d’erreur est constante sur tout l’espace des caractéristiques. Supposons que vous voulez prédire la valeur d’une maison en fonction de sa surface habitable en mètres carrés. Vous estimez un modèle linéaire qui suppose que, quelle que soit la taille de la maison, l’erreur autour de la réponse prédite a la même variance. Cette hypothèse est souvent violée dans la réalité. Dans l’exemple de la maison, il est plausible que la variance des termes d’erreur autour du prix prédit soit plus élevée pour les grandes maisons, car les prix sont plus élevés et il y a plus de place pour les fluctuations de prix. Supposons que l’erreur moyenne (différence entre le prix prédit et le prix réel) dans votre modèle de régression linéaire soit de 50 000 euros. Si vous supposez l’homoscédasticité, vous supposez que l’erreur moyenne de 50 000 est la même pour les maisons qui coûtent 1 million et pour celles qui coûtent seulement 40 000. Ceci est déraisonnable car cela signifierait que nous pouvons nous attendre à des prix de maison négatifs.\nIndépendance\nOn suppose que chaque instance est indépendante de toutes les autres. Si vous effectuez des mesures répétées, telles que plusieurs analyses de sang par patient, les points de données ne sont pas indépendants. Pour des données dépendantes, vous avez besoin de modèles spéciaux de régression linéaire, tels que les modèles à effets mixtes ou les GEE (Generalized Estimating Equations). Si vous utilisez le modèle de régression linéaire “normal”, vous pourriez tirer de mauvaises conclusions du modèle.\nCaractéristiques fixes\nLes caractéristiques d’entrée sont considérées comme “fixes”. Fixe signifie qu’elles sont traitées comme des “constantes données” et non comme des variables statistiques. Cela implique qu’elles sont exemptes d’erreurs de mesure. C’est une hypothèse plutôt irréaliste. Sans cette hypothèse, cependant, vous devriez adapter des modèles d’erreur de mesure très complexes qui tiennent compte des erreurs de mesure de vos caractéristiques d’entrée. Et généralement, vous ne voulez pas faire cela.\nAbsence de multicollinéarité\nVous ne voulez pas de caractéristiques fortement corrélées, car cela perturbe l’estimation des poids. Dans une situation où deux caractéristiques sont fortement corrélées, il devient problématique d’estimer les poids car les effets des caractéristiques sont additifs et il devient indéterminable à laquelle des caractéristiques corrélées attribuer les effets.\n\n5.1.1 Interprétation\nL’interprétation d’un poids dans le modèle de régression linéaire dépend du type de la caractéristique correspondante.\n\nCaractéristique numérique : Augmenter la caractéristique numérique d’une unité change le résultat estimé de son poids. Un exemple de caractéristique numérique est la taille d’une maison.\nCaractéristique binaire : Une caractéristique qui prend l’une des deux valeurs possibles pour chaque instance. Un exemple est la caractéristique “La maison possède un jardin”. L’une des valeurs est considérée comme la catégorie de référence (dans certains langages de programmation encodée par 0), comme “Pas de jardin”. Changer la caractéristique de la catégorie de référence à l’autre catégorie change le résultat estimé du poids de la caractéristique.\nCaractéristique catégorielle avec plusieurs catégories : Une caractéristique avec un nombre fixe de valeurs possibles. Un exemple est la caractéristique “type de sol”, avec les catégories possibles “moquette”, “stratifié” et “parquet”. Une solution pour gérer de nombreuses catégories est le one-hot-encoding, signifiant que chaque catégorie a sa propre colonne binaire. Pour une caractéristique catégorielle avec \\(L\\) catégories, vous n’avez besoin que de \\(L-1\\) colonnes, car la \\(L\\)-ème colonne aurait des informations redondantes (par exemple, lorsque les colonnes 1 à \\(L-1\\) ont toutes une valeur de 0 pour une instance, nous savons que la caractéristique catégorielle de cette instance prend la catégorie \\(L\\)). L’interprétation pour chaque catégorie est alors la même que l’interprétation pour les caractéristiques binaires. Certains langages, comme R, vous permettent de coder les caractéristiques catégorielles de diverses manières, comme décrit plus loin dans ce chapitre.\nIntercept \\(\\beta_0\\) : L’intercept est le poids de la caractéristique pour la “caractéristique constante”, qui est toujours 1 pour toutes les instances. La plupart des logiciels ajoutent automatiquement cette caractéristique “1” pour estimer l’intercept. L’interprétation est : Pour une instance avec toutes les valeurs des caractéristiques numériques à zéro et les valeurs des caractéristiques catégorielles aux catégories de référence, la prédiction du modèle est le poids de l’intercept. L’interprétation de l’intercept n’est généralement pas pertinente car les instances avec toutes les valeurs de caractéristiques à zéro n’ont souvent pas de sens. L’interprétation n’est significative que lorsque les caractéristiques ont été standardisées (moyenne de zéro, écart-type de un). Dans ce cas, l’intercept reflète le résultat prédit d’une instance où toutes les caractéristiques sont à leur valeur moyenne.\n\nL’interprétation des caractéristiques dans le modèle de régression linéaire peut être automatisée en utilisant les modèles de texte suivants.\nInterprétation d’une Caractéristique Numérique\nUne augmentation de la caractéristique \\(x_{k}\\) d’une unité augmente la prédiction pour \\(y\\) de \\(\\beta_k\\) unités lorsque toutes les autres valeurs de caractéristiques restent fixes.\nInterprétation d’une Caractéristique Catégorielle\nChanger la caractéristique \\(x_{k}\\) de la catégorie de référence à l’autre catégorie augmente la prédiction pour \\(y\\) de \\(\\beta_{k}\\) lorsque toutes les autres caractéristiques restent fixes.\nUne autre mesure importante pour interpréter les modèles linéaires est la mesure du \\(R^2\\). Le \\(R^2\\) vous indique quelle part de la variance totale de votre résultat cible est expliquée par le modèle. Plus le \\(R^2\\) est élevé, mieux votre modèle explique les données. La formule pour calculer le \\(R^2\\) est :\n\\[R^2 = 1 - \\frac{SSE}{SST}\\]\nSSE est la somme des carrés des termes d’erreur :\n\\[SSE = \\sum_{i=1}^n (y^{(i)} - \\hat{y}^{(i)})^2\\]\nSST est la somme des carrés de la variance des données :\n\\[SST = \\sum_{i=1}^n (y^{(i)} - \\bar{y})^2\\]\nLe SSE vous indique combien de variance reste après avoir ajusté le modèle linéaire, mesurée par les différences au carré entre les valeurs cibles prédites et réelles. SST est la variance totale du résultat cible. Le \\(R^2\\) vous indique quelle part de votre variance peut être expliquée par le modèle linéaire. Le \\(R^2\\) varie généralement entre 0 pour les modèles où le modèle n’explique pas du tout les données et 1 pour les modèles qui expliquent toute la variance de vos données. Il est également possible que le \\(R^2\\) prenne une valeur négative sans violer aucune règle mathématique. Cela se produit lorsque SSE est supérieur à SST, ce qui signifie qu’un modèle ne capture pas la tendance des données et s’ajuste aux données pire qu’en utilisant la moyenne de la cible comme prédiction.\nIl y a un piège, car le \\(R^2\\) augmente avec le nombre de caractéristiques dans le modèle, même si elles ne contiennent aucune information sur la valeur cible du tout. Par conséquent, il est préférable d’utiliser le \\(R^2\\) ajusté, qui tient compte du nombre de caractéristiques utilisées dans le modèle. Son calcul est :\n\\[\\bar{R}^2 = 1 - (1-R^2) \\frac{n-1}{n-p-1}\\]\noù \\(p\\) est le nombre de caractéristiques et \\(n\\) le nombre d’instances.\nIl n’est pas significatif d’interpréter un modèle avec un \\(R^2\\) (ajusté) très faible, car un tel modèle n’explique en fait pas beaucoup de la variance. Toute interprétation des poids ne serait pas significative.\nImportance des caractéristiques\nL’importance d’une caractéristique dans un modèle de régression linéaire peut être mesurée par la valeur absolue de sa statistique t. La statistique t est le poids estimé mis à l’échelle avec son erreur standard.\n\\[t_{\\hat{\\beta}_j} = \\frac{\\hat{\\beta}_j}{SE(\\hat{\\beta}_j)}\\]\nExaminons ce que cette formule nous indique : L’importance d’une caractéristique augmente avec l’augmentation du poids. Cela a du sens. Plus le poids estimé a de variance (= moins nous sommes certains de la valeur correcte), moins la caractéristique est importante. Cela a également du sens.\n\n\n5.1.2 - Exemple\nDans cet exemple, nous utilisons le modèle de régression linéaire pour prédire le nombre de vélos loués un jour donné, en fonction des informations météorologiques et calendaires. Pour l’interprétation, nous examinons les poids de régression estimés. Les caractéristiques sont composées de caractéristiques numériques et catégorielles. Pour chaque caractéristique, le tableau montre le poids estimé, l’erreur standard de l’estimation (\\(SE\\)) et la valeur absolue de la statistique t (\\(|t|\\)).\n\n\n\nInterpretation of a numerical feature (temperature): An increase of the temperature by 1 degree Celsius increases the predicted number of bicycles by 110.7, when all other features remain fixed.\n\n\nInterprétation d’une caractéristique numérique (température) : Une augmentation de la température de 1 degré Celsius augmente le nombre prévu de vélos de r sprintf('%.1f', lm_summary_print['temp', 'Estimate']), lorsque toutes les autres caractéristiques restent fixes.\nInterprétation d’une caractéristique catégorielle (“weathersit”) : Le nombre estimé de vélos est r sprintf('%.1f', lm_summary_print['weathersitRAIN/SNOW/STORM', 'Estimate']) inférieur lorsqu’il pleut, neige ou en cas de tempête, par rapport à un temps clément – en supposant à nouveau que toutes les autres caractéristiques restent inchangées. Lorsque le temps est brumeux, le nombre prévu de vélos est r sprintf('%.1f', lm_summary_print['weathersitMISTY', 'Estimate']) inférieur par rapport à un temps clément, à condition que toutes les autres caractéristiques restent les mêmes.\nToutes les interprétations viennent toujours avec la note de bas de page que “toutes les autres caractéristiques restent les mêmes”. Cela est dû à la nature des modèles de régression linéaire. La cible prédite est une combinaison linéaire des caractéristiques pondérées. L’équation linéaire estimée est un hyperplan dans l’espace caractéristique/cible (une simple ligne dans le cas d’une seule caractéristique). Les poids spécifient la pente (gradient) de l’hyperplan dans chaque direction. Le bon côté est que l’additivité isole l’interprétation de l’effet d’une caractéristique individuelle de toutes les autres caractéristiques. Cela est possible parce que tous les effets des caractéristiques (= poids fois la valeur de la caractéristique) dans l’équation sont combinés avec un plus. Sur le mauvais côté des choses, l’interprétation ignore la distribution conjointe des caractéristiques. Augmenter une caractéristique, sans changer une autre, peut conduire à des points de données irréalistes ou du moins improbables. Par exemple, augmenter le nombre de pièces pourrait être irréaliste sans également augmenter la taille d’une maison.\n\n\n5.1.3 - Interprétation visuelle\nDiverses visualisations rendent le modèle de régression linéaire facile et rapide à comprendre pour les humains.\n\n5.1.3.1 - Graphique des poids\nLes informations du tableau des poids (estimation des poids et de la variance) peuvent être visualisées dans un graphique des poids. Le graphique suivant montre les résultats du précédent modèle de régression linéaire.\n\n\n\nWeights are displayed as points and the 95% confidence intervals as lines.\n\n\nLe graphique des poids montre que le temps pluvieux/neigeux/tempétueux a un fort effet négatif sur le nombre prévu de vélos. Le poids de la caractéristique du jour ouvrable est proche de zéro et zéro est inclus dans l’intervalle de 95 %, ce qui signifie que l’effet n’est pas statistiquement significatif. Certains intervalles de confiance sont très courts et les estimations sont proches de zéro, mais les effets des caractéristiques étaient statistiquement significatifs. La température est un tel candidat. Le problème avec le graphique des poids est que les caractéristiques sont mesurées à différentes échelles. Alors que pour le temps, le poids estimé reflète la différence entre un temps clément et pluvieux/tempétueux/neigeux, pour la température, il reflète uniquement une augmentation de 1 degré Celsius. Vous pouvez rendre les poids estimés plus comparables en mettant à l’échelle les caractéristiques (moyenne nulle et écart-type de un) avant d’ajuster le modèle linéaire.\n\n\n5.1.3.2 - Graphique de l’effet\nLes poids d’un modèle de régression linéaire peuvent être analysés de manière plus significative lorsqu’ils sont multipliés par les valeurs réelles des caractéristiques. Les poids dépendent de l’échelle des caractéristiques et seront différents si vous avez une caractéristique qui mesure, par exemple, la taille d’une personne et que vous passez de mètre à centimètre. Le poids changera, mais les effets réels dans vos données ne changeront pas. Il est également important de connaître la distribution de votre caractéristique dans les données, car si vous avez une très faible variance, cela signifie que presque toutes les instances ont une contribution similaire de cette caractéristique. Le graphique des effets peut vous aider à comprendre combien la combinaison du poids et de la caractéristique contribue aux prédictions dans vos données. Commencez par calculer les effets, qui sont le poids par caractéristique multiplié par la valeur de la caractéristique d’une instance :\n\\[\\text{effect}_{j}^{(i)}=w_{j}x_{j}^{(i)}\\]\nLes effets peuvent être visualisés avec des diagrammes à boîtes. La boîte dans un diagramme à boîtes contient la plage d’effets pour la moitié des données (quantiles d’effets de 25 % à 75 %). La ligne verticale dans la boîte est l’effet médian, c’est-à-dire que 50 % des instances ont un effet inférieur et l’autre moitié un effet supérieur sur la prédiction. Les points sont des valeurs aberrantes, définies comme des points qui sont plus de 1,5 * IQR (intervalle interquartile, c’est-à-dire la différence entre le premier et le troisième quartile) au-dessus du troisième quartile ou moins de 1,5 * IQR en dessous du premier quartile. Les deux lignes horizontales, appelées moustaches inférieure et supérieure, relient les points en dessous du premier quartile et au-dessus du troisième quartile qui ne sont pas des valeurs aberrantes. S’il n’y a pas de valeurs aberrantes, les moustaches s’étendront jusqu’aux valeurs minimales et maximales.\nLes effets des caractéristiques catégorielles peuvent être résumés dans un seul diagramme à boîtes, par rapport au graphique des poids, où chaque catégorie a sa propre ligne.\n\n\n\nThe feature effect plot shows the distribution of effects (= feature value times feature weight) across the data per feature.\n\n\nSi nous moyennons les prédictions pour les instances des données d’entraînement, nous obtenons une moyenne de r round(predictions_mean, 0). En comparaison, la prédiction de l’instance r i-ème est faible, puisque seulement r round(pred_i, 0) locations de vélos sont prédites. Le graphique des effets révèle la raison. Les diagrammes à boîtes montrent les distributions des effets pour toutes les instances du jeu de données, les croix montrent les effets pour l’instance r i-ème. L’instance r i-ème a un faible effet de température car ce jour-là la température était de r round(X[i, 'temp'],0) degrés, ce qui est faible par rapport à la plupart des autres jours (et rappelons que le poids de la caractéristique de température est positif). De plus, l’effet de la caractéristique de tendance “jours_depuis_2011” est petit par rapport aux autres instances de données car cette instance date du début de 2011 (r X[i, 'days_since_2011'] jours) et la caractéristique de tendance a également un poids positif.\n\n\n\n5.1.4 - Expliquer les prédictions individuelles\nDe combien chaque caractéristique d’une instance a-t-elle contribuée à la prédiction ? La contribution spécifique de chaque caractéristique d’une instance à la prédiction peut être évaluée en calculant les effets pour cette instance. Une interprétation des effets spécifiques à l’instance n’a de sens qu’en comparaison avec la distribution de l’effet pour chaque caractéristique. Nous souhaitons expliquer la prédiction du modèle linéaire pour l’instance r i-ème du jeu de données sur les locations de vélos. L’instance présente les valeurs de caractéristiques suivantes.\n\n\n\nTable\n\n\nPour obtenir les effets des caractéristiques de cette instance, nous devons multiplier ses valeurs de caractéristiques par les poids correspondants du modèle de régression linéaire. Pour la valeur “r df[\"workingday\", \"value\"]” de la caractéristique “r df[\"workingday\", \"feature\"]”, l’effet est r round(lm_summary_print[paste(df[\"workingday\", \"feature\"], df[\"workingday\", \"value\"], sep = \"\"), \"Estimate\"], 1). Pour une température de r round(as.numeric(as.character(df[\"temp\", \"value\"])), 1) degrés Celsius, l’effet est r round(as.numeric(as.character(df[\"temp\", \"value\"])) * lm_summary_print[as.character(df[\"temp\", \"feature\"]), \"Estimate\"], 1). Nous ajoutons ces effets individuels sous forme de croix au graphique des effets, ce qui nous montre la distribution des effets dans les données. Cela nous permet de comparer les effets individuels à la distribution des effets dans les données.\n\n\n\nThe effect plot for one instance shows the effect distribution and highlights the effects of the instance of interest.\n\n\nSi nous moyennons les prédictions pour les instances des données d’entraînement, nous obtenons une moyenne de r round(predictions_mean, 0). En comparaison, la prédiction de l’instance r i-ème est faible, puisque seulement r round(pred_i, 0) locations de vélos sont prédites. Le graphique des effets révèle la raison. Les diagrammes à boîtes montrent les distributions des effets pour toutes les instances du jeu de données, les croix montrent les effets pour l’instance r i-ème. L’instance r i-ème a un faible effet de température car ce jour-là la température était de r round(X[i, 'temp'],0) degrés, ce qui est faible par rapport à la plupart des autres jours (et rappelons que le poids de la caractéristique de température est positif). De plus, l’effet de la caractéristique de tendance “jours_depuis_2011” est petit par rapport aux autres instances de données car cette instance date du début de 2011 (r X[i, 'days_since_2011'] jours) et la caractéristique de tendance a également un poids positif.\n\n\n5.1.5 - Encoder les caractéristiques modales\nIl existe plusieurs manières de coder une caractéristique catégorielle, et le choix influence l’interprétation des poids.\nLa norme dans les modèles de régression linéaire est le codage de traitement, qui est suffisant dans la plupart des cas. Utiliser différents codages revient à créer différentes matrices (de conception) à partir d’une seule colonne avec la caractéristique catégorielle. Cette section présente trois codages différents, mais il en existe beaucoup d’autres. L’exemple utilisé comporte six instances et une caractéristique catégorielle avec trois catégories. Pour les deux premières instances, la caractéristique prend la catégorie A ; pour les instances trois et quatre, la catégorie B ; et pour les deux dernières instances, la catégorie C.\nCodage de traitement\nDans le codage de traitement, le poids par catégorie est la différence estimée dans la prédiction entre la catégorie correspondante et la catégorie de référence. L’intercept du modèle linéaire est la moyenne de la catégorie de référence (lorsque toutes les autres caractéristiques restent les mêmes). La première colonne de la matrice de conception est l’intercept, qui est toujours 1. La deuxième colonne indique si l’instance i appartient à la catégorie B, la troisième colonne indique si elle appartient à la catégorie C. Il n’est pas nécessaire d’avoir une colonne pour la catégorie A, car cela surdéterminerait l’équation linéaire et il ne serait pas possible de trouver une solution unique pour les poids. Il suffit de savoir qu’une instance n’appartient ni à la catégorie B ni à la catégorie C.\nMatrice des caractéristiques : \\[\\begin{pmatrix}1&0&0\\\\1&0&0\\\\1&1&0\\\\1&1&0\\\\1&0&1\\\\1&0&1\\\\\\end{pmatrix}\\]\nCodage des effets\nLe poids par catégorie est la différence estimée de y entre la catégorie correspondante et la moyenne globale (à condition que toutes les autres caractéristiques soient zéro ou la catégorie de référence). La première colonne est utilisée pour estimer l’intercept. Le poids \\(\\beta_{0}\\) associé à l’intercept représente la moyenne globale et \\(\\beta_{1}\\), le poids pour la deuxième colonne, est la différence entre la moyenne globale et la catégorie B. L’effet total de la catégorie B est \\(\\beta_{0}+\\beta_{1}\\). L’interprétation pour la catégorie C est équivalente. Pour la catégorie de référence A, \\(-(\\beta_{1}+\\beta_{2})\\) est la différence par rapport à la moyenne globale et \\(\\beta_{0}-(\\beta_{1}+\\beta_{2})\\) l’effet global.\nMatrice des caractéristiques : \\[\\begin{pmatrix}1&-1&-1\\\\1&-1&-1\\\\1&1&0\\\\1&1&0\\\\1&0&1\\\\1&0&1\\\\\\end{pmatrix}\\]\nCodage indicateur (Dummy coding)\nLe \\(\\beta\\) par catégorie est la valeur moyenne estimée de \\(y\\) pour chaque catégorie (à condition que toutes les autres valeurs de caractéristiques soient zéro ou la catégorie de référence). Notez que l’intercept a été omis ici afin qu’une solution unique puisse être trouvée pour les poids du modèle linéaire. Une autre manière de pallier ce problème de multicollinéarité est d’exclure l’une des catégories.\nMatrice des caractéristiques : \\[\\begin{pmatrix}1&0&0\\\\1&0&0\\\\0&1&0\\\\0&1&0\\\\0&0&1\\\\0&0&1\\\\\\end{pmatrix}\\]\nSi vous souhaitez approfondir les différents codages des caractéristiques catégorielles, consultez cette page de vue d’ensemble et ce billet de blog.\n\n\n5.1.6 - Est-ce que les modèles linéaires produisent des bonnes explications ?\nAu regard des attributs qui constituent une bonne explication, tels que présentés dans le chapitre sur les explications conviviales pour l’être humain, les modèles linéaires ne créent pas les meilleures explications. Ils sont contrastifs, mais l’instance de référence est un point de données où toutes les caractéristiques numériques sont à zéro et les caractéristiques catégorielles sont à leurs catégories de référence. Il s’agit généralement d’une instance artificielle et dénuée de sens, peu susceptible de se produire dans vos données ou dans la réalité. Il y a une exception : si toutes les caractéristiques numériques sont centrées sur la moyenne (caractéristique moins la moyenne de la caractéristique) et que toutes les caractéristiques catégorielles sont codées par effet, l’instance de référence est le point de données où toutes les caractéristiques prennent la valeur moyenne de la caractéristique. Cela peut aussi être un point de données inexistant, mais il peut au moins être plus probable ou plus significatif. Dans ce cas, les poids multipliés par les valeurs des caractéristiques (effets des caractéristiques) expliquent la contribution au résultat prédit en contraste avec l’“instance moyenne”. Un autre aspect d’une bonne explication est la sélectivité, qui peut être atteinte dans les modèles linéaires en utilisant moins de caractéristiques ou en entraînant des modèles linéaires parcimonieux. Mais par défaut, les modèles linéaires ne créent pas d’explications sélectives. Les modèles linéaires créent des explications véridiques, tant que l’équation linéaire est un modèle approprié pour la relation entre les caractéristiques et le résultat. Plus il y a de non-linéarités et d’interactions, moins le modèle linéaire sera précis et moins les explications seront véridiques. La linéarité rend les explications plus générales et plus simples. La nature linéaire du modèle, je crois, est le principal facteur pour lequel les gens utilisent des modèles linéaires pour expliquer les relations.\n\n\n5.1.7 - Modèles Linéaires Parcimonieux\nLes exemples de modèles linéaires que j’ai choisis semblent tous bien ordonnés, n’est-ce pas ? Mais dans la réalité, vous pourriez ne pas avoir juste une poignée de caractéristiques, mais des centaines ou des milliers. Et vos modèles de régression linéaire ? L’interprétabilité se dégrade. Vous pourriez même vous retrouver dans une situation où il y a plus de caractéristiques que d’instances, et vous ne pourriez pas du tout ajuster un modèle linéaire standard. La bonne nouvelle est qu’il existe des moyens d’introduire de la parcimonie (= peu de caractéristiques) dans les modèles linéaires.\n\n5.1.7.1 - Lasso\nLe Lasso est une manière automatique et pratique d’introduire de la parcimonie dans le modèle de régression linéaire. Lasso signifie “Least Absolute Shrinkage and Selection Operator” (opérateur de réduction et de sélection absolue minimale) et, lorsqu’il est appliqué dans un modèle de régression linéaire, effectue une sélection de caractéristiques et une régularisation des poids des caractéristiques sélectionnées. Considérons le problème de minimisation que les poids optimisent :\n\\[min_{\\boldsymbol{\\beta}}\\left(\\frac{1}{n}\\sum_{i=1}^n(y^{(i)}-x_i^T\\boldsymbol{\\beta})^2\\right)\\]\nLasso ajoute un terme à ce problem d’optimisation.\n\\[min_{\\boldsymbol{\\beta}}\\left(\\frac{1}{n}\\sum_{i=1}^n(y^{(i)}-x_{i}^T\\boldsymbol{\\beta})^2+\\lambda||\\boldsymbol{\\beta}||_1\\right)\\]\nLe terme \\(\\|\\boldsymbol{\\beta}\\|_1\\), la norme L1 du vecteur de caractéristiques, conduit à une pénalisation des poids importants. Comme la norme L1 est utilisée, de nombreux poids reçoivent une estimation de 0 et les autres sont réduits. Le paramètre lambda \\((\\lambda)\\) contrôle la force de l’effet de régularisation et est généralement ajusté par validation croisée. Surtout lorsque lambda est grand, de nombreux poids deviennent 0. Les poids des caractéristiques peuvent être visualisés en fonction du terme de pénalité lambda. Chaque poids de caractéristique est représenté par une courbe dans la figure suivante.\n\n\n\nWith increasing penalty of the weights, fewer and fewer features receive a non-zero weight estimate. These curves are also called regularization paths. The number above the plot is the number of non-zero weights.\n\n\nQuelle valeur devrions-nous choisir pour lambda ? Si vous voyez le terme de pénalisation comme un paramètre de réglage, alors vous pouvez trouver le lambda qui minimise l’erreur du modèle avec une validation croisée. Vous pouvez également considérer lambda comme un paramètre pour contrôler l’interprétabilité du modèle. Plus la pénalisation est grande, moins il y a de caractéristiques présentes dans le modèle (car leurs poids sont nuls) et mieux le modèle peut être interprété.\nExemple avec Lasso\nNous allons prédire les locations de vélos en utilisant Lasso. Nous fixons le nombre de caractéristiques que nous voulons avoir dans le modèle à l’avance. Commençons d’abord par fixer le nombre à 2 caractéristiques :\n\n\n\nTable\n\n\nLes deux premières caractéristiques avec des poids non nuls dans le chemin Lasso sont la température (“temp”) et la tendance temporelle (“days_since_2011”).\nMaintenant, sélectionnons 5 caractéristiques :\n\n\n\nTable\n\n\nNotez que les poids pour “temp” et “days_since_2011” diffèrent du modèle avec deux caractéristiques. La raison en est qu’en diminuant lambda, même les caractéristiques qui sont déjà “dans” le modèle sont moins pénalisées et peuvent obtenir un poids absolu plus grand. L’interprétation des poids Lasso correspond à l’interprétation des poids dans le modèle de régression linéaire. Vous devez seulement faire attention à savoir si les caractéristiques sont standardisées ou non, car cela affecte les poids. Dans cet exemple, les caractéristiques ont été standardisées par le logiciel, mais les poids ont été automatiquement retransformés pour nous afin de correspondre aux échelles des caractéristiques originales.\nAutres méthodes pour la parcimonie dans les modèles linéaires\nUn large éventail de méthodes peut être utilisé pour réduire le nombre de caractéristiques dans un modèle linéaire.\nMéthodes de prétraitement :\n\nSélection manuelle des caractéristiques : Vous pouvez toujours utiliser des connaissances expertes pour sélectionner ou écarter certaines caractéristiques. Le grand inconvénient est que cela ne peut pas être automatisé et vous devez avoir accès à quelqu’un qui comprend les données.\nSélection univariée : Un exemple est le coefficient de corrélation. Vous ne considérez que les caractéristiques qui dépassent un certain seuil de corrélation entre la caractéristique et la cible. L’inconvénient est qu’il ne considère que les caractéristiques individuellement. Certaines caractéristiques peuvent ne pas montrer de corrélation avant que le modèle linéaire n’ait pris en compte d’autres caractéristiques. Vous manquerez celles-ci avec les méthodes de sélection univariée.\n\nMéthodes séquentielles :\n\nSélection progressive (Forward selection) : Ajustez le modèle linéaire avec une caractéristique. Faites cela avec chaque caractéristique. Sélectionnez le modèle qui fonctionne le mieux (par exemple, R-carré le plus élevé). Maintenant, à nouveau, pour les caractéristiques restantes, ajustez différentes versions de votre modèle en ajoutant chaque caractéristique à votre modèle actuel le meilleur. Sélectionnez celui qui offre les meilleures performances. Continuez jusqu’à ce qu’un critère soit atteint, comme le nombre maximum de caractéristiques dans le modèle.\nSélection régressive (Backward selection) : Similaire à la sélection progressive. Mais au lieu d’ajouter des caractéristiques, commencez avec le modèle qui contient toutes les caractéristiques et essayez de déterminer quelle caractéristique vous devez retirer pour obtenir l’augmentation de performance la plus élevée. Répétez cela jusqu’à ce qu’un critère d’arrêt soit atteint.\n\nJe recommande d’utiliser Lasso, car il peut être automatisé, prend en compte toutes les caractéristiques simultanément et peut être contrôlé via lambda. Il fonctionne également pour le modèle de régression logistique pour la classification.\n\n\n\n5.1.8 - Avantages\nLa modélisation des prédictions comme une somme pondérée rend transparente la manière dont les prédictions sont produites. Et avec Lasso, nous pouvons nous assurer que le nombre de caractéristiques utilisées reste petit.\nDe nombreuses personnes utilisent des modèles de régression linéaire. Cela signifie que dans de nombreux endroits, ils sont acceptés pour la modélisation prédictive et l’inférence. Il existe un haut niveau d’expérience et d’expertise collective, y compris des matériaux pédagogiques sur les modèles de régression linéaire et des implémentations logicielles. La régression linéaire peut être trouvée dans R, Python, Java, Julia, Scala, Javascript, …\nMathématiquement, il est simple d’estimer les poids et vous avez une garantie de trouver des poids optimaux (à condition que toutes les hypothèses du modèle de régression linéaire soient satisfaites par les données).\nAvec les poids, vous obtenez également des intervalles de confiance, des tests et une théorie statistique solide. Il existe également de nombreuses extensions du modèle de régression linéaire (voir chapitre sur GLM, GAM et plus).\n\n\n5.1.9 - Inconvénients\nLes modèles de régression linéaire ne peuvent représenter que des relations linéaires, c’est-à-dire une somme pondérée des caractéristiques d’entrée. Chaque non-linéarité ou interaction doit être créée manuellement et explicitement fournie au modèle comme une caractéristique d’entrée.\nLes modèles linéaires sont également souvent moins performants en termes de prédiction, car les relations qui peuvent être apprises sont très limitées et simplifient généralement de manière excessive la complexité de la réalité.\nL’interprétation d’un poids peut être contre-intuitive car elle dépend de toutes les autres caractéristiques. Une caractéristique ayant une forte corrélation positive avec le résultat \\(y\\) et une autre caractéristique peut obtenir un poids négatif dans le modèle linéaire, car, étant donné l’autre caractéristique corrélée, elle est négativement corrélée avec \\(y\\) dans l’espace multidimensionnel. Des caractéristiques complètement corrélées rendent même impossible de trouver une solution unique pour l’équation linéaire. Un exemple : Vous avez un modèle pour prédire la valeur d’une maison et vous avez des caractéristiques comme le nombre de pièces et la taille de la maison. La taille de la maison et le nombre de pièces sont fortement corrélés : plus une maison est grande, plus elle a de pièces. Si vous prenez ces deux caractéristiques dans un modèle linéaire, il se peut que la taille de la maison soit le meilleur prédicteur et obtienne un poids positif important. Le nombre de pièces peut finir par obtenir un poids négatif, car, à taille de maison égale, augmenter le nombre de pièces pourrait la rendre moins précieuse ou l’équation linéaire devient moins stable lorsque la corrélation est trop forte.",
    "crumbs": [
      "5 - Modèles interprétables",
      "5.1 - Régéression linéaire"
    ]
  },
  {
    "objectID": "05-interpretable_models/05.1-linear-regression.html#footnotes",
    "href": "05-interpretable_models/05.1-linear-regression.html#footnotes",
    "title": "Apprentissage automatique interprétable",
    "section": "Notes de bas de page",
    "text": "Notes de bas de page\n\n\nFriedman, Jerome, Trevor Hastie, and Robert Tibshirani. “The elements of statistical learning”. hastie.su.domains/ElemStatLearn (2009).↩︎",
    "crumbs": [
      "5 - Modèles interprétables",
      "5.1 - Régéression linéaire"
    ]
  },
  {
    "objectID": "05-interpretable_models/05.5-decision-rules.html",
    "href": "05-interpretable_models/05.5-decision-rules.html",
    "title": "Apprentissage automatique interprétable",
    "section": "",
    "text": "Avertissement\n\n\n\nEn cours de traduction.",
    "crumbs": [
      "5 - Modèles interprétables",
      "5.5 - Règles de décision"
    ]
  },
  {
    "objectID": "05-interpretable_models/05.5-decision-rules.html#règles-de-décision",
    "href": "05-interpretable_models/05.5-decision-rules.html#règles-de-décision",
    "title": "Apprentissage automatique interprétable",
    "section": "5.5 - Règles de décision",
    "text": "5.5 - Règles de décision\nUne règle de décision est une simple instruction SI-ALORS composée d’une condition (également appelée antécédent) et d’une prédiction. Par exemple : SI il pleut aujourd’hui ET si c’est avril (condition), ALORS il pleuvra demain (prédiction). Une seule règle de décision ou une combinaison de plusieurs règles peuvent être utilisées pour faire des prédictions.\nLes règles de décision suivent une structure générale : SI les conditions sont remplies ALORS faites une certaine prédiction. Les règles de décision sont probablement les modèles de prédiction les plus interprétables. Leur structure SI-ALORS ressemble sémantiquement au langage naturel et à notre façon de penser, à condition que la condition soit construite à partir de caractéristiques intelligibles, la longueur de la condition soit courte (petit nombre de paires caracteristique = valeur combinées avec un ET) et il n’y ait pas trop de règles. En programmation, il est très naturel d’écrire des règles SI-ALORS. La nouveauté de l’apprentissage automatique est que les règles de décision sont apprises via un algorithme.\nImaginez que vous utilisiez un algorithme pour apprendre des règles de décision permettant de prédire la valeur d’une maison ( basse, moyenne ou elevee). Une règle de décision apprise par ce modèle pourrait être la suivante : si la surface d’une maison mesure plus de 100 mètres carrés et possède un jardin, alors sa valeur est élevée. Plus formellement : SI surface &gt; 100 AND jardin = 1 ALORS valeur = elevee.\nDécomposons la règle de décision :\n\nsurface &gt; 100est la première condition de la partie SI.\njardin = 1 est la deuxième condition de la partie SI.\nLes deux conditions sont reliées par un « ET » pour créer une nouvelle condition. Les deux doivent être vrais pour que la règle s’applique.\nLe résultat prédit (PUIS-partie) est valeur = elevee.\n\nUne règle de décision utilise au moins une feature=valueinstruction dans la condition, sans limite supérieure quant au nombre d’instructions supplémentaires pouvant être ajoutées avec un « ET ». Une exception est la règle par défaut qui n’a pas de partie SI explicite et qui s’applique lorsqu’aucune autre règle ne s’applique, mais nous en parlerons plus tard.\nL’utilité d’une règle de décision se résume généralement en deux chiffres : support et précision.\nSupport ou couverture d’une règle : Le pourcentage d’instances auxquelles s’applique la condition d’une règle est appelé le support. Prenons par exemple la règle surface = grande ET location = bonne ALORS valeur = elevee de prédiction de la valeur des maisons. Supposons que 100 maisons sur 1 000 soient grandes et bien situées, alors le soutien à la règle est de 10%. La prédiction (PUIS-partie) n’est pas importante pour le calcul du support.\nPrécision ou confiance d’une règle : l’exactitude d’une règle est une mesure de la précision avec laquelle elle prédit la classe correcte pour les instances auxquelles la condition de la règle s’applique. Par exemple : disons que sur les 100 maisons où la règle surface = grande ET location = bonne ALORS valeur = elevee s’applique, 85 ont valeur = elevee, 14 ont valeur = moyenne et 1 a valeur = basse, alors l’exactitude de la règle est de 85%.\nIl existe généralement un compromis entre précision et support : en ajoutant plus de fonctionnalités à la condition, nous pouvons obtenir une plus grande précision, mais perdre le support.\nPour créer un bon classificateur permettant de prédire la valeur d’une maison, vous devrez peut-être apprendre non seulement une règle, mais peut-être 10 ou 20. Les choses peuvent alors devenir plus compliquées et vous pouvez rencontrer l’un des problèmes suivants :\n\nLes règles peuvent se chevaucher : que se passe-t-il si je veux prédire la valeur d’une maison et que deux règles ou plus s’appliquent et qu’elles me donnent des prédictions contradictoires ?\nAucune règle ne s’applique : que se passe-t-il si je souhaite prédire la valeur d’une maison et qu’aucune des règles ne s’applique ?\n\nIl existe deux stratégies principales pour combiner plusieurs règles : les listes de décisions (ordonnées) et les ensembles de décisions (non ordonnés). Les deux stratégies impliquent des solutions différentes au problème du chevauchement des règles.\nUne liste de décisions introduit un ordre dans les règles de décision. Si la condition de la première règle est vraie pour une instance, nous utilisons la prédiction de la première règle. Sinon, nous passons à la règle suivante et vérifions si elle s’applique et ainsi de suite. Les listes de décisions résolvent le problème des règles qui se chevauchent en renvoyant uniquement la prédiction de la première règle de la liste qui s’applique.\nUn ensemble de décisions ressemble à une démocratie de règles, sauf que certaines règles peuvent avoir un pouvoir de vote plus élevé. Dans un ensemble, soit les règles s’excluent mutuellement, soit il existe une stratégie pour résoudre les conflits, comme le vote majoritaire, qui peut être pondérée par la précision des règles individuelles ou d’autres mesures de qualité. L’interprétabilité souffre potentiellement lorsque plusieurs règles s’appliquent.\nLes listes et les ensembles de décisions peuvent souffrir du problème selon lequel aucune règle ne s’applique à une instance. Ce problème peut être résolu en introduisant une règle par défaut. La règle par défaut est la règle qui s’applique lorsqu’aucune autre règle ne s’applique. La prédiction de la règle par défaut constitue souvent la classe la plus fréquente de points de données qui ne sont pas couverts par d’autres règles. Si un ensemble ou une liste de règles couvre tout l’espace des fonctionnalités, nous le qualifions d’exhaustif. En ajoutant une règle par défaut, un ensemble ou une liste devient automatiquement exhaustif.\nIl existe de nombreuses façons d’apprendre des règles à partir de données et ce livre est loin de toutes les couvrir. Ce chapitre vous en montre trois. Les algorithmes sont choisis pour couvrir un large éventail d’idées générales d’apprentissage des règles, ils représentent donc tous les trois des approches très différentes.\n\nOneR apprend les règles à partir d’une seule fonctionnalité. OneR se caractérise par sa simplicité, son interprétabilité et son utilisation comme benchmark.\nLa couverture séquentielle est une procédure générale qui apprend de manière itérative les règles et supprime les points de données couverts par la nouvelle règle. Cette procédure est utilisée par de nombreux algorithmes d’apprentissage de règles.\nLes listes de règles bayésiennes combinent des modèles fréquents pré-exploités dans une liste de décisions utilisant des statistiques bayésiennes. L’utilisation de modèles pré-exploités est une approche courante utilisée par de nombreux algorithmes d’apprentissage de règles.\n\nCommençons par l’approche la plus simple : utiliser la meilleure fonctionnalité pour apprendre les règles.\n\n5.5.1 - Learn Rules from a Single Feature (OneR)\nL’algorithme OneR proposé par Holte (1993)1 est l’un des algorithmes d’induction de règles les plus simples. Parmi toutes les fonctionnalités, OneR sélectionne celle qui contient le plus d’informations sur le résultat qui l’intéresse et crée des règles de décision à partir de cette fonctionnalité.\nMalgré son nom OneR, qui signifie « One Rule », l’algorithme génère plus d’une règle : il s’agit en fait d’une règle par valeur de caractéristique unique de la meilleure fonctionnalité sélectionnée. Un meilleur nom serait OneFeatureRules.\nL’algorithme est simple et rapide :\n\nDiscrétisez les fonctionnalités continues en choisissant des intervalles appropriés.\nPour chaque fonctionnalité :\n\nCréez un tableau croisé entre les valeurs des caractéristiques et le résultat (catégorique).\nPour chaque valeur de la fonctionnalité, créez une règle qui prédit la classe la plus fréquente des instances qui ont cette valeur de fonctionnalité particulière (peut être lue à partir du tableau croisé).\nCalculez l’erreur totale des règles pour la fonctionnalité.\n\nSélectionnez la fonctionnalité avec la plus petite erreur totale.\n\nOneR couvre toujours toutes les instances de l’ensemble de données, car il utilise tous les niveaux de l’entité sélectionnée. Les valeurs manquantes peuvent être soit traitées comme une valeur de caractéristique supplémentaire, soit imputées au préalable.\nUn modèle OneR est un arbre de décision avec une seule division. La répartition n’est pas nécessairement binaire comme dans CART, mais dépend du nombre de valeurs de caractéristiques uniques.\nRegardons un exemple de la façon dont la meilleure fonctionnalité est choisie par OneR. Le tableau suivant présente un ensemble de données artificielles sur les maisons avec des informations sur leur valeur, leur emplacement, leur taille et si les animaux sont autorisés. Nous souhaitons apprendre un modèle simple pour prédire la valeur d’une maison.\n\nOneR crée les tableaux croisés entre chaque fonctionnalité et le résultat :\n\nPour chaque fonctionnalité, nous parcourons le tableau ligne par ligne : chaque valeur de fonctionnalité est la partie SI d’une règle ; la classe la plus courante pour les instances avec cette valeur de caractéristique est la prédiction, la partie ALORS de la règle. Par exemple, la fonction de taille avec les niveaux basse, moyenne et elevee donne trois règles. Pour chaque fonctionnalité, nous calculons le taux d’erreur total des règles générées, qui est la somme des erreurs. La fonctionnalité de localisation a les valeurs possibles mauvaise et bonne. La valeur la plus fréquente pour les maisons situées dans de mauvais emplacements est basse et lorsque nous l’utilisons comme prédiction, nous faisons deux erreurs, car deux maisons ont une valeur moyenne. La valeur prédite des maisons bien situées est égale à elevee et encore une fois, nous commettons deux erreurs, car deux maisons ont une valeur moyenne. L’erreur que nous faisons en utilisant la fonction de localisation est de 4/10, pour la fonction de taille, elle est de 3/10 et pour la fonction d’animal de compagnie, elle est de 4/10. La fonctionnalité de taille produit les règles avec l’erreur la plus faible et sera utilisée pour le modèle OneR final :\nSI size = petite ALORS value = basse SI size = moyenne ALORS value = moyenne SI size = grance ALORS value = elevee\nOneR préfère les fonctionnalités avec de nombreux niveaux possibles, car ces fonctionnalités peuvent plus facilement suradapter la cible. Imaginez un ensemble de données qui ne contient que du bruit et aucun signal, ce qui signifie que toutes les caractéristiques prennent des valeurs aléatoires et n’ont aucune valeur prédictive pour la cible. Certaines fonctionnalités ont plus de niveaux que d’autres. Les fonctionnalités avec plus de niveaux peuvent désormais être surajustées plus facilement. Une fonctionnalité qui a un niveau distinct pour chaque instance des données prédirait parfaitement l’ensemble de l’ensemble de données d’entraînement. Une solution serait de diviser les données en ensembles de formation et de validation, d’apprendre les règles sur les données de formation et d’évaluer l’erreur totale de choix de la fonctionnalité sur l’ensemble de validation.\nLes égalités sont un autre problème, c’est-à-dire lorsque deux caractéristiques aboutissent à la même erreur totale. OneR résout les égalités en prenant soit la première caractéristique avec l’erreur la plus faible, soit celle avec la valeur p la plus basse d’un test du chi carré.\nExemple\nEssayons OneR avec des données réelles. Nous utilisons la tâche de classification du cancer du col de l’utérus pour tester l’algorithme OneR. Toutes les caractéristiques d’entrée continue ont été discrétisées en leurs 5 quantiles. Les règles suivantes sont créées :\n\nLa fonctionnalité âge a été choisie par OneR comme la meilleure fonctionnalité prédictive. Puisque le cancer est rare, pour chaque règle, la classe majoritaire et donc l’étiquette prédite sont toujours en bonne santé, ce qui est plutôt inutile. Cela n’a pas de sens d’utiliser la prédiction d’étiquette dans ce cas déséquilibré. Le tableau croisé entre les intervalles « Âge » et Cancer/En bonne santé ainsi que le pourcentage de femmes atteintes de cancer est plus informatif :\n\nMais avant de commencer à interpréter quoi que ce soit : étant donné que la prédiction pour chaque fonctionnalité et chaque valeur est saine, le taux d’erreur total est le même pour toutes les fonctionnalités. Les liens dans l’erreur totale sont, par défaut, résolus en utilisant la première fonctionnalité parmi celles avec les taux d’erreur les plus bas (ici, toutes les fonctionnalités ont 55/858), qui se trouve être la fonctionnalité Âge.\nOneR ne prend pas en charge les tâches de régression. Mais nous pouvons transformer une tâche de régression en tâche de classification en découpant le résultat continu en intervalles. Nous utilisons cette astuce pour prédire le nombre de vélos loués avec OneR en découpant le nombre de vélos en quatre quartiles (0-25%, 25-50%, 50-75% et 75-100%). Le tableau suivant montre la fonctionnalité sélectionnée après l’ajustement du modèle OneR :\n\nLa fonctionnalité sélectionnée est le mois. La fonctionnalité mensuelle a (surprise !) 12 niveaux de fonctionnalités, ce qui est plus que la plupart des autres fonctionnalités. Il existe donc un risque de surapprentissage. Du côté le plus optimiste : la fonction mensuelle peut gérer la tendance saisonnière (par exemple moins de vélos loués en hiver) et les prévisions semblent raisonnables.\nNous passons maintenant du simple algorithme OneR à une procédure plus complexe utilisant des règles avec des conditions plus complexes composées de plusieurs fonctionnalités : Sequential Covering.\n\n\n5.5.2 - Sequential Covering\nLa couverture séquentielle est une procédure générale qui apprend de manière répétée une seule règle pour créer une liste de décisions (ou un ensemble) qui couvre l’intégralité de l’ensemble de données, règle par règle. De nombreux algorithmes d’apprentissage de règles sont des variantes de l’algorithme de couverture séquentielle. Ce chapitre présente la recette principale et utilise RIPPER, une variante de l’algorithme de couverture séquentielle pour les exemples.\nL’idée est simple : tout d’abord, trouvez une bonne règle qui s’applique à certains points de données. Supprimez tous les points de données couverts par la règle. Un point de données est couvert lorsque les conditions s’appliquent, que les points soient classés correctement ou non. Répétez l’apprentissage des règles et la suppression des points couverts avec les points restants jusqu’à ce qu’il ne reste plus de points ou qu’une autre condition d’arrêt soit remplie. Le résultat est une liste de décisions. Cette approche d’apprentissage répété des règles et de suppression des points de données couverts est appelée « séparer et conquérir ».\nSupposons que nous disposions déjà d’un algorithme capable de créer une règle unique couvrant une partie des données. L’algorithme de couverture séquentielle pour deux classes (une positive, une négative) fonctionne comme ceci :\n\nCommencez avec une liste vide de règles (rlist).\nApprenez une règle r.\nBien que la liste des règles soit inférieure à un certain seuil de qualité (ou que les exemples positifs ne soient pas encore couverts) :\n\nAjoutez la règle r à rlist.\nSupprimez tous les points de données couverts par la règle r.\nApprenez une autre règle sur les données restantes.\n\nRenvoyez la liste de décisions.\n\n\n\n\nL’algorithme de couverture fonctionne en couvrant séquentiellement l’espace des fonctionnalités avec des règles uniques et en supprimant les points de données déjà couverts par ces règles. À des fins de visualisation, les fonctionnalités x1 et x2 sont continues, mais la plupart des algorithmes d’apprentissage de règles nécessitent des fonctionnalités catégorielles.\n\n\nPar exemple : nous avons une tâche et un ensemble de données pour prédire les valeurs des maisons en fonction de leur taille, de leur emplacement et si les animaux sont autorisés. Nous apprenons la première règle, qui s’avère être : Si size = biget location = good, alors value = high. Ensuite, nous supprimons de l’ensemble de données toutes les grandes maisons bien situées. Avec les données restantes, nous apprenons la règle suivante. Peut-être : si location = good, alors value = medium. Notez que cette règle est apprise sur des données sans grandes maisons bien situées, ne laissant que des maisons moyennes et petites bien situées.\nPour les paramètres multi-classes, l’approche doit être modifiée. Premièrement, les classes sont classées par prévalence croissante. L’algorithme de couverture séquentielle commence par la classe la moins commune, apprend une règle pour celle-ci, supprime toutes les instances couvertes, puis passe à la deuxième classe la moins commune et ainsi de suite. La classe actuelle est toujours traitée comme la classe positive et toutes les classes ayant une prévalence plus élevée sont regroupées dans la classe négative. La dernière classe est la règle par défaut. C’est ce qu’on appelle également la stratégie du un contre tous en matière de classification.\nComment apprend-on une seule règle ? L’algorithme OneR serait inutile ici, car il couvrirait toujours tout l’espace des fonctionnalités. Mais il existe bien d’autres possibilités. Une possibilité consiste à apprendre une seule règle à partir d’un arbre de décision avec recherche de faisceau :\n\nApprenez un arbre de décision (avec CART ou un autre algorithme d’apprentissage d’arbre).\nCommencez par le nœud racine et sélectionnez de manière récursive le nœud le plus pur (par exemple avec le taux d’erreurs de classification le plus bas).\nLa classe majoritaire du nœud terminal est utilisée comme prédiction de règle ; le chemin menant à ce nœud est utilisé comme condition de règle.\n\nLa figure suivante illustre la recherche de poutres dans un arbre :\n\n\n\nApprendre une règle en recherchant un chemin dans un arbre de décision. Un arbre de décision est développé pour prédire la cible d’intérêt. Nous commençons au nœud racine, suivons goulûment et itérativement le chemin qui produit localement le sous-ensemble le plus pur (par exemple la plus haute précision) et ajoutons toutes les valeurs divisées à la condition de règle. On obtient : Si location = good et size = big, alors value=high.\n\n\nL’apprentissage d’une seule règle est un problème de recherche, où l’espace de recherche est l’espace de toutes les règles possibles. Le but de la recherche est de trouver la meilleure règle selon certains critères. Il existe de nombreuses stratégies de recherche différentes : recherche par ascension, recherche par faisceau, recherche exhaustive, recherche par le meilleur premier, recherche ordonnée, recherche stochastique, recherche descendante, recherche ascendante, …\nRIPPER (Repeated Incremental Pruning to Produce Error Reduction) de Cohen (1995)2 est une variante de l’algorithme Sequential Covering. RIPPER est un peu plus sophistiqué et utilise une phase de post-traitement (élagage des règles) pour optimiser la liste (ou l’ensemble) de décisions. RIPPER peut fonctionner en mode ordonné ou non et générer soit une liste de décisions, soit un ensemble de décisions.\nExemples\nNous utiliserons RIPPER pour les exemples.\nL’algorithme RIPPER ne trouve aucune règle dans la tâche de classification du cancer du col de l’utérus.\nLorsque nous utilisons RIPPER sur la tâche de régression pour prédire le nombre de vélos, certaines règles sont trouvées. Puisque RIPPER ne fonctionne que pour la classification, le décompte des vélos doit être transformé en un résultat catégorique. J’y suis parvenu en réduisant le nombre de vélos en quartiles. Par exemple (4548, 5956) est l’intervalle couvrant le nombre de vélos prévu entre 4548 et 5956. Le tableau suivant montre la liste de décision des règles apprises.\n\nL’interprétation est simple : si les conditions sont réunies, nous prédisons l’intervalle sur la droite pour le nombre de vélos. La dernière règle est la règle par défaut qui s’applique lorsqu’aucune des autres règles ne s’applique à une instance. Pour prédire une nouvelle instance, commencez par le haut de la liste et vérifiez si une règle s’applique. Lorsqu’une condition correspond, le côté droit de la règle correspond à la prédiction pour cette instance. La règle par défaut garantit qu’il y a toujours une prédiction.\n\n\n5.5.3 - Bayesian Rule Lists\nDans cette section, je vais vous montrer une autre approche pour apprendre une liste de décisions, qui suit cette recette approximative :\n\nPré-exploitez les modèles fréquents à partir des données qui peuvent être utilisés comme conditions pour les règles de décision.\nApprenez une liste de décisions à partir d’une sélection de règles prédéfinies.\n\nUne approche spécifique utilisant cette recette est appelée Bayesian Rule Lists (Letham et al., 2015)3 ou BRL en abrégé. BRL utilise les statistiques bayésiennes pour apprendre des listes de décisions à partir de modèles fréquents pré-exploités avec l’algorithme FP-tree (Borgelt 2005)4.\nMais commençons lentement par la première étape du BRL.\nPré-exploration de modèles fréquents\nUn modèle fréquent est la (co-)occurrence fréquente de valeurs de caractéristiques. En tant qu’étape de prétraitement pour l’algorithme BRL, nous utilisons les fonctionnalités (nous n’avons pas besoin du résultat cible dans cette étape) et en extrayons des modèles fréquents. Un modèle peut être une valeur de caractéristique unique telle que size=mediumou une combinaison de valeurs de caractéristiques telle que size = medium ET location = bad.\nLa fréquence d’un motif est mesurée avec son support dans l’ensemble de données :\n\\[Support(x_j=A)=\\frac{1}n{}\\sum_{i=1}^nI(x^{(i)}_{j}=A)\\]\noù \\(A\\) est la valeur de la caractéristique, \\(n\\) le nombre de points de données dans l’ensemble de données et \\(I\\) la fonction indicatrice qui renvoie 1 si la caractéristique \\(X_j\\) de l’instance \\(i\\) a le niveau \\(A\\) sinon 0. Dans un ensemble de données de valeurs de maison, si 20% des maisons n’ont pas de balcon et 80% en ont un ou plusieurs, alors la prise en charge du modèle balcony = 0est de 20%. La prise en charge peut également être mesurée pour des combinaisons de valeurs de fonctionnalités, par exemple pour balcony = 0 ET pets = allowed.\nIl existe de nombreux algorithmes pour trouver des modèles aussi fréquents, par exemple Apriori ou FP-Growth. Ce que vous utilisez n’a pas beaucoup d’importance, seule la vitesse à laquelle les motifs sont trouvés est différente, mais les motifs résultants sont toujours les mêmes.\nJe vais vous donner une idée générale du fonctionnement de l’algorithme Apriori pour trouver des modèles fréquents. En fait, l’algorithme Apriori se compose de deux parties, la première partie trouvant des modèles fréquents et la seconde partie construisant des règles d’association à partir d’elles. Pour l’algorithme BRL, nous nous intéressons uniquement aux modèles fréquents générés dans la première partie d’Apriori.\nDans la première étape, l’algorithme Apriori commence avec toutes les valeurs de caractéristiques qui ont un support supérieur au support minimum défini par l’utilisateur. Si l’utilisateur dit que le support minimum devrait être de 10% et que seulement 5% des maisons en ont size = big, nous supprimerions cette valeur de fonctionnalité et conserverions uniquement size = medium et size = smallen tant que modèles. Cela ne signifie pas que les maisons sont supprimées des données, cela signifie simplement qu’elles size = big ne sont pas renvoyées selon un modèle fréquent. Basé sur des modèles fréquents avec une seule valeur de caractéristique, l’algorithme Apriori tente de manière itérative de trouver des combinaisons de valeurs de caractéristiques d’ordre de plus en plus élevé. Les modèles sont construits en combinant feature=value des instructions avec un ET logique, par exemple size = medium ET location = bad. Les modèles générés avec un support inférieur au support minimum sont supprimés. Au final, nous avons tous les schémas fréquents. Tout sous-ensemble de clauses d’un modèle fréquent est à nouveau fréquent, ce que l’on appelle la propriété Apriori. Cela a du sens intuitivement : en supprimant une condition d’un modèle, le modèle réduit ne peut couvrir que plus ou le même nombre de points de données, mais pas moins. Par exemple, si 20% des maisons le sont size = medium ET location = good, alors le soutien aux maisons qui le sont seulement size = medium est de 20% ou plus. La propriété Apriori est utilisée pour réduire le nombre de motifs à inspecter. Ce n’est que dans le cas de modèles fréquents que nous devons vérifier les modèles d’ordre supérieur.\nNous en avons maintenant terminé avec les conditions préalables à l’exploration pour l’algorithme de liste de règles bayésiennes. Mais avant de passer à la deuxième étape du BRL, je voudrais faire allusion à une autre manière d’apprendre des règles basée sur des modèles pré-exploités. D’autres approches suggèrent d’inclure le résultat d’intérêt dans le processus fréquent d’exploration de modèles et également d’exécuter la deuxième partie de l’algorithme Apriori qui construit les règles SI-ALORS. Puisque l’algorithme n’est pas supervisé, la partie ALORS contient également des valeurs de caractéristiques qui ne nous intéressent pas. Mais nous pouvons filtrer par règles qui n’ont que le résultat qui nous intéresse dans la partie ALORS. Ces règles forment déjà un ensemble de décisions, mais il serait également possible d’arranger, d’élaguer, de supprimer ou de recombiner les règles.\nCependant, dans l’approche BRL, nous travaillons avec les modèles fréquents et apprenons la partie ALORS et comment organiser les modèles dans une liste de décision à l’aide des statistiques bayésiennes.\nApprentissage des listes de règles bayésiennes\nLe but de l’algorithme BRL est d’apprendre une liste de décisions précise en utilisant une sélection de conditions pré-minées, tout en donnant la priorité aux listes avec peu de règles et des conditions courtes. BRL répond à cet objectif en définissant une distribution de listes de décisions avec des distributions préalables pour la longueur des conditions (de préférence des règles plus courtes) et le nombre de règles (de préférence une liste plus courte).\nLa distribution de probabilité a posteriori des listes permet de déterminer la probabilité d’une liste de décision, compte tenu des hypothèses de brièveté et de l’adéquation de la liste aux données. Notre objectif est de trouver la liste qui maximise cette probabilité a posteriori. Puisqu’il n’est pas possible de trouver la meilleure liste exacte directement à partir des distributions de listes, BRL suggère la recette suivante : 1. Générer une liste de décision initiale, tirée aléatoirement de la distribution a priori. 1. Modifier la liste de manière itérative en ajoutant, changeant ou supprimant des règles, en veillant à ce que les listes résultantes suivent la distribution postérieure des listes. 1. Sélectionnez la liste de décision parmi les listes échantillonnées avec la probabilité la plus élevée selon la distribution a posteriori.\nExaminons l’algorithme de plus près : l’algorithme commence par des modèles de valeur de caractéristique de pré-exploration avec l’algorithme FP-Growth. BRL fait un certain nombre d’hypothèses sur la distribution de la cible et la distribution des paramètres qui définissent la distribution de la cible. (C’est une statistique bayésienne.) Si vous n’êtes pas familier avec les statistiques bayésiennes, ne vous laissez pas trop emporter par les explications suivantes. Il est important de savoir que l’approche bayésienne est un moyen de combiner les connaissances ou exigences existantes (appelées distributions a priori) tout en s’adaptant aux données. Dans le cas des listes de décisions, l’approche bayésienne est logique, puisque les hypothèses préalables incitent les listes de décisions à être courtes avec des règles courtes.\nLe but est d’échantillonner les listes de décisions d à partir de la distribution a posteriori :\n\\[\\underbrace{p(d|x,y,A,\\alpha,\\lambda,\\eta)}_{posteriori}\\propto\\underbrace{p(y|x,d,\\alpha)}_{likelihood}\\cdot\\underbrace{p(d|A,\\lambda,\\eta)}_{priori}\\]\noù \\(d\\) est une liste de décisions, \\(x\\) sont les caractéristiques, \\(y\\) est la cible, \\(A\\) l’ensemble des conditions pré-exploitées, \\(\\lambda\\) la longueur prévue antérieurement des listes de décisions, \\(\\eta\\) le nombre de conditions attendu auparavant dans une règle, \\(\\alpha\\) le pseudo-compte antérieur pour les classes positives et négatives qui est mieux fixé à (1,1).\n\\[p(d|x,y,A,\\alpha,\\lambda,\\eta)\\]\nquantifie la probabilité d’une liste de décisions, compte tenu des données observées et des hypothèses priori. Ceci est proportionnel à la probabilité du résultat y compte tenu de la liste de décisions et des données multipliée par la probabilité de la liste compte tenu des hypothèses préalables et des conditions préalables.\n\\[p(y|x,d,\\alpha)\\]\nest la probabilité du y observé, compte tenu de la liste de décisions et des données. BRL suppose que y est généré par une distribution Dirichlet-Multinomiale. Plus la liste de décisions d explique bien les données, plus la probabilité est élevée.\n\\[p(d|A,\\lambda,\\eta)\\]\nest la distribution préalable des listes de décisions. Il combine multiplicativement une distribution de Poisson tronquée (paramètre \\(\\lambda\\)) pour le nombre de règles dans la liste et une distribution de Poisson tronquée (paramètre \\(\\eta\\)) pour le nombre de valeurs de caractéristiques dans les conditions des règles.\nUne liste de décisions a une probabilité a posteriori élevée si elle explique bien le résultat et est également probable selon les hypothèses antérieures.\nLes estimations dans les statistiques bayésiennes sont toujours un peu délicates, car nous ne pouvons généralement pas calculer directement la bonne réponse, mais nous devons sélectionner des candidats, les évaluer et mettre à jour nos estimations a posteriori en utilisant la méthode de Monte Carlo par chaîne de Markov. Pour les listes de décisions, c’est encore plus délicat, car il faut s’inspirer de la répartition des listes de décisions. Les auteurs du BRL proposent d’abord de dessiner une liste de décisions initiale, puis de la modifier de manière itérative pour générer des échantillons de listes de décisions à partir de la distribution postérieure des listes (une chaîne de Markov de listes de décisions). Les résultats sont potentiellement dépendants de la liste de décision initiale, il est donc conseillé de répéter cette procédure pour garantir une grande variété de listes. La valeur par défaut dans l’implémentation du logiciel est 10 fois. La recette suivante nous indique comment dresser une liste de décisions initiale :\n\nModèles de pré-exploitation avec FP-Growth.\nÉchantillonnez le paramètre de longueur de liste m à partir d’une distribution de Poisson tronquée.\nPour la règle par défaut : échantillonnez le paramètre de distribution Dirichlet-Multinomial \\(\\theta_0\\) de la valeur cible (c’est-à-dire la règle qui s’applique lorsque rien d’autre ne s’applique).\nPour la règle de liste de décision j=1,…,m, faites :\n\nÉchantillonnez le paramètre de longueur de règle \\(l\\) (nombre de conditions) pour la règle \\(j\\).\nÉchantillonner une condition de longueur \\(l_j\\) des conditions pré-exploitées.\nÉchantillonnez le paramètre de distribution Dirichlet-Multinomial pour la partie ALORS (c’est-à-dire pour la distribution du résultat cible compte tenu de la règle)\n\nPour chaque observation de l’ensemble de données :\n\nRecherchez la règle dans la liste de décisions qui s’applique en premier (de haut en bas).\nDessinez le résultat prédit à partir de la distribution de probabilité (binomiale) suggérée par la règle qui s’applique.\n\n\nL’étape suivante consiste à générer de nombreuses nouvelles listes à partir de cet échantillon initial pour obtenir de nombreux échantillons à partir de la distribution a posteriori des listes de décision.\nLes nouvelles listes de décisions sont échantillonnées en commençant par la liste initiale, puis en déplaçant aléatoirement une règle vers une position différente dans la liste ou en ajoutant une règle à la liste de décisions actuelle à partir des conditions prédéfinies ou en supprimant une règle de la liste de décisions. La règle qui est modifiée, ajoutée ou supprimée est choisie au hasard. A chaque étape, l’algorithme évalue la probabilité a posteriori de la liste de décision (mélange de précision et de brièveté). L’algorithme Metropolis Hastings garantit que nous échantillonnons des listes de décisions qui ont une probabilité a posteriori élevée. Cette procédure nous fournit de nombreux échantillons issus de la distribution des listes de décisions. L’algorithme BRL sélectionne la liste de décision des échantillons avec la probabilité a posteriori la plus élevée.\nExemples\nVoilà pour la théorie, voyons maintenant la méthode BRL en action. Les exemples utilisent une variante plus rapide de BRL appelée Scalable Bayesian Rule Lists (SBRL) par Yang et al. (2017)5. Nous utilisons l’algorithme SBRL pour prédire le risque de cancer du col de l’utérus. J’ai d’abord dû discrétiser toutes les fonctionnalités d’entrée pour que l’algorithme SBRL fonctionne. À cette fin, j’ai regroupé les caractéristiques continues en fonction de la fréquence des valeurs par quantiles.\nNous obtenons les règles suivantes :\n\nNotez que nous obtenons des règles raisonnables, puisque la prédiction sur la partie ALORS n’est pas le résultat de la classe, mais la probabilité prédite de cancer.\nLes conditions ont été sélectionnées à partir de modèles pré-exploités avec l’algorithme FP-Growth. Le tableau suivant affiche l’ensemble des conditions parmi lesquelles l’algorithme SBRL peut choisir pour créer une liste de décisions. Le nombre maximum de valeurs de fonctionnalités dans une condition que j’autorisais en tant qu’utilisateur était de deux. Voici un échantillon de dix modèles :\n\nEnsuite, nous appliquons l’algorithme SBRL à la tâche de prédiction de location de vélos. Cela ne fonctionne que si le problème de régression consistant à prédire le nombre de vélos est converti en une tâche de classification binaire. J’ai arbitrairement créé une tâche de classification en créant une étiquette qui vaut 1 si le nombre de vélos dépasse 4000 vélos par jour, sinon 0.\nLa liste suivante a été apprise par SBRL :\n\nPrévoyons la probabilité que le nombre de vélos dépasse 4 000 par jour en 2012 avec une température de 17 degrés Celsius. La première règle ne s’applique pas, car elle ne s’applique qu’aux jours de 2011. La deuxième règle s’applique, car le jour est en 2012 et 17 degrés se situent dans l’intervalle \\([7.35 , 19.9)\\). Notre prédiction concernant la probabilité que plus de 4 000 vélos soient loués est de 88%.\n\n\n5.5.4 - Avantages\nCette section traite des avantages des règles SI-ALORS en général.\nLes règles SI-ALORS sont faciles à interpréter. Ce sont probablement les modèles interprétables les plus interprétables. Cette affirmation ne s’applique que si le nombre de règles est petit, les conditions des règles sont courtes (maximum 3 je dirais) et si les règles sont organisées dans une liste de décisions ou un ensemble de décisions qui ne se chevauchent pas.\nLes règles de décision peuvent être aussi expressives que les arbres de décision, tout en étant plus compactes. Les arbres de décision souffrent souvent également de sous-arbres répliqués, c’est-à-dire lorsque les divisions dans un nœud enfant gauche et droit ont la même structure.\nLa prédiction avec les règles SI-ALORS est rapide, puisque seules quelques instructions binaires doivent être vérifiées pour déterminer quelles règles s’appliquent.\nLes règles de décision sont robustes aux transformations monotones des caractéristiques d’entrée, car seul le seuil des conditions change. Ils sont également robustes face aux valeurs aberrantes, car il importe uniquement de savoir si une condition s’applique ou non.\nLes règles SI-ALORS génèrent généralement des modèles clairsemés, ce qui signifie que peu de fonctionnalités sont incluses. Ils sélectionnent uniquement les fonctionnalités pertinentes pour le modèle. Par exemple, un modèle linéaire attribue par défaut un poids à chaque entité en entrée. Les fonctionnalités qui ne sont pas pertinentes peuvent simplement être ignorées par les règles SI-ALORS.\nDes règles simples comme celles de OneR peuvent être utilisées comme base pour des algorithmes plus complexes.\n\n\n5.5.5 - Inconvénients\nCette section traite des inconvénients des règles SI-ALORS en général.\nLa recherche et la littérature sur les règles SI-ALORS se concentrent sur la classification et négligent presque complètement la régression. Même si vous pouvez toujours diviser une cible continue en intervalles et en faire un problème de classification, vous perdez toujours des informations. En général, les approches sont plus attractives si elles peuvent être utilisées à la fois pour la régression et la classification.\nSouvent, les caractéristiques doivent également être catégoriques. Cela signifie que les fonctionnalités numériques doivent être catégorisées si vous souhaitez les utiliser. Il existe de nombreuses façons de découper une caractéristique continue en intervalles, mais cela n’est pas anodin et s’accompagne de nombreuses questions sans réponses claires. En combien d’intervalles la fonctionnalité doit-elle être divisée ? Quels sont les critères de fractionnement : longueurs d’intervalle fixes, quantiles ou autre ? La catégorisation des fonctionnalités continues est une question non triviale qui est souvent négligée et les gens utilisent simplement la meilleure méthode suivante (comme je l’ai fait dans les exemples).\nLa plupart des anciens algorithmes d’apprentissage de règles sont sujets au surapprentissage. Les algorithmes présentés ici ont tous au moins quelques garde-fous pour éviter le surajustement : OneR est limité car il ne peut utiliser qu’une seule fonctionnalité (uniquement problématique si la fonctionnalité a trop de niveaux ou s’il y a plusieurs fonctionnalités, ce qui équivaut au problème de tests multiples), RIPPER effectue l’élagage et les listes de règles bayésiennes imposent une distribution préalable sur les listes de décision.\nLes règles de décision ne parviennent pas à décrire les relations linéaires entre les caractéristiques et la sortie. C’est un problème qu’ils partagent avec les arbres de décision. Les arbres de décision et les règles ne peuvent produire que des fonctions de prédiction de type étape par étape, dans lesquelles les changements dans la prédiction sont toujours des étapes discrètes et jamais des courbes lisses. Ceci est lié au problème selon lequel les entrées doivent être catégoriques. Dans les arbres de décision, ils sont implicitement catégorisés en les divisant.\n\n\n5.5.6 - Logiciels et alternatives\nOneR est implémenté dans le package R OneR, qui a été utilisé pour les exemples de ce livre. OneR est également implémenté dans la bibliothèque de machine learning Weka et à ce titre disponible en Java, R et Python. RIPPER est également implémenté dans Weka. Pour les exemples, j’ai utilisé l’implémentation R de JRIP dans le package RWeka. SBRL est disponible sous forme de package R (que j’ai utilisé pour les exemples), en Python ou en implémentation C. De plus, je recommande le package imodels, qui implémente des modèles basés sur des règles tels que des listes de règles bayésiennes, CORELS, OneR, des listes de règles gourmandes, etc. dans un package Python avec une interface scikit-learn unifiée.\nJe n’essaierai même pas de lister toutes les alternatives pour apprendre des ensembles et des listes de règles de décision, mais je soulignerai quelques travaux de synthèse. Je recommande le livre « Foundations of Rule Learning » de Fuernkranz et al. (2012)6. Il s’agit d’un travail approfondi sur les règles d’apprentissage, pour ceux qui souhaitent approfondir le sujet. Il fournit un cadre holistique pour réfléchir à l’apprentissage des règles et présente de nombreux algorithmes d’apprentissage des règles. Je recommande également de consulter les apprenants de règles Weka, qui implémentent RIPPER, M5Rules, OneR, PART et bien d’autres. Les règles SI-ALORS peuvent être utilisées dans des modèles linéaires comme décrit dans ce livre dans le chapitre sur l’algorithme RuleFit.",
    "crumbs": [
      "5 - Modèles interprétables",
      "5.5 - Règles de décision"
    ]
  },
  {
    "objectID": "05-interpretable_models/05.5-decision-rules.html#footnotes",
    "href": "05-interpretable_models/05.5-decision-rules.html#footnotes",
    "title": "Apprentissage automatique interprétable",
    "section": "Notes de bas de page",
    "text": "Notes de bas de page\n\n\nHolte, Robert C. “Very simple classification rules perform well on most commonly used datasets.” Machine learning 11.1 (1993): 63-90.↩︎\nCohen, William W. “Fast effective rule induction.” Machine Learning Proceedings (1995). 115-123.↩︎\nLetham, Benjamin, Cynthia Rudin, Tyler H. McCormick, and David Madigan. “Interpretable classifiers using rules and Bayesian analysis: Building a better stroke prediction model.” The Annals of Applied Statistics 9, no. 3 (2015): 1350-1371.↩︎\nBorgelt, C. “An implementation of the FP-growth algorithm.” Proceedings of the 1st International Workshop on Open Source Data Mining Frequent Pattern Mining Implementations - OSDM ’05, 1–5. http://doi.org/10.1145/1133905.1133907 (2005).↩︎\nYang, Hongyu, Cynthia Rudin, and Margo Seltzer. “Scalable Bayesian rule lists.” Proceedings of the 34th International Conference on Machine Learning-Volume 70. JMLR. org, 2017.↩︎\nFürnkranz, Johannes, Dragan Gamberger, and Nada Lavrač. “Foundations of rule learning.” Springer Science & Business Media, (2012).↩︎",
    "crumbs": [
      "5 - Modèles interprétables",
      "5.5 - Règles de décision"
    ]
  },
  {
    "objectID": "06-model_agnostic_methods/index.html",
    "href": "06-model_agnostic_methods/index.html",
    "title": "6 - Méthodes indépendantes du modèle",
    "section": "",
    "text": "Avertissement\n\n\n\nEn cours de traduction.",
    "crumbs": [
      "6 - Méthodes indépendantes du modèle"
    ]
  },
  {
    "objectID": "06-model_agnostic_methods/index.html#footnotes",
    "href": "06-model_agnostic_methods/index.html#footnotes",
    "title": "6 - Méthodes indépendantes du modèle",
    "section": "Notes de bas de page",
    "text": "Notes de bas de page\n\n\nRibeiro, Marco Tulio, Sameer Singh, and Carlos Guestrin. “Model-agnostic interpretability of machine learning.” ICML Workshop on Human Interpretability in Machine Learning. (2016).↩︎",
    "crumbs": [
      "6 - Méthodes indépendantes du modèle"
    ]
  },
  {
    "objectID": "08-global_model_agnostic_methods/08.1-pdp.html",
    "href": "08-global_model_agnostic_methods/08.1-pdp.html",
    "title": "Apprentissage automatique interprétable",
    "section": "",
    "text": "Avertissement\n\n\n\nEn cours de traduction.",
    "crumbs": [
      "8 - Méthodes globales indépendantes du modèle",
      "8.1 - Diagramme de dépendance partielle (PDP)"
    ]
  },
  {
    "objectID": "08-global_model_agnostic_methods/08.1-pdp.html#diagramme-de-dépendance-partielle-pdp",
    "href": "08-global_model_agnostic_methods/08.1-pdp.html#diagramme-de-dépendance-partielle-pdp",
    "title": "Apprentissage automatique interprétable",
    "section": "8.1 - Diagramme de dépendance partielle (PDP)",
    "text": "8.1 - Diagramme de dépendance partielle (PDP)\nLe tracé de dépendance partielle (court PDP ou PD plot) montre l’effet marginal d’une ou deux caractéristiques sur le résultat prédit d’un modèle d’apprentissage automatique (JH Friedman 20011). Un diagramme de dépendance partielle peut montrer si la relation entre la cible et une entité est linéaire, monotone ou plus complexe. Par exemple, lorsqu’ils sont appliqués à un modèle de régression linéaire, les diagrammes de dépendance partielle montrent toujours une relation linéaire.\nLa fonction de dépendance partielle pour la régression est définie comme :\n\\[\\hat{f}_S(x_S)=E_{X_C}\\left[\\hat{f}(x_S,X_C)\\right]=\\int\\hat{f}(x_S,X_C)d\\mathbb{P}(X_C)\\]\nLe \\(x_S\\) sont les caractéristiques pour lesquelles la fonction de dépendance partielle doit être tracée et \\(X_C\\) sont les autres fonctionnalités utilisées dans le modèle d’apprentissage automatique \\(\\hat{f}\\), qui sont ici traitées comme des variables aléatoires. Habituellement, il n’y a qu’une ou deux fonctionnalités dans l’ensemble S. La ou les fonctionnalités dans S sont celles dont nous voulons connaître l’effet sur la prédiction. Les vecteurs de caractéristiques \\(x_S\\) et \\(x_C\\) combinés constituent l’espace total des fonctionnalités x. La dépendance partielle fonctionne en marginalisant la sortie du modèle d’apprentissage automatique sur la distribution des caractéristiques de l’ensemble C, de sorte que la fonction montre la relation entre les caractéristiques de l’ensemble S qui nous intéressent et le résultat prédit. En marginalisant les autres fonctionnalités, nous obtenons une fonction qui dépend uniquement des fonctionnalités de S, interactions avec d’autres fonctionnalités incluses.\nLa fonction partielle \\(\\hat{f}_S\\) est estimé en calculant des moyennes dans les données d’entraînement, également appelée méthode de Monte Carlo :\n\\[\\hat{f}_S(x_S)=\\frac{1}{n}\\sum_{i=1}^n\\hat{f}(x_S,x^{(i)}_{C})\\]\nLa fonction partielle nous indique pour une ou plusieurs valeurs données de caractéristiques S quel est l’effet marginal moyen sur la prédiction. Dans cette formule, \\(x^{(i)}_{C}\\) sont les valeurs réelles des fonctionnalités de l’ensemble de données pour les fonctionnalités qui ne nous intéressent pas, et n est le nombre d’instances dans l’ensemble de données. Une hypothèse du PDP est que les caractéristiques de C ne sont pas corrélées avec les caractéristiques de S. Si cette hypothèse n’est pas respectée, les moyennes calculées pour le diagramme de dépendance partielle incluront des points de données très improbables, voire impossibles (voir inconvénients).\nPour la classification où le modèle d’apprentissage automatique génère des probabilités, le tracé de dépendance partielle affiche la probabilité pour une certaine classe en fonction de différentes valeurs pour les caractéristiques de S. Un moyen simple de gérer plusieurs classes consiste à tracer une ligne ou un tracé par classe.\nLe tracé de dépendance partielle est une méthode globale : la méthode prend en compte toutes les instances et donne une déclaration sur la relation globale d’une caractéristique avec le résultat prédit.\nCaractéristiques catégorielles\nJusqu’à présent, nous n’avons considéré que les caractéristiques numériques. Pour les caractéristiques catégorielles, la dépendance partielle est très facile à calculer. Pour chacune des catégories, nous obtenons une estimation PDP en forçant toutes les instances de données à avoir la même catégorie. Par exemple, si nous examinons l’ensemble de données de location de vélos et que nous nous intéressons au graphique de dépendance partielle pour la saison, nous obtenons quatre nombres, un pour chaque saison. Pour calculer la valeur de « été », nous remplaçons la saison de toutes les instances de données par « été » et faisons la moyenne des prédictions.\n\n8.1.1 - Importance des fonctionnalités basées sur PDP\nGreenwell et coll. (2018)2 ont proposé une mesure simple de l’importance des caractéristiques basée sur la dépendance partielle. La motivation de base est qu’un PDP plat indique que la fonctionnalité n’est pas importante, et plus le PDP varie, plus la fonctionnalité est importante. Pour les caractéristiques numériques, l’importance est définie comme l’écart de chaque valeur de caractéristique unique par rapport à la courbe moyenne :\n\\[I(x_S) =  \\sqrt{\\frac{1}{K-1}\\sum_{k=1}^K(\\hat{f}_S(x^{(k)}_S) - \\frac{1}{K}\\sum_{k=1}^K \\hat{f}_S({x^{(k)}_S))^2}}\\]\nNotez qu’ici le \\(x^{(k)}_S\\) sont les K valeurs uniques de la caractéristique \\(X_S\\). Pour les fonctionnalités catégorielles, nous avons :\n\\[I(x_S) = (max_k(\\hat{f}_S(x^{(k)}_S)) - min_k(\\hat{f}_S(x^{(k)}_S)))/4\\]\nIl s’agit de la plage des valeurs PDP pour les catégories uniques divisée par quatre. Cette étrange façon de calculer l’écart s’appelle la règle de plage. Il est utile d’obtenir une estimation approximative de l’écart lorsque vous ne connaissez que la plage. Et le dénominateur quatre vient de la distribution normale standard : dans la distribution normale, 95% des données sont moins deux et plus deux écarts types autour de la moyenne. Ainsi, la fourchette divisée par quatre donne une estimation approximative qui sous-estime probablement la variance réelle.\nCette importance des fonctionnalités basées sur PDP doit être interprétée avec prudence. Il capture uniquement l’effet principal de la fonctionnalité et ignore les interactions possibles entre les fonctionnalités. Une fonctionnalité pourrait être très importante sur la base d’autres méthodes telles que l’importance des fonctionnalités de permutation, mais le PDP pourrait être plat car la fonctionnalité affecte la prédiction principalement via des interactions avec d’autres fonctionnalités. Un autre inconvénient de cette mesure est qu’elle est définie sur des valeurs uniques. Une valeur de caractéristique unique avec une seule instance reçoit le même poids dans le calcul de l’importance qu’une valeur avec plusieurs instances.\n\n\n8.1.2 - Exemples\nEn pratique, l’ensemble de fonctionnalités S ne contient généralement qu’une seule fonctionnalité ou au maximum deux, car une fonctionnalité produit des tracés 2D et deux fonctionnalités produisent des tracés 3D. Au-delà, tout est assez délicat. Même la 3D sur un papier ou un moniteur 2D est déjà un défi.\nRevenons à l’exemple de régression, dans lequel on prédit le nombre de vélos qui seront loués un jour donné. Nous ajustons d’abord un modèle d’apprentissage automatique, puis nous analysons les dépendances partielles. Dans ce cas, nous avons ajusté une forêt aléatoire pour prédire le nombre de vélos et utilisé le diagramme de dépendance partielle pour visualiser les relations apprises par le modèle. L’influence des caractéristiques météorologiques sur le nombre de vélos prévu est visualisée dans la figure suivante.\n\n\n\nPDP pour le modèle de prévision du nombre de vélos et la température, l’humidité et la vitesse du vent. Les plus grandes différences peuvent être observées au niveau de la température. Plus il fait chaud, plus on loue de vélos. Cette tendance monte jusqu’à 20 degrés Celsius, puis s’aplatit et diminue légèrement à 30. Les marques sur l’axe des X indiquent la distribution des données.\n\n\nPar temps chaud mais pas trop chaud, le modèle prévoit en moyenne un nombre élevé de vélos loués. Les motards potentiels sont de plus en plus réticents à louer un vélo lorsque l’humidité dépasse 60 %. De plus, plus il y a de vent, moins les gens aiment faire du vélo, ce qui est logique. Il est intéressant de noter que le nombre prévu de locations de vélos ne diminue pas lorsque la vitesse du vent augmente de 25 à 35 km/h, mais il n’existe pas beaucoup de données d’entraînement, de sorte que le modèle d’apprentissage automatique ne pourrait probablement pas apprendre une prédiction significative pour cette plage. Au moins intuitivement, je m’attendrais à ce que le nombre de vélos diminue avec l’augmentation de la vitesse du vent, surtout lorsque la vitesse du vent est très élevée.\nPour illustrer un graphique de dépendance partielle avec une caractéristique catégorielle, nous examinons l’effet de la caractéristique saisonnière sur les locations de vélos prévues.\n\n\n\nPDP pour le modèle de prédiction du comptage de vélos et la saison. De manière inattendue, toutes les saisons montrent un effet similaire sur les prévisions du modèle, mais pour l’hiver seulement, le modèle prédit moins de locations de vélos.\n\n\nNous calculons également la dépendance partielle pour la classification du cancer du col de l’utérus. Cette fois, nous avons ajusté une forêt aléatoire pour prédire si une femme pourrait souffrir d’un cancer du col de l’utérus en fonction de facteurs de risque. Nous calculons et visualisons la dépendance partielle de la probabilité de cancer sur différentes caractéristiques de la forêt aléatoire :\n\n\n\nPDP de probabilité de cancer en fonction de l’âge et du nombre d’années d’utilisation de contraceptifs hormonaux. Pour l’âge, le PDP montre que la probabilité est faible jusqu’à 40 ans et augmente après. Plus le nombre d’années passées sous contraceptifs hormonaux est élevé, plus le risque de cancer est élevé, surtout après 10 ans. Pour les deux caractéristiques, peu de points de données avec des valeurs élevées étaient disponibles, de sorte que les estimations de PD sont moins fiables dans ces régions.\n\n\nNous pouvons également visualiser la dépendance partielle de deux caractéristiques à la fois :\n\n\n\nPDP de la probabilité de cancer et interaction entre l’âge et le nombre de grossesses. Le graphique montre l’augmentation de la probabilité de cancer à 45 ans. Pour les âges inférieurs à 25 ans, les femmes qui ont eu 1 ou 2 grossesses ont un risque de cancer prédit plus faible, par rapport aux femmes qui ont eu 0 ou plus de 2 grossesses. Mais soyez prudent lorsque vous tirez des conclusions : il pourrait s’agir simplement d’une corrélation et non d’une causalité !",
    "crumbs": [
      "8 - Méthodes globales indépendantes du modèle",
      "8.1 - Diagramme de dépendance partielle (PDP)"
    ]
  },
  {
    "objectID": "08-global_model_agnostic_methods/08.1-pdp.html#avantages",
    "href": "08-global_model_agnostic_methods/08.1-pdp.html#avantages",
    "title": "Apprentissage automatique interprétable",
    "section": "8.1.3 - Avantages",
    "text": "8.1.3 - Avantages\nLe calcul des tracés de dépendance partielle est intuitif : la fonction de dépendance partielle à une valeur de caractéristique particulière représente la prédiction moyenne si nous forçons tous les points de données à assumer cette valeur de caractéristique. D’après mon expérience, les profanes comprennent généralement rapidement le concept des PDP.\nSi la fonctionnalité pour laquelle vous avez calculé le PDP n’est pas corrélée aux autres fonctionnalités, alors les PDP représentent parfaitement la manière dont la fonctionnalité influence la prédiction en moyenne. Dans le cas non corrélé, l’interprétation est claire: le diagramme de dépendance partielle montre comment la prédiction moyenne de votre ensemble de données change lorsque la j-ème entité est modifiée. C’est plus compliqué lorsque les caractéristiques sont corrélées, voir aussi les inconvénients.\nLes parcelles de dépendance partielle sont faciles à mettre en oeuvre.\nLe calcul des tracés de dépendance partielle a une interprétation causale. Nous intervenons sur une fonctionnalité et mesurons les changements dans les prédictions. Ce faisant, nous analysons la relation causale entre la caractéristique et la prédiction3. La relation est causale pour le modèle – car nous modélisons explicitement le résultat en fonction des caractéristiques – mais pas nécessairement pour le monde réel !",
    "crumbs": [
      "8 - Méthodes globales indépendantes du modèle",
      "8.1 - Diagramme de dépendance partielle (PDP)"
    ]
  },
  {
    "objectID": "08-global_model_agnostic_methods/08.1-pdp.html#inconvénients",
    "href": "08-global_model_agnostic_methods/08.1-pdp.html#inconvénients",
    "title": "Apprentissage automatique interprétable",
    "section": "8.1.4 - Inconvénients",
    "text": "8.1.4 - Inconvénients\nLe nombre maximum réaliste d’entités dans une fonction de dépendance partielle est de deux. Ce n’est pas la faute des PDP, mais de la représentation en 2 dimensions (papier ou écran) et aussi de notre incapacité à imaginer plus de 3 dimensions.\nCertains tracés PD ne montrent pas la distribution des caractéristiques. Omettre la distribution peut être trompeur, car vous risquez de surinterpréter des régions ne comportant pratiquement aucune donnée. Ce problème est facilement résolu en affichant un tapis (indicateurs pour les points de données sur l’axe des x) ou un histogramme.\nL’hypothèse d’indépendance est le plus gros problème des tracés PD. On suppose que la ou les caractéristiques pour lesquelles la dépendance partielle est calculée ne sont pas corrélées avec d’autres caractéristiques. Par exemple, supposons que vous souhaitiez prédire la vitesse à laquelle une personne marche, en fonction de son poids et de sa taille. Pour la dépendance partielle d’une des caractéristiques, par exemple la taille, nous supposons que les autres caractéristiques (le poids) ne sont pas corrélées à la taille, ce qui est évidemment une fausse hypothèse. Pour le calcul du PDP à une certaine hauteur (par exemple 200 cm), nous faisons la moyenne sur la distribution marginale du poids, qui peut inclure un poids inférieur à 50 kg, ce qui est irréaliste pour une personne de 2 mètres. En d’autres termes : lorsque les caractéristiques sont corrélées, nous créons de nouveaux points de données dans les zones de la distribution des caractéristiques où la probabilité réelle est très faible (par exemple, il est peu probable qu’une personne mesure 2 mètres mais pèse moins de 50 kg). Une solution à ce problème consiste à utiliser les tracés d’effet local accumulé ou les tracés ALE courts qui fonctionnent avec la distribution conditionnelle plutôt qu’avec la distribution marginale.\nLes effets hétérogènes peuvent être masqués car les graphiques PD ne montrent que les effets marginaux moyens. Supposons que pour une caractéristique, la moitié de vos points de données ont une association positive avec la prédiction – plus la valeur de la caractéristique est grande, plus la prédiction est grande – et l’autre moitié a une association négative – plus la valeur de la caractéristique est petite, plus la prédiction est grande. La courbe PD pourrait être une ligne horizontale, puisque les effets des deux moitiés de l’ensemble de données pourraient s’annuler. Vous concluez alors que la fonctionnalité n’a aucun effet sur la prédiction. En traçant les courbes d’espérances conditionnelles individuelles au lieu de la ligne agrégée, nous pouvons découvrir des effets hétérogènes.",
    "crumbs": [
      "8 - Méthodes globales indépendantes du modèle",
      "8.1 - Diagramme de dépendance partielle (PDP)"
    ]
  },
  {
    "objectID": "08-global_model_agnostic_methods/08.1-pdp.html#logiciels-et-alternatives",
    "href": "08-global_model_agnostic_methods/08.1-pdp.html#logiciels-et-alternatives",
    "title": "Apprentissage automatique interprétable",
    "section": "8.1.5 - Logiciels et alternatives",
    "text": "8.1.5 - Logiciels et alternatives\nIl existe un certain nombre de packages R qui implémentent des PDP. J’ai utilisé le package iml pour les exemples, mais il existe également pdp or DALEX. En Python, des tracés de dépendance partielle sont intégrés scikit-learnet vous pouvez utiliser PDPBox.\nLes alternatives aux PDP présentées dans ce livre sont les tracés ALE et les courbes ICE.",
    "crumbs": [
      "8 - Méthodes globales indépendantes du modèle",
      "8.1 - Diagramme de dépendance partielle (PDP)"
    ]
  },
  {
    "objectID": "08-global_model_agnostic_methods/08.1-pdp.html#footnotes",
    "href": "08-global_model_agnostic_methods/08.1-pdp.html#footnotes",
    "title": "Apprentissage automatique interprétable",
    "section": "Notes de bas de page",
    "text": "Notes de bas de page\n\n\nFriedman, Jerome H. “Greedy function approximation: A gradient boosting machine.” Annals of statistics (2001): 1189-1232.↩︎\nGreenwell, Brandon M., Bradley C. Boehmke, and Andrew J. McCarthy. “A simple and effective model-based variable importance measure.” arXiv preprint arXiv:1805.04755 (2018).↩︎\nZhao, Qingyuan, and Trevor Hastie. “Causal interpretations of black-box models.” Journal of Business & Economic Statistics, to appear. (2017).↩︎",
    "crumbs": [
      "8 - Méthodes globales indépendantes du modèle",
      "8.1 - Diagramme de dépendance partielle (PDP)"
    ]
  },
  {
    "objectID": "08-global_model_agnostic_methods/08.3-feature-interaction.html",
    "href": "08-global_model_agnostic_methods/08.3-feature-interaction.html",
    "title": "Apprentissage automatique interprétable",
    "section": "",
    "text": "Avertissement\n\n\n\nEn cours de traduction.",
    "crumbs": [
      "8 - Méthodes globales indépendantes du modèle",
      "8.3 - Interactions avec les fonctionnalités"
    ]
  },
  {
    "objectID": "08-global_model_agnostic_methods/08.3-feature-interaction.html#interactions-avec-les-fonctionnalités",
    "href": "08-global_model_agnostic_methods/08.3-feature-interaction.html#interactions-avec-les-fonctionnalités",
    "title": "Apprentissage automatique interprétable",
    "section": "8.3 - Interactions avec les fonctionnalités",
    "text": "8.3 - Interactions avec les fonctionnalités\nLorsque des fonctionnalités interagissent les unes avec les autres dans un modèle de prédiction, la prédiction ne peut pas être exprimée comme la somme des effets des fonctionnalités, car l’effet d’une fonctionnalité dépend de la valeur de l’autre fonctionnalité. Le prédicat d’Aristote « Le tout est plus grand que la somme de ses parties » s’applique en présence d’interactions.\n\n8.3.1 Interaction des fonctionnalités ?\nSi un modèle d’apprentissage automatique effectue une prédiction basée sur deux fonctionnalités, nous pouvons décomposer la prédiction en quatre termes : un terme constant, un terme pour la première fonctionnalité, un terme pour la deuxième fonctionnalité et un terme pour l’interaction entre les deux fonctionnalités. L’interaction entre deux caractéristiques est le changement dans la prédiction qui se produit en faisant varier les caractéristiques après avoir pris en compte les effets des caractéristiques individuelles.\nPar exemple, un modèle prédit la valeur d’une maison, en utilisant comme caractéristiques sa taille (grande ou petite) et son emplacement (bon ou mauvais), ce qui donne quatre prédictions possibles :\n\n\n\nLocation\nSize\nPrediction\n\n\n\n\ngood\nbig\n300,000\n\n\ngood\nsmall\n200,000\n\n\nbad\nbig\n250,000\n\n\nbad\nsmall\n150,000\n\n\n\nNous décomposons la prédiction du modèle en parties suivantes : un terme constant (150 000), un effet pour la caractéristique de taille (+100 000 si grande ; +0 si petite) et un effet pour l’emplacement (+50 000 si bon ; +0 si mauvais). Cette décomposition explique pleinement les prédictions du modèle. Il n’y a pas d’effet d’interaction, car la prédiction du modèle est une somme des effets de caractéristiques uniques pour la taille et l’emplacement. Lorsque vous agrandissez une petite maison, la prédiction augmente toujours de 100 000, quel que soit l’emplacement. En outre, la différence de prédiction entre un bon et un mauvais emplacement est de 50 000, quelle que soit sa taille.\nRegardons maintenant un exemple avec interaction :\n\n\n\nLocation\nSize\nPrediction\n\n\n\n\ngood\nbig\n400,000\n\n\ngood\nsmall\n200,000\n\n\nbad\nbig\n250,000\n\n\nbad\nsmall\n150,000\n\n\n\nNous décomposons le tableau de prédiction en parties suivantes : un terme constant (150 000), un effet pour la caractéristique de taille (+100 000 si grande, +0 si petite) et un effet pour l’emplacement (+50 000 si bon, +0 si mauvais). Pour ce tableau, nous avons besoin d’un terme supplémentaire pour l’interaction : +100 000 si la maison est grande et bien située. Il s’agit d’une interaction entre la taille et l’emplacement, car dans ce cas, la différence de prédiction entre une grande et une petite maison dépend de l’emplacement.\nUne façon d’estimer la force de l’interaction consiste à mesurer dans quelle mesure la variation de la prédiction dépend de l’interaction des caractéristiques. Cette mesure est appelée statistique H, introduite par Friedman et Popescu (2008)1.\n\n\n8.3.2 - Théorie : statistique H de Friedman\nNous allons traiter deux cas : premièrement, une mesure d’interaction bidirectionnelle qui nous indique si et dans quelle mesure deux caractéristiques du modèle interagissent entre elles ; deuxièmement, une mesure d’interaction totale qui nous indique si et dans quelle mesure une fonctionnalité interagit dans le modèle avec toutes les autres fonctionnalités. En théorie, des interactions arbitraires entre un nombre quelconque de caractéristiques peuvent être mesurées, mais ces deux cas sont les plus intéressants.\nSi deux caractéristiques n’interagissent pas, nous pouvons décomposer la fonction de dépendance partielle comme suit (en supposant que les fonctions de dépendance partielles sont centrées sur zéro) :\n\\[PD_{jk}(x_j,x_k)=PD_j(x_j)+PD_k(x_k)\\]\noù \\(PD_{jk}(x_j,x_k)\\) est la fonction de dépendance partielle bidirectionnelle des deux caractéristiques et \\(PD_j(x_j)\\) et \\(PD_k(x_k)\\) les fonctions de dépendance partielle des caractéristiques uniques.\nDe même, si une fonctionnalité n’a aucune interaction avec aucune des autres fonctionnalités, nous pouvons exprimer la fonction de prédiction \\(\\hat{f}(x)\\) comme somme de fonctions de dépendance partielles, où la première somme dépend uniquement de j et la seconde de toutes les autres caractéristiques sauf j :\n\\[\\hat{f}(x)=PD_j(x_j)+PD_{-j}(x_{-j})\\]\noù \\(PD_{-j}(x_{-j})\\) est la fonction de dépendance partielle qui dépend de toutes les fonctionnalités à l’exception de la j-ème fonctionnalité.\nCette décomposition exprime la fonction de dépendance partielle (ou de prédiction complète) sans interactions (entre les caractéristiques j et k, ou respectivement j et toutes les autres caractéristiques). Dans une étape suivante, nous mesurons la différence entre la fonction de dépendance partielle observée et la fonction décomposée sans interactions. Nous calculons la variance de la sortie de la dépendance partielle (pour mesurer l’interaction entre deux fonctionnalités) ou de la fonction entière (pour mesurer l’interaction entre une fonctionnalité et toutes les autres fonctionnalités). L’ampleur de la variance expliquée par l’interaction (différence entre la PD observée et sans interaction) est utilisée comme statistique de force d’interaction. La statistique est 0 s’il n’y a aucune interaction et 1 si toute la variance du \\(PD_{jk}\\) ou \\(\\hat{f}\\) s’explique par la somme des fonctions de dépendance partielles. Une statistique d’interaction de 1 entre deux caractéristiques signifie que chaque fonction PD est constante et que l’effet sur la prédiction ne résulte que de l’interaction. La statistique H peut également être supérieure à 1, ce qui est plus difficile à interpréter. Cela peut se produire lorsque la variance de l’interaction bidirectionnelle est supérieure à la variance du tracé de dépendance partielle bidimensionnel.\nMathématiquement, la statistique H proposée par Friedman et Popescu pour l’interaction entre les caractéristiques j et k est :\n\\[H^2_{jk} = \\frac{\\sum_{i=1}^n\\left[PD_{jk}(x_{j}^{(i)},x_k^{(i)})-PD_j(x_j^{(i)}) - PD_k(x_{k}^{(i)})\\right]^2}{\\sum_{i=1}^n{PD}^2_{jk}(x_j^{(i)},x_k^{(i)})}\\]\nIl en va de même pour mesurer si une fonctionnalité j interagit avec une autre fonctionnalité :\n\\[H^2_{j}=\\frac{\\sum_{i=1}^n\\left[\\hat{f}(x^{(i)})-PD_j(x_j^{(i)})-PD_{-j}(x_{-j}^{(i)})\\right]^2}{\\sum_{i=1}^n\\hat{f}^2(x^{(i)})}\\]\nLa statistique H est coûteuse à évaluer, car elle itère sur tous les points de données et à chaque point, la dépendance partielle doit être évaluée, ce qui à son tour est effectué avec les n points de données. Dans le pire des cas, nous avons besoin de 2n 2 appels à la fonction de prédiction des modèles d’apprentissage automatique pour calculer la statistique H bidirectionnelle (j contre k) et de \\(3n^2\\) pour la statistique H totale (j contre tous). Pour accélérer le calcul, nous pouvons échantillonner à partir des n points de données. Cela présente l’inconvénient d’augmenter la variance des estimations de dépendance partielle, ce qui rend la statistique H instable. Donc, si vous utilisez l’échantillonnage pour réduire la charge de calcul, assurez-vous d’échantillonner suffisamment de points de données.\nFriedman et Popescu proposent également une statistique de test pour évaluer si la statistique H diffère significativement de zéro. L’hypothèse nulle est l’absence d’interaction. Pour générer la statistique d’interaction sous l’hypothèse nulle, vous devez être capable d’ajuster le modèle afin qu’il n’y ait aucune interaction entre les caractéristiques j et k ou toutes les autres. Cela n’est pas possible pour tous les types de modèles. Par conséquent, ce test est spécifique au modèle, et non indépendant du modèle, et n’est donc pas abordé ici.\nLa statistique de force d’interaction peut également être appliquée dans un cadre de classification si la prédiction est une probabilité.\n\n\n8.3.3 - Exemples\nVoyons à quoi ressemblent les interactions entre fonctionnalités dans la pratique ! Nous mesurons la force d’interaction des caractéristiques dans une machine à vecteurs de support qui prédit le nombre de vélos loués en fonction des caractéristiques météorologiques et calendaires. Le graphique suivant montre la statistique H de l’interaction des fonctionnalités :\n\n\n\nLa force d’interaction (statistique H) pour chaque caractéristique avec toutes les autres caractéristiques pour une machine à vecteurs de support prédisant la location de vélos. Globalement, les effets d’interaction entre les caractéristiques sont très faibles (inférieurs à 10 % de variance expliquée par caractéristique).\n\n\nDans l’exemple suivant, nous calculons la statistique d’interaction pour un problème de classification. Nous analysons les interactions entre les caractéristiques d’une forêt aléatoire formée pour prédire le cancer du col de l’utérus, compte tenu de certains facteurs de risque.\n\n\n\nForce d’interaction (statistique H) pour chaque caractéristique avec toutes les autres caractéristiques pour une forêt aléatoire prédisant la probabilité de cancer du col de l’utérus. Les années sous contraceptifs hormonaux ont l’effet d’interaction relatif le plus élevé avec toutes les autres caractéristiques, suivies par le nombre de grossesses.\n\n\nAprès avoir examiné les interactions de chaque fonctionnalité avec toutes les autres fonctionnalités, nous pouvons sélectionner l’une des fonctionnalités et approfondir toutes les interactions bidirectionnelles entre la fonctionnalité sélectionnée et les autres fonctionnalités.\n\n\n\nLes forces d’interaction bidirectionnelle (statistique H) entre le nombre de grossesses et chaque autre caractéristique. Il existe une forte interaction entre le nombre de grossesses et l’âge.\n\n\n\n\n8.3.4 - Avantages\nLa statistique H d’interaction a une théorie sous-jacente à travers la décomposition de dépendance partielle.\nLa statistique H a une interprétation significative : l’interaction est définie comme la part de variance expliquée par l’interaction.\nÉtant donné que la statistique est sans dimension, elle est comparable entre les fonctionnalités et même entre les modèles.\nLa statistique détecte toutes sortes d’interactions, quelle que soit leur forme particulière.\nAvec la statistique H, il est également possible d’analyser des interactions arbitraires plus élevées telles que la force d’interaction entre 3 caractéristiques ou plus.\n\n\n8.3.5 - Inconvénients\nLa première chose que vous remarquerez : la statistique H d’interaction prend beaucoup de temps à calculer, car elle est coûteuse en calcul.\nLe calcul implique l’estimation des distributions marginales. Ces estimations présentent également une certaine variance si nous n’utilisons pas tous les points de données. Cela signifie que lorsque nous échantillonnons des points, les estimations varient également d’une exécution à l’autre et les résultats peuvent être instables. Je recommande de répéter le calcul de la statistique H plusieurs fois pour voir si vous disposez de suffisamment de données pour obtenir un résultat stable.\nIl n’est pas clair si une interaction est significativement supérieure à 0. Il faudrait effectuer un test statistique, mais ce test n’est pas (encore) disponible dans une version indépendante du modèle.\nConcernant le problème de test, il est difficile de dire quand la statistique H est suffisamment grande pour que l’on considère une interaction « forte ».\nDe plus, la statistique H peut être supérieure à 1, ce qui rend l’interprétation difficile.\nLorsque l’effet total de deux caractéristiques est faible, mais consiste principalement en interactions, la statistique H sera très grande. Ces interactions parasites nécessitent un petit dénominateur de la statistique H et sont aggravées lorsque les caractéristiques sont corrélées. Une interaction parasite peut être facilement surinterprétée comme un effet d’interaction fort, alors qu’en réalité les deux caractéristiques jouent un rôle mineur dans le modèle. Une solution possible consiste à visualiser la version non normalisée de la statistique H2, qui est la racine carrée du numérateur de la statistique H 36 . Cela met la statistique H au même niveau que la réponse, au moins pour la régression, et met moins l’accent sur les interactions parasites.\n\\[H^{*}_{jk} = \\sqrt{\\sum_{i=1}^n\\left[PD_{jk}(x_{j}^{(i)},x_k^{(i)})-PD_j(x_j^{(i)}) - PD_k(x_{k}^{(i)})\\right]^2}\\]\nLa statistique H nous indique la force des interactions, mais elle ne nous dit pas à quoi ressemblent ces interactions. C’est à cela que servent les parcelles de dépendance partielle. Un flux de travail significatif consiste à mesurer les forces d’interaction, puis à créer des tracés de dépendance partielle en 2D pour les interactions qui vous intéressent.\nLa statistique H ne peut pas être utilisée de manière significative si les entrées sont des pixels. La technique n’est donc pas utile pour le classificateur d’images.\nLa statistique d’interaction fonctionne sous l’hypothèse que nous pouvons mélanger les fonctionnalités indépendamment. Si les caractéristiques sont fortement corrélées, l’hypothèse est violée et nous intégrons des combinaisons de caractéristiques qui sont très improbables en réalité. C’est le même problème que rencontrent les parcelles de dépendance partielle. Les caractéristiques corrélées peuvent conduire à des valeurs élevées de la statistique H.\nParfois, les résultats sont étranges et pour de petites simulations, ils ne donnent pas les résultats escomptés. Mais il s’agit là plutôt d’une observation anecdotique.\n\n\n8.3.6 - Implémentations\nPour les exemples de ce livre, j’ai utilisé le package R iml, disponible sur CRAN et la version de développement sur GitHub. Il existe d’autres implémentations, qui se concentrent sur des modèles spécifiques : Le package R pre implémente RuleFit et H-statistic. Le package R gbm implémente des modèles améliorés par gradient et la statistique H.\n\n\n8.3.7 - Alternatives\nLa statistique H n’est pas le seul moyen de mesurer les interactions :\nLes réseaux d’interaction variable (VIN) de Hooker (2004)3 37 sont une approche qui décompose la fonction de prédiction en effets principaux et interactions de fonctionnalités. Les interactions entre les fonctionnalités sont ensuite visualisées sous forme de réseau. Malheureusement, aucun logiciel n’est encore disponible.\nInteraction de fonctionnalités basée sur une dépendance partielle par Greenwell et al. (2018)4 38 mesure l’interaction entre deux fonctionnalités. Cette approche mesure l’importance des caractéristiques (définie comme la variance de la fonction de dépendance partielle) d’une caractéristique en fonction de points fixes différents de l’autre caractéristique. Si la variance est élevée, alors les caractéristiques interagissent les unes avec les autres, si elle est nulle, elles n’interagissent pas. Le package R correspondant vip est disponible sur GitHub. Le package couvre également les tracés de dépendance partielle et l’importance des fonctionnalités.",
    "crumbs": [
      "8 - Méthodes globales indépendantes du modèle",
      "8.3 - Interactions avec les fonctionnalités"
    ]
  },
  {
    "objectID": "08-global_model_agnostic_methods/08.3-feature-interaction.html#footnotes",
    "href": "08-global_model_agnostic_methods/08.3-feature-interaction.html#footnotes",
    "title": "Apprentissage automatique interprétable",
    "section": "Notes de bas de page",
    "text": "Notes de bas de page\n\n\nFriedman, Jerome H, and Bogdan E Popescu. “Predictive learning via rule ensembles.” The Annals of Applied Statistics. JSTOR, 916–54. (2008).↩︎\nInglis, Alan, Andrew Parnell, and Catherine Hurley. “Visualizing Variable Importance and Variable Interaction Effects in Machine Learning Models.” arXiv preprint arXiv:2108.04310 (2021).↩︎\nHooker, Giles. “Discovering additive structure in black box functions.” Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining. (2004).↩︎\nGreenwell, Brandon M., Bradley C. Boehmke, and Andrew J. McCarthy. “A simple and effective model-based variable importance measure.” arXiv preprint arXiv:1805.04755 (2018).↩︎",
    "crumbs": [
      "8 - Méthodes globales indépendantes du modèle",
      "8.3 - Interactions avec les fonctionnalités"
    ]
  },
  {
    "objectID": "08-global_model_agnostic_methods/08.5-permutation-feature-importance.html",
    "href": "08-global_model_agnostic_methods/08.5-permutation-feature-importance.html",
    "title": "8.5.5 - Inconvénients",
    "section": "",
    "text": "Avertissement\n\n\n\nEn cours de traduction.",
    "crumbs": [
      "8 - Méthodes globales indépendantes du modèle",
      "8.5 - Décomposition fonctionnelle"
    ]
  },
  {
    "objectID": "08-global_model_agnostic_methods/08.5-permutation-feature-importance.html#décomposition-fonctionnelle",
    "href": "08-global_model_agnostic_methods/08.5-permutation-feature-importance.html#décomposition-fonctionnelle",
    "title": "8.5.5 - Inconvénients",
    "section": "8.5 - Décomposition fonctionnelle",
    "text": "8.5 - Décomposition fonctionnelle\nL’importance des caractéristiques de permutation mesure l’augmentation de l’erreur de prédiction du modèle après la permutation des valeurs de la caractéristique, ce qui rompt la relation entre la caractéristique et le résultat réel.\n\n8.5.1 - Théorie\nLe concept est très simple : nous mesurons l’importance d’une caractéristique en calculant l’augmentation de l’erreur de prédiction du modèle après permutation de la caractéristique. Une caractéristique est « importante » si le mélange de ses valeurs augmente l’erreur du modèle, car dans ce cas, le modèle s’est appuyé sur la caractéristique pour la prédiction. Une caractéristique est « sans importance » si le mélange de ses valeurs laisse l’erreur du modèle inchangée, car dans ce cas, le modèle a ignoré la caractéristique pour la prédiction. La mesure de l’importance des caractéristiques de permutation a été introduite par Breiman (2001)1 pour les forêts aléatoires. Sur la base de cette idée, Fisher, Rudin et Dominici (2018)2 ont proposé une version indépendante du modèle de l’importance des caractéristiques et l’ont appelée dépendance du modèle. Ils ont également introduit des idées plus avancées sur l’importance des fonctionnalités, par exemple une version (spécifique au modèle) qui prend en compte le fait que de nombreux modèles de prédiction peuvent bien prédire les données. Leur article mérite d’être lu.\nL’algorithme d’importance des caractéristiques de permutation basé sur Fisher, Rudin et Dominici (2018) :\nEntrée : modèle formé \\(\\hat{f}\\), matrice de fonctionnalités \\(X\\), vecteur cible \\(y\\), mesure d’erreur \\(L(y,\\hat{f})\\).\n\nEstimer l’erreur du modèle d’origine \\(e_{orig} = L(y, \\hat{f}(X))\\) (par exemple erreur quadratique moyenne)\nPour chaque caractéristique \\(j \\in \\{1,...,p\\}\\) faire:\n\nGénérer une matrice de fonctionnalités \\(X_{perm}\\) en permutant la fonctionnalité \\(j\\) et le vrai résultat \\(y\\).\nEstimer l’erreur \\(e_{perm} = L(Y,\\hat{f}(X_{perm}))\\) basée sur les prédictions des données permutées.\nCalculer l’importance des caractéristiques de permutation sous forme de quotient \\(FI_j= e_{perm}/e_{orig}\\) ou de différence \\(FI_j = e_{perm} - e_{orig}\\).\nTrier les entités par \\(FI\\) décroissant.\n\n\nFisher, Rudin et Dominici (2018) suggèrent dans leur article de diviser l’ensemble de données en deux et d’échanger les valeurs de la caractéristique \\(j\\) des deux moitiés au lieu de permuter la caractéristique \\(j\\). C’est exactement la même chose que la permutation de la fonctionnalité \\(j\\), si vous y réfléchissez. Si vous souhaitez une estimation plus précise, vous pouvez estimer l’erreur de permutation de la caractéristique \\(j\\) en associant chaque instance à la valeur de la caractéristique \\(j\\) de chaque autre instance (sauf avec elle-même). Cela vous donne un ensemble de données de taille \\(n(n-1)\\) pour estimer l’erreur de permutation, et cela prend beaucoup de temps de calcul. Je ne peux recommander d’utiliser la \\(n(n-1)\\) méthode - que si vous souhaitez sérieusement obtenir des estimations extrêmement précises.\n\n\n8.5.2 - Dois-je calculer l’importance des données de formation ou de test ?\ntl;dr : Vous devriez probablement utiliser des données de test.\nRépondre à la question sur les données de formation ou de test touche à la question fondamentale de savoir quelle est l’importance des fonctionnalités. La meilleure façon de comprendre la différence entre l’importance des fonctionnalités basée sur la formation et celle basée sur les données de test est un exemple « extrême ». J’ai entraîné une machine à vecteurs de support pour prédire un résultat cible continu et aléatoire étant donné 50 caractéristiques aléatoires (200 instances). Par « aléatoire », j’entends que le résultat cible est indépendant des 50 caractéristiques. C’est comme prédire la température de demain à partir des derniers chiffres de loterie. Si le modèle « apprend » des relations, il est alors surajusté. Et en fait, le SVM a surajusté les données d’entraînement. L’erreur absolue moyenne (abréviation : mae) pour les données d’entraînement est de 0,29 et pour les données de test de 0,82, ce qui est également l’erreur du meilleur modèle possible qui prédit toujours le résultat moyen de 0 (mae de 0,78). En d’autres termes, le modèle SVM est une poubelle. Quelles valeurs d’importance des fonctionnalités attendez-vous pour les 50 fonctionnalités de ce SVM suréquipé ? Zéro parce qu’aucune des fonctionnalités ne contribue à améliorer les performances sur des données de test invisibles ? Ou les importances devraient-elles refléter dans quelle mesure le modèle dépend de chacune des caractéristiques, que les relations apprises se généralisent ou non à des données invisibles ? Voyons en quoi les distributions des importances des fonctionnalités pour les données de formation et de test diffèrent.\n\n\n\nDistributions des valeurs d’importance des caractéristiques par type de données. Un SVM a été formé sur un ensemble de données de régression avec 50 fonctionnalités aléatoires et 200 instances. Le SVM surajuste les données : l’importance des fonctionnalités basée sur les données d’entraînement montre de nombreuses fonctionnalités importantes. Calculées sur des données de test invisibles, les importances des fonctionnalités sont proches d’un rapport de un (= sans importance).\n\n\nJe ne sais pas lequel des deux résultats est le plus souhaitable. Je vais donc essayer de défendre les deux versions.\nLe cas des données de test\nIl s’agit d’un cas simple : les estimations d’erreurs de modèle basées sur les données d’entraînement sont des ordures -&gt; l’importance des caractéristiques repose sur les estimations d’erreurs du modèle -&gt; l’importance des caractéristiques basée sur les données d’entraînement est une ordure. En réalité, c’est l’une des premières choses que vous apprenez en apprentissage automatique : si vous mesurez l’erreur (ou les performances) du modèle sur les mêmes données sur lesquelles le modèle a été formé, la mesure est généralement trop optimiste, ce qui signifie que le modèle semble fonctionne bien mieux qu’il ne le fait en réalité. Et comme l’importance des caractéristiques de permutation repose sur des mesures de l’erreur du modèle, nous devrions utiliser des données de test invisibles. L’importance des fonctionnalités basée sur les données d’entraînement nous fait croire à tort que les fonctionnalités sont importantes pour les prédictions, alors qu’en réalité le modèle était simplement surajusté et les fonctionnalités n’étaient pas importantes du tout.\nLe cas des données de formation\nLes arguments en faveur de l’utilisation des données de formation sont un peu plus difficiles à formuler, mais à mon humble avis, ils sont tout aussi convaincants que les arguments en faveur de l’utilisation des données de test. Nous jetons un autre regard sur notre SVM poubelle. D’après les données d’entraînement, la fonctionnalité la plus importante était X42. Regardons un tracé de dépendance partielle de la fonctionnalité X42. Le tracé de dépendance partielle montre comment la sortie du modèle change en fonction des modifications de la fonctionnalité et ne repose pas sur l’erreur de généralisation. Peu importe que le PDP soit calculé avec des données d’entraînement ou de test.\n\n\n\nPDP de la fonctionnalité X42, qui est la fonctionnalité la plus importante selon l’importance de la fonctionnalité basée sur les données d’entraînement. Le graphique montre comment le SVM dépend de cette fonctionnalité pour faire des prédictions.\n\n\nLe graphique montre clairement que le SVM a appris à s’appuyer sur la fonctionnalité X42 pour ses prédictions, mais selon l’importance de la fonctionnalité basée sur les données de test (1), ce n’est pas important. Sur la base des données de formation, l’importance est de 1,19, ce qui reflète que le modèle a appris à utiliser cette fonctionnalité. L’importance des fonctionnalités basée sur les données d’entraînement nous indique quelles fonctionnalités sont importantes pour le modèle dans le sens où il en dépend pour faire des prédictions.\nDans le cadre de l’utilisation des données de formation, je voudrais introduire un argument contre les données de test. En pratique, vous souhaitez utiliser toutes vos données pour entraîner votre modèle afin d’obtenir au final le meilleur modèle possible. Cela signifie qu’il ne reste aucune donnée de test inutilisée pour calculer l’importance des fonctionnalités. Vous rencontrez le même problème lorsque vous souhaitez estimer l’erreur de généralisation de votre modèle. Si vous utilisiez la validation croisée (imbriquée) pour l’estimation de l’importance des caractéristiques, vous rencontreriez le problème que l’importance des caractéristiques n’est pas calculée sur le modèle final avec toutes les données, mais sur des modèles avec des sous-ensembles de données qui pourraient se comporter différemment.\nCependant, en fin de compte, je recommande d’utiliser les données de test pour l’importance des fonctionnalités de permutation. Parce que si vous souhaitez savoir dans quelle mesure les prédictions du modèle sont influencées par une fonctionnalité, vous devez utiliser d’autres mesures d’importance telles que l’importance SHAP.\nEnsuite, nous examinerons quelques exemples. J’ai basé le calcul de l’importance sur les données de formation, car je devais en choisir une et l’utilisation des données de formation nécessitait quelques lignes de code en moins.\n\n\n8.5.3 - Exemple et interprétation\nJe montre des exemples de classification et de régression.\nCancer du col de l’utérus (classification)\nNous ajustons un modèle forestier aléatoire pour prédire le cancer du col de l’utérus. Nous mesurons l’augmentation de l’erreur de 1-AUC (1 moins l’aire sous la courbe ROC). Les caractéristiques associées à une erreur de modèle augmentée d’un facteur 1 (= aucun changement) n’étaient pas importantes pour prédire le cancer du col de l’utérus.\n\n\n\nImportance de chacune des caractéristiques pour prédire le cancer du col de l’utérus avec une forêt aléatoire. La caractéristique la plus importante était les contraceptifs hormonaux..années.. Les contraceptifs.hormonaux.permutants..années. a entraîné une augmentation de l’ASC-1 d’un facteur 6,13.\n\n\nLa caractéristique la plus importante était celle des contraceptifs hormonaux..années. associé à une augmentation d’erreur de 6,13 après permutation.\nPartage de vélos (régression)\nNous ajustons un modèle de machine à vecteurs de support pour prédire le nombre de vélos loués, en fonction des conditions météorologiques et des informations du calendrier. Comme mesure d’erreur, nous utilisons l’erreur absolue moyenne.\n\n\n\nL’importance de chacune des caractéristiques dans la prévision du nombre de vélos avec une machine à vecteurs de support. La caractéristique la plus importante était la température, la moins importante était les vacances.\n\n\n\n\n8.5.4 Avantages\nBelle interprétation : l’importance d’une fonctionnalité est l’augmentation de l’erreur de modèle lorsque les informations de la fonctionnalité sont détruites.\nL’importance des fonctionnalités fournit un aperçu global hautement compressé du comportement du modèle.\nUn aspect positif de l’utilisation du taux d’erreur au lieu de la différence d’erreur est que les mesures de l’importance des caractéristiques sont comparables entre différents problèmes .\nLa mesure d’importance prend automatiquement en compte toutes les interactions avec d’autres fonctionnalités. En permutant la fonctionnalité, vous détruisez également les effets d’interaction avec d’autres fonctionnalités. Cela signifie que l’importance des fonctionnalités de permutation prend en compte à la fois l’effet des fonctionnalités principales et les effets d’interaction sur les performances du modèle. Ceci constitue également un inconvénient car l’importance de l’interaction entre deux caractéristiques est incluse dans les mesures d’importance des deux caractéristiques. Cela signifie que l’importance des fonctionnalités ne correspond pas à la baisse totale des performances, mais que la somme est plus importante. Ce n’est que s’il n’y a pas d’interaction entre les caractéristiques, comme dans un modèle linéaire, que les importances s’additionnent approximativement.\nL’importance des fonctionnalités de permutation ne nécessite pas de recyclage du modèle. Certaines autres méthodes suggèrent de supprimer une fonctionnalité, de recycler le modèle, puis de comparer l’erreur du modèle. Étant donné que le recyclage d’un modèle d’apprentissage automatique peut prendre beaucoup de temps, « seule » permuter une fonctionnalité peut faire gagner beaucoup de temps. Les méthodes d’importance qui recyclent le modèle avec un sous-ensemble de fonctionnalités semblent intuitives à première vue, mais le modèle avec les données réduites n’a aucun sens pour l’importance des fonctionnalités. Nous nous intéressons à l’importance des fonctionnalités d’un modèle fixe. Le recyclage avec un ensemble de données réduit crée un modèle différent de celui qui nous intéresse. Supposons que vous entraîniez un modèle linéaire clairsemé (avec Lasso) avec un nombre fixe d’entités avec un poids non nul. L’ensemble de données comporte 100 fonctionnalités, vous définissez le nombre de poids non nuls sur 5. Vous analysez l’importance de l’une des fonctionnalités qui ont un poids non nul. Vous supprimez la fonctionnalité et recyclez le modèle. Les performances du modèle restent les mêmes car une autre fonctionnalité tout aussi intéressante obtient un poids non nul et votre conclusion serait que la fonctionnalité n’était pas importante. Autre exemple : le modèle est un arbre de décision et nous analysons l’importance de la fonctionnalité qui a été choisie comme première division. Vous supprimez la fonctionnalité et recyclez le modèle. Puisqu’une autre caractéristique est choisie comme première division, l’arbre entier peut être très différent, ce qui signifie que nous comparons les taux d’erreur d’arbres (potentiellement) complètement différents pour décider de l’importance de cette caractéristique pour l’un des arbres.",
    "crumbs": [
      "8 - Méthodes globales indépendantes du modèle",
      "8.5 - Décomposition fonctionnelle"
    ]
  },
  {
    "objectID": "08-global_model_agnostic_methods/08.5-permutation-feature-importance.html#footnotes",
    "href": "08-global_model_agnostic_methods/08.5-permutation-feature-importance.html#footnotes",
    "title": "8.5.5 - Inconvénients",
    "section": "Notes de bas de page",
    "text": "Notes de bas de page\n\n\nBreiman, Leo.“Random Forests.” Machine Learning 45 (1). Springer: 5-32 (2001).↩︎\nFisher, Aaron, Cynthia Rudin, and Francesca Dominici. “All models are wrong, but many are useful: Learning a variable’s importance by studying an entire class of prediction models simultaneously.” https://arxiv.org/abs/1801.01489 (2018).↩︎\nWei, Pengfei, Zhenzhou Lu, and Jingwen Song. “Variable importance analysis: a comprehensive review.” Reliability Engineering & System Safety 142 (2015): 399-432.↩︎",
    "crumbs": [
      "8 - Méthodes globales indépendantes du modèle",
      "8.5 - Décomposition fonctionnelle"
    ]
  },
  {
    "objectID": "09-local_model_agnostic_methods/09.1-ice.html",
    "href": "09-local_model_agnostic_methods/09.1-ice.html",
    "title": "Apprentissage automatique interprétable",
    "section": "",
    "text": "Avertissement\n\n\n\nEn cours de traduction.",
    "crumbs": [
      "9 - Méthodes locales indépendantes du modèle",
      "9.1 - Attente Conditionnelle Individuelle (*Individual Conditional Expectation - ICE*)"
    ]
  },
  {
    "objectID": "09-local_model_agnostic_methods/09.1-ice.html#attente-conditionnelle-individuelle-individual-conditional-expectation---ice",
    "href": "09-local_model_agnostic_methods/09.1-ice.html#attente-conditionnelle-individuelle-individual-conditional-expectation---ice",
    "title": "Apprentissage automatique interprétable",
    "section": "9.1 - Attente Conditionnelle Individuelle (Individual Conditional Expectation - ICE)",
    "text": "9.1 - Attente Conditionnelle Individuelle (Individual Conditional Expectation - ICE)\nLes tracés ICE (Individual Conditional Expectation) affichent une ligne par instance qui montre comment la prédiction de l’instance change lorsqu’une fonctionnalité change.\nLe diagramme de dépendance partielle pour l’effet moyen d’une caractéristique est une méthode globale car elle ne se concentre pas sur des instances spécifiques, mais sur une moyenne globale. L’équivalent d’un PDP pour les instances de données individuelles est appelé tracé des attentes conditionnelles individuelles (ICE) (Goldstein et al. 2017)1. Un tracé ICE visualise la dépendance de la prédiction sur une caractéristique pour chaque instance séparément, ce qui donne une ligne par instance, contre une ligne globale dans les tracés de dépendance partielle. Un PDP est la moyenne des lignes d’un tracé ICE. Les valeurs d’une ligne (et d’une instance) peuvent être calculées en conservant toutes les autres entités identiques, en créant des variantes de cette instance en remplaçant la valeur de l’entité par les valeurs d’une grille et en effectuant des prédictions avec le modèle de boîte noire pour ces instances nouvellement créées. Le résultat est un ensemble de points pour une instance avec la valeur de caractéristique de la grille et les prédictions respectives.\nQuel est l’intérêt de s’intéresser aux attentes individuelles plutôt qu’aux dépendances partielles ? Les diagrammes de dépendance partielle peuvent masquer une relation hétérogène créée par les interactions. Les PDP peuvent vous montrer à quoi ressemble la relation moyenne entre une fonctionnalité et la prédiction. Cela ne fonctionne bien que si les interactions entre les fonctionnalités pour lesquelles le PDP est calculé et les autres fonctionnalités sont faibles. En cas d’interactions, le tracé ICE fournira beaucoup plus d’informations.\nUne définition plus formelle : dans les tracés ICE, pour chaque instance de \\(\\{(x_{S}^{(i)},x_{C}^{(i)})\\}_{i=1}^N\\) la courbe \\(\\hat{f}_S^{(i)}\\) est comploté contre \\(x^{(i)}_{S}\\), alors que \\(x^{(i)}_{C}\\) reste fixe.",
    "crumbs": [
      "9 - Méthodes locales indépendantes du modèle",
      "9.1 - Attente Conditionnelle Individuelle (*Individual Conditional Expectation - ICE*)"
    ]
  },
  {
    "objectID": "09-local_model_agnostic_methods/09.1-ice.html#exemples",
    "href": "09-local_model_agnostic_methods/09.1-ice.html#exemples",
    "title": "Apprentissage automatique interprétable",
    "section": "9.1.1 - Exemples",
    "text": "9.1.1 - Exemples\nRevenons à l’ensemble de données sur le cancer du col de l’utérus et voyons comment la prédiction de chaque instance est associée à la fonctionnalité « Âge ». Nous analyserons une forêt aléatoire qui prédit la probabilité de cancer pour une femme compte tenu des facteurs de risque. Dans le graphique de dépendance partielle, nous avons vu que la probabilité de cancer augmente vers l’âge de 50 ans, mais est-ce vrai pour chaque femme de l’ensemble de données ? Le graphique ICE révèle que pour la plupart des femmes, l’effet de l’âge suit la tendance moyenne d’augmentation à 50 ans, à quelques exceptions près : pour les quelques femmes qui ont une probabilité prédite élevée à un jeune âge, la probabilité prédite de cancer ne change pas. beaucoup avec l’âge.\n\n\n\nGraphique ICE de la probabilité de cancer du col de l’utérus par âge. Chaque ligne représente une femme. Pour la plupart des femmes, la probabilité prévue de cancer augmente avec l’âge. Pour certaines femmes dont la probabilité prédite de cancer est supérieure à 0,4, la prédiction ne change pas beaucoup à un âge plus avancé.\n\n\nLa figure suivante montre les tracés ICE pour la prévision de la location de vélos. Le modèle de prédiction sous-jacent est une forêt aléatoire.\n\n\n\nGraphiques ICE des locations de vélos prévues en fonction des conditions météorologiques. Les mêmes effets peuvent être observés que dans les diagrammes de dépendance partielle.\n\n\nToutes les courbes semblent suivre le même parcours, il n’y a donc pas d’interactions évidentes. Cela signifie que le PDP est déjà un bon résumé des relations entre les caractéristiques affichées et le nombre prévu de vélos.\n\n9.1.1.1 - Tracé ICE centré\nIl y a un problème avec les tracés ICE : il peut parfois être difficile de déterminer si les courbes ICE diffèrent d’un individu à l’autre, car elles partent de prédictions différentes. Une solution simple consiste à centrer les courbes en un certain point de l’entité et à afficher uniquement la différence de prédiction jusqu’à ce point. Le tracé résultant est appelé tracé ICE centré (c-ICE). Ancrer les courbes à l’extrémité inférieure de l’entité est un bon choix. Les nouvelles courbes sont définies comme :\n\\[\\hat{f}_{cent}^{(i)}=\\hat{f}^{(i)}-\\mathbf{1}\\hat{f}(x^{a},x^{(i)}_{C})\\]\noù \\(\\mathbf{1}\\) est un vecteur de 1 avec le nombre approprié de dimensions (généralement une ou deux), \\(\\hat{f}\\) est le modèle ajusté et \\(x^a\\) est le point d’ancrage.\n\n\n9.1.1.2 - Exemple\nPar exemple, prenons le tracé ICE du cancer du col de l’utérus pour l’âge et centrez les lignes sur l’âge observé le plus jeune :\n\n\n\nGraphique ICE centré pour la probabilité prévue de cancer par âge. Les lignes sont fixées à 0 à 14 ans. Par rapport à 14 ans, les prédictions pour la plupart des femmes restent inchangées jusqu’à l’âge de 45 ans, où la probabilité prédite augmente.\n\n\nLes tracés ICE centrés facilitent la comparaison des courbes d’instances individuelles. Cela peut être utile si nous ne voulons pas voir le changement absolu d’une valeur prédite, mais la différence dans la prédiction par rapport à un point fixe de la plage de caractéristiques.\nJetons un coup d’œil aux tracés ICE centrés pour la prédiction de la location de vélos :\n\n\n\nGraphiques ICE centrés du nombre prévu de vélos selon les conditions météorologiques. Les lignes montrent la différence de prédiction par rapport à la prédiction avec la valeur de la caractéristique respective à son minimum observé.\n\n\n\n\n9.1.1.3 - Graphique ICE dérivé\nUne autre façon de faciliter visuellement la détection de l’hétérogénéité consiste à examiner les dérivées individuelles de la fonction de prédiction par rapport à une caractéristique. Le tracé résultant est appelé tracé ICE dérivé (d-ICE). Les dérivées d’une fonction (ou d’une courbe) vous indiquent si des changements se produisent et dans quelle direction ils se produisent. Avec le tracé ICE dérivé, il est facile de repérer les plages de valeurs de caractéristiques où les prédictions de la boîte noire changent pour (au moins certaines) instances. S’il n’y a pas d’interaction entre la fonctionnalité analysée \\(x_S\\) et les autres fonctionnalités \\(x_C\\), alors la fonction de prédiction peut être exprimée comme suit :\n\\[\\hat{f}(x)=\\hat{f}(x_S,x_C)=g(x_S)+h(x_C),\\quad\\text{with}\\quad\\frac{\\delta\\hat{f}(x)}{\\delta{}x_S}=g'(x_S)\\]\nSans interactions, les dérivées partielles individuelles devraient être les mêmes pour toutes les instances. S’ils diffèrent, cela est dû à des interactions et cela devient visible sur le tracé d-ICE. En plus d’afficher les courbes individuelles de la dérivée de la fonction de prédiction par rapport à la caractéristique dans \\(S\\), l’affichage de l’écart type de la dérivée permet de mettre en évidence les régions de la caractéristique dans \\(S\\) présentant une hétérogénéité dans les dérivées estimées. Le tracé ICE dérivé prend beaucoup de temps à calculer et est plutôt peu pratique.\n\n\n9.1.2 - Avantages\nLes courbes d’espérances conditionnelles individuelles sont encore plus intuitives à comprendre que les diagrammes de dépendance partielle. Une ligne représente les prédictions pour une instance si nous faisons varier la caractéristique d’intérêt.\nContrairement aux diagrammes de dépendance partielle, les courbes ICE peuvent révéler des relations hétérogènes.\n\n\n9.1.3 - Inconvénients\nLes courbes ICE ne peuvent afficher qu’une seule entité de manière significative, car deux entités nécessiteraient le dessin de plusieurs surfaces superposées et vous ne verriez rien dans le tracé.\nLes courbes ICE souffrent du même problème que les PDP : si la caractéristique d’intérêt est corrélée aux autres caractéristiques, alors certains points des lignes peuvent être des points de données invalides selon la distribution commune des caractéristiques.\nSi de nombreuses courbes ICE sont tracées, le tracé peut devenir surpeuplé et vous ne verrez rien. La solution : soit ajoutez un peu de transparence aux lignes, soit dessinez uniquement un échantillon des lignes.\nDans les graphiques ICE, il n’est peut-être pas facile de voir la moyenne. Il existe une solution simple : combiner les courbes d’espérances conditionnelles individuelles avec le tracé de dépendance partielle.\n\n\n9.1.4 - Logiciels et alternatives\nLes tracés ICE sont implémentés dans les packages R iml (utilisés pour ces exemples), ICEbox2 et pdp. Un autre package R qui fait quelque chose de très similaire à ICE est condvis. En Python, les tracés de dépendance partielle sont intégrés à scikit-learn à partir de la version 0.24.0.",
    "crumbs": [
      "9 - Méthodes locales indépendantes du modèle",
      "9.1 - Attente Conditionnelle Individuelle (*Individual Conditional Expectation - ICE*)"
    ]
  },
  {
    "objectID": "09-local_model_agnostic_methods/09.1-ice.html#footnotes",
    "href": "09-local_model_agnostic_methods/09.1-ice.html#footnotes",
    "title": "Apprentissage automatique interprétable",
    "section": "Notes de bas de page",
    "text": "Notes de bas de page\n\n\nGoldstein, Alex, Adam Kapelner, Justin Bleich, and Emil Pitkin. “Peeking inside the black box: Visualizing statistical learning with plots of individual conditional expectation.” journal of Computational and Graphical Statistics 24, no. 1 (2015): 44-65.↩︎\nGoldstein, Alex, Adam Kapelner, Justin Bleich, and Maintainer Adam Kapelner. “Package ‘ICEbox’.” (2017).↩︎",
    "crumbs": [
      "9 - Méthodes locales indépendantes du modèle",
      "9.1 - Attente Conditionnelle Individuelle (*Individual Conditional Expectation - ICE*)"
    ]
  },
  {
    "objectID": "09-local_model_agnostic_methods/09.3-counterfactual.html",
    "href": "09-local_model_agnostic_methods/09.3-counterfactual.html",
    "title": "Apprentissage automatique interprétable",
    "section": "",
    "text": "Avertissement\n\n\n\nEn cours de traduction.",
    "crumbs": [
      "9 - Méthodes locales indépendantes du modèle",
      "9.3 - Explications contrefactuelles"
    ]
  },
  {
    "objectID": "09-local_model_agnostic_methods/09.3-counterfactual.html#explications-contrefactuelles",
    "href": "09-local_model_agnostic_methods/09.3-counterfactual.html#explications-contrefactuelles",
    "title": "Apprentissage automatique interprétable",
    "section": "9.3 - Explications contrefactuelles",
    "text": "9.3 - Explications contrefactuelles\nAuteurs : Susanne Dandl et Christoph Molnar\nUne explication contrefactuelle décrit une situation causale sous la forme : « Si X ne s’était pas produit, Y ne se serait pas produit ». Par exemple : « Si je n’avais pas bu une gorgée de ce café chaud, je ne me serais pas brûlé la langue ». L’événement Y est que je me suis brûlé la langue ; parce que X, c’est que j’ai pris un café chaud. Penser de manière contrefactuelle nécessite d’imaginer une réalité hypothétique qui contredit les faits observés (par exemple, un monde dans lequel je n’ai pas bu de café chaud), d’où le nom de « contrefactuel ». La capacité de penser de manière contrefactuelle nous rend, les humains, si intelligents par rapport aux autres animaux.\nDans l’apprentissage automatique interprétable, des explications contrefactuelles peuvent être utilisées pour expliquer les prédictions d’instances individuelles. L’« événement » est le résultat prédit d’une instance, les « causes » sont les valeurs de caractéristiques particulières de cette instance qui ont été entrées dans le modèle et « ont provoqué » une certaine prédiction. Affichée sous forme de graphique, la relation entre les entrées et la prédiction est très simple : les valeurs des caractéristiques provoquent la prédiction.\n\n\n\nLes relations causales entre les entrées d’un modèle d’apprentissage automatique et les prédictions, lorsque le modèle est simplement vu comme une boîte noire. Les entrées provoquent la prédiction (ne reflétant pas nécessairement la relation causale réelle des données).\n\n\nMême si en réalité la relation entre les entrées et le résultat à prédire n’est pas causale, nous pouvons considérer les entrées d’un modèle comme la cause de la prédiction.\nCompte tenu de ce graphique simple, il est facile de voir comment nous pouvons simuler des contrefactuels pour les prédictions des modèles d’apprentissage automatique : nous modifions simplement les valeurs des caractéristiques d’une instance avant de faire les prédictions et nous analysons comment la prédiction change. Nous nous intéressons aux scénarios dans lesquels la prédiction change de manière pertinente, comme un changement de classe prédite (par exemple, demande de crédit acceptée ou rejetée), ou dans lesquels la prédiction atteint un certain seuil (par exemple, la probabilité de cancer atteint \\(10%\\)). Une explication contrefactuelle d’une prédiction décrit le plus petit changement apporté aux valeurs des caractéristiques qui transforme la prédiction en une sortie prédéfinie.\nIl existe des méthodes d’explication contrefactuelles indépendantes du modèle et spécifiques au modèle, mais dans ce chapitre, nous nous concentrons sur les méthodes indépendantes du modèle qui fonctionnent uniquement avec les entrées et les sorties du modèle (et non avec la structure interne de modèles spécifiques). Ces méthodes trouveraient également leur place dans le chapitre indépendant du modèle, puisque l’interprétation peut être exprimée comme un résumé des différences dans les valeurs des caractéristiques (« modifier les caractéristiques A et B pour modifier la prédiction »). Mais une explication contrefactuelle est elle-même une nouvelle instance, elle figure donc dans ce chapitre (« en partant de l’instance X, changez A et B pour obtenir une instance contrefactuelle »). Contrairement aux prototypes, les contrefactuels ne doivent pas nécessairement être des instances réelles issues des données d’entraînement, mais peuvent être une nouvelle combinaison de valeurs de caractéristiques.\nAvant d’expliquer comment créer des contrefactuels, j’aimerais discuter de quelques cas d’utilisation des contrefactuels et de ce à quoi ressemble une bonne explication contrefactuelle.\nDans ce premier exemple, Peter demande un prêt et est rejeté par le logiciel bancaire (reposant par l’apprentissage automatique). Il se demande pourquoi sa demande a été rejetée et comment il pourrait améliorer ses chances d’obtenir un prêt. La question du « pourquoi » peut être formulée de manière contrefactuelle : quel est le plus petit changement dans les caractéristiques (revenu, nombre de cartes de crédit, âge, …) qui ferait passer la prédiction de rejetée à approuvée ? Une réponse possible pourrait être : si Pierre gagnait 10 000 de plus par an, il obtiendrait le prêt. Ou si Peter avait moins de cartes de crédit et n’avait pas fait défaut sur un prêt il y a cinq ans, il obtiendrait le prêt. Peter ne connaîtra jamais les raisons du refus, car la banque n’a aucun intérêt à la transparence, mais c’est une autre histoire.\nDans notre deuxième exemple, nous souhaitons expliquer un modèle qui prédit un résultat continu avec des explications contrefactuelles. Anna souhaite louer son appartement, mais elle ne sait pas exactement combien facturer. Elle décide donc de former un modèle d’apprentissage automatique pour prédire le loyer. Bien sûr, comme Anna est une data scientist, c’est ainsi qu’elle résout ses problèmes. Après avoir entré tous les détails concernant la taille, l’emplacement, si les animaux sont autorisés, etc., le mannequin lui dit qu’elle peut facturer 900 EUR. Elle s’attendait à 1 000 EUR ou plus, mais elle fait confiance à son modèle et décide de jouer avec les valeurs caractéristiques de l’appartement pour voir comment elle peut améliorer la valeur de l’appartement. Elle découvre que l’appartement pourrait être loué pour plus de 1 000 euros s’il était plus grand de \\(15 m^2\\). Une connaissance intéressante, mais non exploitable, car elle ne peut pas agrandir son appartement. Enfin, en modifiant uniquement les valeurs des caractéristiques sous son contrôle (cuisine intégrée oui/non, animaux admis oui/non, type de sol, etc.), elle découvre que si elle autorise les animaux et installe des fenêtres avec une meilleure isolation, elle peut facturer 1000 EUR. Anna a intuitivement travaillé avec des hypothèses contrefactuelles pour changer le résultat.\nLes contrefactuels sont des explications conviviales, car ils contrastent avec l’instance actuelle et parce qu’ils sont sélectifs, ce qui signifie qu’ils se concentrent généralement sur un petit nombre de changements de fonctionnalités. Mais les contrefactuels souffrent de « l’effet Rashomon ». Rashomon est un film japonais dans lequel le meurtre d’un samouraï est raconté par différentes personnes. Chacune des histoires explique également bien le résultat, mais les histoires se contredisent. La même chose peut également se produire avec les hypothèses contrefactuelles, car il existe généralement plusieurs explications contrefactuelles différentes. Chaque contrefactuel raconte une « histoire » différente sur la façon dont un certain résultat a été atteint. Un contrefactuel pourrait dire de changer la caractéristique A, l’autre contrefactuel pourrait dire de laisser A inchangé mais de changer la caractéristique B, ce qui est une contradiction. Cette question des vérités multiples peut être résolue soit en rapportant toutes les explications contrefactuelles, soit en ayant un critère pour évaluer les contrefactuelles et sélectionner la meilleure.\nEn parlant de critères, comment définir une bonne explication contrefactuelle ? Premièrement, l’utilisateur d’une explication contrefactuelle définit un changement pertinent dans la prédiction d’une instance (= la réalité alternative). Une première exigence évidente est qu’une instance contrefactuelle produise la prédiction prédéfinie aussi fidèlement que possible. Il n’est pas toujours possible de trouver un contrefactuel avec la prédiction prédéfinie. Par exemple, dans un paramètre de classification comportant deux classes, une classe rare et une classe fréquente, le modèle peut toujours classer une instance comme classe fréquente. Changer les valeurs des caractéristiques afin que l’étiquette prédite passe de la classe fréquente à la classe rare peut être impossible. Nous souhaitons donc assouplir l’exigence selon laquelle la prédiction du contrefactuel doit correspondre exactement au résultat prédéfini. Dans l’exemple de classification, nous pourrions rechercher un contrefactuel dans lequel la probabilité prédite de la classe rare est augmentée à \\(10%\\) au lieu des \\(2%\\) actuels. La question est alors de savoir quels sont les changements minimaux dans les caractéristiques pour que la probabilité prédite passe de \\(2%\\) à \\(10%\\) (ou près de \\(10%\\)) ?\nUn autre critère de qualité est qu’un contrefactuel doit être aussi similaire que possible à l’instance en ce qui concerne les valeurs des caractéristiques. La distance entre deux instances peut être mesurée, par exemple, avec la distance de Manhattan ou la distance de Gower si nous avons à la fois des caractéristiques discrètes et continues. Le contrefactuel ne doit pas seulement être proche de l’instance d’origine, mais doit également modifier le moins de caractéristiques possible. Pour mesurer la qualité d’une explication contrefactuelle dans cette métrique, nous pouvons simplement compter le nombre de caractéristiques modifiées ou, en termes mathématiques sophistiqués, mesurer la \\(L_0\\) norme entre instance contrefactuelle et instance réelle.\nTroisièmement, il est souvent souhaitable de générer de multiples explications contrefactuelles afin que le sujet de la décision ait accès à de multiples façons viables de générer un résultat différent. Par exemple, en poursuivant notre exemple de prêt, une explication contrefactuelle pourrait suggérer de doubler simplement le revenu pour obtenir un prêt, tandis qu’une autre explication contrefactuelle pourrait suggérer de déménager dans une ville voisine et d’augmenter légèrement le revenu pour obtenir un prêt. Il convient de noter que même si le premier scénario contrefactuel pourrait être possible pour certains, le second pourrait être plus exploitable pour d’autres. Ainsi, en plus de fournir au sujet de la décision différentes manières d’obtenir le résultat souhaité, la diversité permet également à des individus « divers » de modifier les caractéristiques qui leur conviennent.\nLa dernière exigence est qu’une instance contrefactuelle doit avoir des valeurs de caractéristiques probables. Cela n’aurait pas de sens de générer une explication contrefactuelle pour l’exemple du loyer où la taille d’un appartement est négative ou le nombre de pièces est fixé à 200. C’est encore mieux lorsque le contrefactuel est probable selon la distribution conjointe des données, par exemple, un appartement de 10 pièces et \\(20 m^2\\) ne doit pas être considéré comme une explication contrefactuelle. Idéalement, si le nombre de mètres carrés augmente, il faudrait également proposer une augmentation du nombre de pièces.\n\n9.3.1 - Générer des explications contrefactuelles\nUne approche simple et naïve pour générer des explications contrefactuelles consiste à rechercher par essais et erreurs. Cette approche implique de modifier de manière aléatoire les valeurs des caractéristiques de l’instance d’intérêt et de s’arrêter lorsque le résultat souhaité est prédit. Comme l’exemple où Anna a essayé de trouver une version de son appartement pour laquelle elle pourrait facturer un loyer plus élevé. Mais il existe de meilleures approches que les essais et erreurs. Tout d’abord, nous définissons une fonction de perte basée sur les critères mentionnés ci-dessus. Cette perte prend en entrée l’instance d’intérêt, un contrefactuel et le résultat (contrefactuel) souhaité. Ensuite, nous pouvons trouver l’explication contrefactuelle qui minimise cette perte à l’aide d’un algorithme d’optimisation. De nombreuses méthodes procèdent de cette manière, mais diffèrent par leur définition de la fonction de perte et leur méthode d’optimisation.\nDans ce qui suit, nous nous concentrons sur deux d’entre eux : d’abord celui de Wachter et al. (2017)1, qui ont introduit l’explication contrefactuelle comme méthode d’interprétation et, deuxièmement, celle de Dandl et al. (2020)2 qui prend en compte les quatre critères mentionnés ci-dessus.\n\n9.3.1.1 - Méthode de Wachter et al.\nWachter et coll. suggèrent de minimiser la perte suivante :\n\\[L(x,x^\\prime,y^\\prime,\\lambda)=\\lambda\\cdot(\\hat{f}(x^\\prime)-y^\\prime)^2+d(x,x^\\prime)\\]\nLe premier terme est la distance quadratique entre la prédiction du modèle pour le contrefactuel \\(x^\\prime\\) et le résultat souhaité \\(y^\\prime\\), que l’utilisateur doit définir à l’avance. Le deuxième terme est la distance d entre l’instance \\(x\\) à expliquer et le contrefactuel \\(x^\\prime\\). La perte mesure la distance entre le résultat prédit du contrefactuel et le résultat prédéfini et la distance entre le contrefactuel et l’instance d’intérêt. La fonction de distance d est définie comme la distance de Manhattan pondérée par l’écart médian absolu inverse (MAD) de chaque entité.\n\\[d(x,x^\\prime)=\\sum_{j=1}^p\\frac{|x_j-x^\\prime_j|}{MAD_j}\\]\nLa distance totale est la somme de toutes les p distances selon les caractéristiques, c’est-à-dire les différences absolues des valeurs des caractéristiques entre l’instance x et le contrefactuel x’. Les distances par entité sont mises à l’échelle par l’inverse de l’écart absolu médian de l’entité j sur l’ensemble de données défini comme :\n\\[MAD_j=\\text{median}_{i\\in{}\\{1,\\ldots,n\\}}(|x_{i,j}-\\text{median}_{l\\in{}\\{1,\\ldots,n\\}}(x_{l,j})|)\\]\nLa médiane d’un vecteur est la valeur à laquelle la moitié des valeurs du vecteur sont supérieures et l’autre moitié inférieure. Le MAD est l’équivalent de la variance d’une entité, mais au lieu d’utiliser la moyenne comme centre et de faire la somme sur les distances carrées, nous utilisons la médiane comme centre et de faire la somme sur les distances absolues. La fonction de distance proposée présente l’avantage par rapport à la distance euclidienne d’ être plus robuste aux valeurs aberrantes. La mise à l’échelle avec le MAD est nécessaire pour amener toutes les caractéristiques à la même échelle – peu importe que vous mesuriez la taille d’un appartement en mètres carrés ou en pieds carrés.\nLe paramètre \\(\\lambda\\) équilibre la distance dans la prédiction (premier terme) par rapport à la distance dans les valeurs des caractéristiques (deuxième terme). La perte est résolue pour un temps donné \\(\\lambda\\) et renvoie un x’ contrefactuel. Une valeur plus élevée de \\(\\lambda\\) signifie que nous préférons les contrefactuels avec des prédictions proches du résultat souhaité y’, une valeur inférieure signifie que nous préférons les contrefactuels x’ qui sont très similaires à x dans les valeurs des caractéristiques. Si \\(\\lambda\\) est très grande, l’instance avec la prédiction la plus proche de y’ sera sélectionnée, quelle que soit sa distance par rapport à x. En fin de compte, l’utilisateur doit décider comment équilibrer l’exigence selon laquelle la prédiction du contrefactuel correspond au résultat souhaité avec l’exigence que le contrefactuel soit similaire à x. Les auteurs de la méthode suggèrent au lieu de sélectionner une valeur pour \\(\\lambda\\) pour sélectionner une tolérance \\(\\epsilon\\) à quelle distance de \\(y^\\prime\\) la prédiction de l’instance contrefactuelle est autorisée. Cette contrainte peut s’écrire :\n\\[|\\hat{f}(x^\\prime)-y^\\prime|\\leq\\epsilon\\]\nPour minimiser cette fonction de perte, tout algorithme d’optimisation approprié peut être utilisé, tel que Nelder-Mead. Si vous avez accès aux dégradés du modèle d’apprentissage automatique, vous pouvez utiliser des méthodes basées sur les dégradés comme ADAM. L’instance \\(x\\) à expliquer, la sortie souhaitée \\(y^\\prime\\) et le paramètre de tolérance \\(\\epsilon\\) doit être réglé à l’avance. La fonction de perte est minimisée pour \\(x^\\prime\\) et le \\(x^\\prime\\) contrefactuel (localement) optimal est renvoyé tout en augmentant \\(\\lambda\\) jusqu’à ce qu’une solution suffisamment proche soit trouvée (= dans le paramètre de tolérance) :\n\\[\\arg\\min_{x^\\prime}\\max_{\\lambda}L(x,x^\\prime,y^\\prime,\\lambda).\\]\nDans l’ensemble, la recette pour produire les contrefactuels est simple : 1. Sélectionnez une instance \\(x\\) à expliquer, le résultat souhaité \\(y^\\prime\\), une tolérance \\(\\epsilon\\) et une (faible) valeur initiale pour \\(\\lambda\\). 2. Échantillonnez une instance aléatoire comme contrefactuel initial. 3. Optimisez la perte avec le contrefactuel initialement échantillonné comme point de départ. 4. Alors que \\(|\\hat{f}(x^\\prime)-y^\\prime|&gt;\\epsilon\\): - Augmenter \\(\\lambda\\). - Optimisez la perte avec le contrefactuel actuel comme point de départ. - Renvoyez le contrefactuel qui minimise la perte. 5. Répétez les étapes 2 à 4 et renvoyez la liste des contrefactuels ou celle qui minimise la perte.\nLa méthode proposée présente certains inconvénients. Il ne prend en compte que le premier et le deuxième critères, pas les deux derniers (« produire des contrefactuels avec seulement quelques changements de caractéristiques et des valeurs de caractéristiques probables »). \\(d\\) ne préfère pas les solutions clairsemées puisque l’augmentation de 10 entités par 1 donnera la même distance à \\(x\\) que l’augmentation d’une entité par 10. Les combinaisons d’entités irréalistes ne sont pas pénalisées.\nLa méthode ne gère pas bien les caractéristiques catégorielles avec de nombreux niveaux différents. Les auteurs de la méthode ont suggéré d’exécuter la méthode séparément pour chaque combinaison de valeurs de caractéristiques des caractéristiques catégorielles, mais cela entraînera une explosion combinatoire si vous disposez de plusieurs caractéristiques catégorielles avec de nombreuses valeurs. Par exemple, six caractéristiques catégorielles avec dix niveaux uniques signifieraient un million d’exécutions.\nVoyons maintenant une autre approche pour surmonter ces problèmes.\n\n\n9.3.1.2 Méthode de Dandl et al.\nDandler et coll. suggèrent de minimiser simultanément une perte de quatre objectifs : \\[L(x,x',y',X^{obs})=\\big(o_1(\\hat{f}(x'),y'),o_2(x, x'),o_3(x,x'),o_4(x',X^{obs})\\big) \\]\nChacun des quatre objectifs \\(o_1\\) à \\(o_4\\) correspond à l’un des quatre critères mentionnés ci-dessus. Le premier objectif \\(o_1\\) reflète que la prédiction de notre contrefactuel \\(x^\\prime\\) doit être aussi proche que possible de notre prédiction souhaitée \\(y^\\prime\\). Nous souhaitons donc minimiser la distance entre \\(\\hat{f}(x')\\) et \\(y^\\prime\\), calculé ici par la métrique de Manhattan (norme \\(L_1\\)):\n\\[o_1(\\hat{f}(x'),y')=\\begin{cases}0&\\text{if $\\hat{f}(x')\\in{}y'$}\\\\\\inf\\limits_{y'\\in y'}|\\hat{f}(x')-y'|&\\text{else}\\end{cases}\\]\nLe deuxième objectif \\(o_2\\) reflète que notre contrefactuel doit être aussi similaire que possible à notre exemple \\(x\\). Il quantifie la distance entre x’ et x comme la distance de Gower :\n\\[o_2(x,x')=\\frac{1}{p}\\sum_{j=1}^{p}\\delta_G(x_j, x'_j)\\]\navec \\(p\\) étant le nombre de fonctionnalités. La valeur de \\(\\delta_G\\) dépend du type de fonctionnalité de \\(x_j\\) :\n\\[\\delta_G(x_j,x'_j)=\\begin{cases}\\frac{1}{\\widehat{R}_j}|x_j-x'_j|&\\text{if $x_j$ numerical}\\\\\\mathbb{I}_{x_j\\neq{}x'_j}&\\text{if $x_j$ categorical}\\end{cases}\\]\nDiviser la distance d’une entité numérique \\(j\\) par \\(\\widehat{R}_j\\), la plage de valeurs observées, les échelles \\(\\delta_G\\) pour toutes les fonctionnalités entre 0 et 1.\nLa distance de Gower peut gérer à la fois les caractéristiques numériques et catégorielles, mais ne prend pas en compte le nombre de caractéristiques modifiées. On compte donc le nombre de fonctionnalités dans un troisième objectif \\(o_3\\) en utilisant la norme \\(L_0\\) :\n\\[o_3(x, x^\\prime) = \\|x-x^\\prime\\|_0 = \\sum_{j=1}^{p} \\mathbb{I}_{x'_j \\neq x_j}\\]\nEn minimisant \\(o_3\\) nous visons notre troisième critère – des changements de fonctionnalités clairsemés.\nLe quatrième objectif \\(o_4\\) reflète que nos contrefactuels devraient avoir des valeurs/combinaisons de caractéristiques probables. Nous pouvons déduire la « probabilité » qu’un point de données utilise les données d’entraînement ou un autre ensemble de données. Nous désignons cet ensemble de données par \\(X^{obs}\\). Comme approximation de la vraisemblance, \\(o_4\\) mesure la distance moyenne de Gower entre x’ et le point de données observé le plus proche \\(x^{[1]}\\in{}X^{obs}\\):\n\\[o_4(x',\\textbf{X}^{obs})=\\frac{1}{p}\\sum_{j=1}^{p}\\delta_G(x'_j,x^{[1]}_j)\\]\nPar rapport à Wachter et al., \\(L(x,x',y',X^{obs})\\) n’a pas de termes d’équilibrage/pondération comme \\(\\lambda\\). Nous ne voulons pas effondrer les quatre objectifs \\(o_1\\), \\(o_2\\), \\(o_3\\) et \\(o_4\\) en un seul objectif en les résumant et en les pondérant, mais nous souhaitons optimiser les quatre termes simultanément.\nComment peut-on faire ça? Nous utilisons l’ algorithme génétique de tri non dominé 55 ou NSGA-II abrégé. NSGA-II est un algorithme inspiré de la nature qui applique la loi de Darwin de la « survie du plus fort ». Nous désignons la fitness d’un contrefactuel par son vecteur de valeurs d’objectifs \\((o_1,o_2,o_3,o_4)\\). Plus les valeurs des objectifs d’un contrefactuel sont faibles, plus il est « adapté ».\nL’algorithme se compose de quatre étapes qui sont répétées jusqu’à ce qu’un critère d’arrêt soit satisfait, par exemple un nombre maximum d’itérations/générations. La figure suivante visualise les quatre étapes d’une génération.\n\n\n\nVisualisation d’une génération de l’algorithme NSGA-II.\n\n\nDans la première génération, un groupe de candidats contrefactuels est initialisé en modifiant aléatoirement certaines des caractéristiques par rapport à notre instance x à expliquer. En reprenant l’exemple de crédit ci-dessus, un contrefactuel pourrait suggérer d’augmenter le revenu de 30 000 € tandis qu’un autre propose de n’avoir aucun défaut au cours des cinq dernières années et une réduction de l’âge de dix ans. Toutes les autres valeurs de caractéristiques sont égales aux valeurs de x. Chaque candidat est ensuite évalué à l’aide des quatre fonctions objectives ci-dessus. Parmi eux, nous sélectionnons au hasard certains candidats, les candidats les plus en forme étant plus susceptibles d’être sélectionnés. Les candidats sont recombinés par paires pour produire des enfants qui leur ressemblent en faisant la moyenne de leurs valeurs de caractéristiques numériques ou en croisant leurs caractéristiques catégorielles. De plus, nous modifions légèrement les valeurs des fonctionnalités des enfants pour explorer tout l’espace des fonctionnalités.\nParmi les deux groupes résultants, un avec les parents et un avec les enfants, nous ne voulons que la meilleure moitié en utilisant deux algorithmes de tri. L’algorithme de tri non dominé trie les candidats selon leurs valeurs objectives. Si les candidats sont également bons, l’algorithme de tri par distance de foule trie les candidats en fonction de leur diversité.\nCompte tenu du classement des deux algorithmes de tri, nous sélectionnons la moitié des candidats la plus prometteuse et/ou la plus diversifiée. Nous utilisons cet ensemble pour la prochaine génération et recommençons le processus de sélection, de recombinaison et de mutation. En répétant les étapes encore et encore, nous espérons approcher un ensemble diversifié de candidats prometteurs avec de faibles valeurs objectives. À partir de cet ensemble, nous pouvons choisir ceux dont nous sommes le plus satisfaits, ou nous pouvons donner un résumé de tous les scénarios contrefactuels en soulignant quelles caractéristiques ont été modifiées et à quelle fréquence.\n\n\n\n9.3.2 - Exemple\nL’exemple suivant est basé sur l’exemple d’ensemble de données sur le crédit de Dandl et al. (2020). L’ensemble de données allemand sur le risque de crédit est disponible sur la plateforme de défis d’apprentissage automatique kaggle.com.\nLes auteurs ont formé une machine à vecteurs de support (avec noyau à base radiale) pour prédire la probabilité qu’un client présente un bon risque de crédit. L’ensemble de données correspondant contient 522 observations complètes et neuf fonctionnalités contenant des informations sur le crédit et les clients.\nL’objectif est de trouver des explications contrefactuelles pour un client avec les valeurs de caractéristiques suivantes :\n\n\n\nage\nsex\nemploi\nlogement\népargne\nmontant\ndurée\nbut\n\n\n\n\n58\nf\nnon qualifié\ngratuit\npetit\n6143\n48\nvoiture\n\n\n\nLe SVM prédit que la femme présente un bon risque de crédit avec une probabilité de 24,2 %. Les hypothèses contrefactuelles devraient expliquer comment les caractéristiques d’entrée doivent être modifiées pour obtenir une probabilité prédite supérieure à 50 % ?\nLe tableau suivant présente les dix meilleurs scénarios contrefactuels :\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nage\nsex\nemploi\nmontant\ndurée\n\\(o_2\\)\n\\(o_3\\)\n\\(o_4\\)\n\\(\\hat{f}(x')\\)\n\n\n\n\n\n\nqualifié\n\n-20\n0.108\n2\n0.036\n0.501\n\n\n\n\nqualifié\n\n-24\n0.114\n2\n0.029\n0.525\n\n\n\n\nqualifié\n\n-22\n0.111\n2\n0.033\n0.513\n\n\n-6\n\nqualifié\n\n-24\n0.126\n3\n0.018\n0.505\n\n\n-3\n\nqualifié\n\n-24\n0.120\n3\n0.024\n0.515\n\n\n-1\n\nqualifié\n\n-24\n0.116\n3\n0.027\n0.522\n\n\n-3\nm\n\n\n-24\n0.195\n3\n0.012\n0.501\n\n\n-6\nm\n\n\n-25\n0.202\n3\n0.011\n0.501\n\n\n-30\nm\nqualifié\n\n-24\n0.285\n4\n0.005\n0.590\n\n\n-4\nm\n\n-1254\n-24\n0.204\n4\n0.002\n0.506\n\n\n\nLes cinq premières colonnes contiennent les modifications de fonctionnalités proposées (seules les fonctionnalités modifiées sont affichées), les trois colonnes suivantes montrent les valeurs objectives (\\(o_1\\) est égal à 0 dans tous les cas) et la dernière colonne affiche la probabilité prédite.\nTous les scénarios contrefactuels ont des probabilités prédites supérieures à \\(50%\\) et ne se dominent pas. Non dominé signifie qu’aucun des scénarios contrefactuels n’a des valeurs plus petites dans tous les objectifs que les autres scénarios contrefactuels. Nous pouvons considérer nos scénarios contrefactuels comme un ensemble de solutions de compromis.\nIls suggèrent tous une réduction de la durée de 48 mois à un minimum de 23 mois, certains d’entre eux proposent que la femme devienne qualifiée plutôt que non qualifiée. Certains contrefactuels suggèrent même de changer le genre de femme à homme, ce qui montre un biais sexiste du modèle. Ce changement s’accompagne toujours d’une réduction de l’âge entre un et 30 ans. Nous pouvons également constater que, bien que certains contrefactuels suggèrent des modifications de quatre caractéristiques, ces contrefactuels sont ceux qui se rapprochent le plus des données de formation.\n\n\n9.3.3 - Avantages\nL’interprétation des explications contrefactuelles est très claire. Si les valeurs des caractéristiques d’une instance sont modifiées en fonction du contrefactuel, la prédiction passe à la prédiction prédéfinie. Il n’y a aucune hypothèse supplémentaire ni aucune magie en arrière-plan. Cela signifie également qu’elle n’est pas aussi dangereuse que des méthodes comme LIME, où il n’est pas clair dans quelle mesure nous pouvons extrapoler le modèle local pour l’interprétation.\nLa méthode contrefactuelle crée une nouvelle instance, mais nous pouvons également résumer une méthode contrefactuelle en signalant quelles valeurs de caractéristiques ont changé. Cela nous donne deux options pour rendre compte de nos résultats. Vous pouvez soit signaler l’instance contrefactuelle, soit mettre en évidence les caractéristiques qui ont été modifiées entre l’instance qui vous intéresse et l’instance contrefactuelle.\nLa méthode contrefactuelle ne nécessite pas d’accès aux données ou au modèle. Il suffit d’accéder à la fonction de prédiction du modèle, qui fonctionnerait également via une API web par exemple. Ceci est intéressant pour les entreprises qui sont auditées par des tiers ou qui proposent des explications aux utilisateurs sans divulguer le modèle ou les données. Une entreprise a intérêt à protéger son modèle et ses données, pour des raisons de secrets commerciaux ou de protection des données. Les explications contrefactuelles offrent un équilibre entre l’explication des prédictions du modèle et la protection des intérêts du propriétaire du modèle.\nLa méthode fonctionne également avec des systèmes qui n’utilisent pas l’apprentissage automatique. Nous pouvons créer des contrefactuels pour tout système qui reçoit des entrées et renvoie des sorties. Le système qui prédit les loyers des appartements pourrait également être constitué de règles manuscrites, et les explications contrefactuelles fonctionneraient toujours.\nLa méthode d’explication contrefactuelle est relativement facile à mettre en oeuvre, puisqu’il s’agit essentiellement d’une fonction de perte (avec un ou plusieurs objectifs) qui peut être optimisée avec des bibliothèques d’optimiseurs standards. Certains détails supplémentaires doivent être pris en compte, tels que la limitation des valeurs des caractéristiques à des plages significatives (par exemple uniquement des tailles d’appartement positives).\n\n\n9.3.4 - Inconvénients\nPour chaque cas, vous trouverez généralement plusieurs explications contrefactuelles (effet Rashomon). Ceci n’est pas pratique : la plupart des gens préfèrent les explications simples à la complexité du monde réel. C’est aussi un défi pratique. Disons que nous avons généré 23 explications contrefactuelles pour une instance. Les signalons-nous tous ? Seulement le meilleur? Et s’ils étaient tous relativement « bons », mais très différents ? Il faut répondre à ces questions à nouveau pour chaque projet. Il peut également être avantageux d’avoir plusieurs explications contrefactuelles, car les humains peuvent alors sélectionner celles qui correspondent à leurs connaissances antérieures.\n\n\n9.3.5 - Logiciels et alternatives\nLa méthode d’explication contrefactuelle multi-objectif de Dandl et al. est implémenté dans un référentiel GitHub.\nDans le package Python, les auteurs d’Alibi ont implémenté une méthode contrefactuelle simple ainsi qu’une méthode étendue qui utilise des prototypes de classe pour améliorer l’interprétabilité et la convergence des résultats de l’algorithme3.\nKarimi et coll. (2020)4 ont également fourni une implémentation Python de leur algorithme MACE dans un référentiel GitHub. Ils ont traduit les critères nécessaires pour des contrefactuels appropriés en formules logiques et utilisent des solveurs de satisfiabilité pour trouver des contrefactuels qui les satisfont.\nMothilal et coll. (2020)5 ont développé DiCE (Diverse Counterfactual Explanation) pour générer un ensemble diversifié d’explications contrefactuelles basées sur des processus ponctuels déterminants. DiCE implémente à la fois une méthode indépendante du modèle et une méthode basée sur le gradient.\nUne autre façon de rechercher des contrefactuels est l’algorithme Growing Spheres de Laugel et al. (2017)6. Ils n’utilisent pas le mot contrefactuel dans leur article, mais la méthode est assez similaire. Ils définissent également une fonction de perte qui favorise les contrefactuels avec le moins de changements possible dans les valeurs des caractéristiques. Au lieu d’optimiser directement la fonction, ils suggèrent de dessiner d’abord une sphère autour du point d’intérêt, d’échantillonner des points à l’intérieur de cette sphère et de vérifier si l’un des points échantillonnés donne la prédiction souhaitée. Ensuite, ils contractent ou élargissent la sphère en conséquence jusqu’à ce qu’un contrefactuel (rare) soit trouvé et finalement renvoyé.\nAncres de Ribeiro et al. (2018)7 sont à l’opposé des contrefactuels, voir le chapitre sur les règles de portée (ancres).",
    "crumbs": [
      "9 - Méthodes locales indépendantes du modèle",
      "9.3 - Explications contrefactuelles"
    ]
  },
  {
    "objectID": "09-local_model_agnostic_methods/09.3-counterfactual.html#footnotes",
    "href": "09-local_model_agnostic_methods/09.3-counterfactual.html#footnotes",
    "title": "Apprentissage automatique interprétable",
    "section": "Notes de bas de page",
    "text": "Notes de bas de page\n\n\nWachter, Sandra, Brent Mittelstadt, and Chris Russell. “Counterfactual explanations without opening the black box: Automated decisions and the GDPR.” (2017).↩︎\nDandl, Susanne, Christoph Molnar, Martin Binder, Bernd Bischl. “Multi-objective counterfactual explanations”. In: Bäck T. et al. (eds) Parallel Problem Solving from Nature – PPSN XVI. PPSN 2020. Lecture Notes in Computer Science, vol 12269. Springer, Cham (2020).↩︎\nVan Looveren, Arnaud, and Janis Klaise. “Interpretable counterfactual explanations guided by prototypes.” arXiv preprint arXiv:1907.02584 (2019).↩︎\nKarimi, Amir-Hossein, Gilles Barthe, Borja Balle and Isabel Valera. “Model-agnostic counterfactual explanations for consequential decisions.” AISTATS (2020).↩︎\nMothilal, Ramaravind K., Amit Sharma, and Chenhao Tan. “Explaining machine learning classifiers through diverse counterfactual explanations.” Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency. (2020).↩︎\nLaugel, Thibault, Marie-Jeanne Lesot, Christophe Marsala, Xavier Renard, and Marcin Detyniecki. “Inverse classification for comparison-based interpretability in machine learning.” arXiv preprint arXiv:1712.08443 (2017).↩︎\nRibeiro, Marco Tulio, Sameer Singh, and Carlos Guestrin. “Anchors: High-precision model-agnostic explanations.” AAAI Conference on Artificial Intelligence (2018).↩︎",
    "crumbs": [
      "9 - Méthodes locales indépendantes du modèle",
      "9.3 - Explications contrefactuelles"
    ]
  },
  {
    "objectID": "09-local_model_agnostic_methods/09.5-shapley.html",
    "href": "09-local_model_agnostic_methods/09.5-shapley.html",
    "title": "Apprentissage automatique interprétable",
    "section": "",
    "text": "Avertissement\n\n\n\nEn cours de traduction.",
    "crumbs": [
      "9 - Méthodes locales indépendantes du modèle",
      "9.5 - Valeurs de Shapley"
    ]
  },
  {
    "objectID": "09-local_model_agnostic_methods/09.5-shapley.html#valeurs-de-shapley",
    "href": "09-local_model_agnostic_methods/09.5-shapley.html#valeurs-de-shapley",
    "title": "Apprentissage automatique interprétable",
    "section": "9.5 - Valeurs de Shapley",
    "text": "9.5 - Valeurs de Shapley\nUne prédiction peut être expliquée en supposant que chaque valeur de caractéristique de l’instance est un « joueur » dans un jeu où la prédiction est le paiement. Les valeurs de Shapley – une méthode issue de la théorie des jeux coalitionnels – nous indiquent comment répartir équitablement le « paiement » entre les fonctionnalités.\n\n\n\n\n\n\nNote\n\n\n\nVous recherchez un livre pratique et approfondi sur les valeurs SHAP et Shapley ? J’en ai trouvé un pour vous.\n\n\n\n9.5.1 - Idée générale\nSupposons le scénario suivant :\nVous avez entraîné un modèle d’apprentissage automatique pour prédire les prix des appartements. Pour un certain appartement, il prévoit 300 000 € et vous devez expliquer cette prévision. L’appartement a une superficie de 50 m 2 , est situé au 2ème étage, dispose d’un parc à proximité et les chats sont interdits :\n\n\n\nLe prix prévu pour un appartement de $50 m\\(2\\) au 2ème étage avec un parc à proximité et une interdiction de chat est de 300 000 €. Notre objectif est d’expliquer comment chacune de ces valeurs de caractéristiques a contribué à la prédiction.\n\n\nLa prévision moyenne pour tous les appartements est de 310 000 €. Dans quelle mesure chaque valeur de caractéristique a-t-elle contribué à la prédiction par rapport à la prédiction moyenne ?\nLa réponse est simple pour les modèles de régression linéaire. L’effet de chaque fonctionnalité est le poids de la fonctionnalité multiplié par la valeur de la fonctionnalité. Cela ne fonctionne qu’en raison de la linéarité du modèle. Pour les modèles plus complexes, nous avons besoin d’une solution différente. Par exemple, LIME propose des modèles locaux pour estimer les effets. Une autre solution vient de la théorie des jeux coopératifs : la valeur de Shapley, inventée par Shapley (1953)1, est une méthode permettant d’attribuer des paiements aux joueurs en fonction de leur contribution au paiement total. Les joueurs coopèrent au sein d’une coalition et tirent un certain profit de cette coopération.\nJoueurs? Jeu? Paiement ? Quel est le lien avec les prédictions et l’interprétabilité de l’apprentissage automatique ? Le « jeu » est la tâche de prédiction pour une seule instance de l’ensemble de données. Le « gain » est la prédiction réelle pour cette instance moins la prédiction moyenne pour toutes les instances. Les « joueurs » sont les valeurs des caractéristiques de l’instance qui collaborent pour recevoir le gain (= prédire une certaine valeur). Dans notre exemple d’appartement, les valeurs de caractéristique park-nearby, cat-banned et area-50 ont floor-2nd travaillé ensemble pour atteindre la prédiction de 300 000 €. Notre objectif est d’expliquer l’écart entre la prédiction réelle (300 000 €) et la prédiction moyenne (310 000 €) : un écart de -10 000 €.\nLa réponse pourrait être : les park-nearby 30 000 € cotisés ; area-50 contribué 10 000 € ; floor-2nd cotisé 0 € ; cat-banned apporté -50 000 €. Les cotisations s’élèvent à -10 000 €, la prévision finale moins le prix moyen prévisionnel de l’appartement.\nComment calculons-nous la valeur de Shapley pour une caractéristique ?\nLa valeur de Shapley est la contribution marginale moyenne d’une valeur de caractéristique dans toutes les coalitions possibles. Tout est clair maintenant ?\nDans la figure suivante, nous évaluons la contribution de la cat-banned valeur de la caractéristique lorsqu’elle est ajoutée à une coalition de park-nearby et area-50. Nous simulons cela uniquement park-nearby, cat-banned et area-50 sont une coalition en tirant au hasard un autre appartement à partir des données et en utilisant sa valeur pour la caractéristique d’étage La valeur floor-2nd a été remplacée par celle tirée au hasard floor-1st. On prédit ensuite le prix de l’appartement avec cette combinaison (310 000 €). Dans un deuxième temps, nous retirons cat-bannedde la coalition en la remplaçant par une valeur aléatoire de la caractéristique chat autorisé/banni de l’appartement tiré au sort. Dans l’exemple, c’était cat-allowed, mais cela aurait pu être cat-banned à nouveau. Nous prévoyons le prix de l’appartement pour la coalition des park-nearby et area-50 (320 000 €). La contribution de cat-banned était de 310 000 € - 320 000 € = -10 000 €. Cette estimation dépend des valeurs de l’appartement tiré au hasard qui a servi de « donneur » pour les valeurs des caractéristiques du chat et de l’étage. Nous obtiendrons de meilleures estimations si nous répétons cette étape d’échantillonnage et faisons la moyenne des contributions.\n\n\n\nUn échantillon de répétition pour estimer la contribution de cat-bannedà la prédiction lorsqu’il est ajouté à la coalition de park-nearby et area-50.\n\n\nNous répétons ce calcul pour toutes les coalitions possibles. La valeur de Shapley est la moyenne de toutes les contributions marginales à toutes les coalitions possibles. Le temps de calcul augmente de façon exponentielle avec le nombre de fonctionnalités. Une solution pour conserver un temps de calcul gérable consiste à calculer les contributions pour seulement quelques échantillons des coalitions possibles.\nLa figure suivante montre toutes les coalitions de valeurs de caractéristiques nécessaires pour déterminer la valeur de Shapley pour cat-banned. La première ligne montre la coalition sans aucune valeur de caractéristique. Les deuxième, troisième et quatrième lignes montrent différentes coalitions avec une taille croissante, séparées par « | ». Au total, les coalitions suivantes sont possibles :\n\nNo feature values\npark-nearby\narea-50\nfloor-2nd\npark-nearby + area-50\npark-nearby + floor-2nd\narea-50 + floor-2nd\npark-nearby + area-50 + floor-2nd\n\nPour chacune de ces coalitions, nous calculons le prix prévu de l’appartement avec et sans la valeur caractéristique cat-banned et prenons la différence pour obtenir la contribution marginale. La valeur Shapley est la moyenne (pondérée) des contributions marginales. Nous remplaçons les valeurs des caractéristiques qui ne font pas partie d’une coalition par des valeurs de caractéristiques aléatoires de l’ensemble de données d’appartement pour obtenir une prédiction du modèle d’apprentissage automatique.\n\n\n\nLes 8 coalitions nécessaires pour calculer la valeur Shapley exacte de la cat-banned valeur de la caractéristique.\n\n\nSi nous estimons les valeurs de Shapley pour toutes les valeurs de caractéristiques, nous obtenons la distribution complète de la prédiction (moins la moyenne) parmi les valeurs de caractéristiques.\n\n\n9.5.2 - Exemples et interprétation\nL’interprétation de la valeur de Shapley pour la valeur de caractéristique \\(j\\) est la suivante : la valeur de la \\(j^{ième}\\) caractéristique apportée \\(\\phi_j\\) à la prédiction de cette instance particulière par rapport à la prédiction moyenne pour l’ensemble de données.\nLa valeur de Shapley fonctionne à la fois pour la classification (si nous avons affaire à des probabilités) et pour la régression.\nNous utilisons la valeur de Shapley pour analyser les prédictions d’un modèle forestier aléatoire prédisant le cancer du col de l’utérus :\n\n\n\nValeurs Shapley pour une femme dans l’ensemble de données sur le cancer du col de l’utérus. Avec une prédiction de 0,57, la probabilité de cancer de cette femme est de 0,54 supérieure à la prédiction moyenne de 0,03. Le nombre de MST diagnostiquées augmente le plus la probabilité. La somme des contributions donne la différence entre la prévision réelle et moyenne (0,54).\n\n\nPour l’ensemble de données de location de vélos, nous formons également une forêt aléatoire pour prédire le nombre de vélos loués pour une journée, en fonction des informations météorologiques et du calendrier. Les explications créées pour la prédiction aléatoire de la forêt d’un jour particulier :\n\n\n\nValeurs Shapley pour le jour 285. Avec une prévision de 2409 vélos de location, ce jour est de -2108 en dessous de la prévision moyenne de 4518. La situation météorologique et l’humidité ont eu les contributions négatives les plus importantes. La température de cette journée a eu une contribution positive. La somme des valeurs de Shapley donne la différence entre les prévisions réelles et moyennes (-2 108).\n\n\nAttention à interpréter correctement la valeur de Shapley : la valeur de Shapley est la contribution moyenne d’une valeur de caractéristique à la prédiction dans différentes coalitions. La valeur de Shapley n’est PAS la différence de prédiction lorsque nous supprimerions la fonctionnalité du modèle.\n\n\n9.5.3 - La valeur Shapley en détail\nCette section approfondit la définition et le calcul de la valeur de Shapley pour le lecteur curieux. Sautez cette section et allez directement à « Avantages et inconvénients » si les détails techniques ne vous intéressent pas.\nNous nous intéressons à la manière dont chaque fonctionnalité affecte la prédiction d’un point de données. Dans un modèle linéaire, il est facile de calculer les effets individuels. Voici à quoi ressemble une prédiction de modèle linéaire pour une instance de données :\n\\[\\hat{f}(x)=\\beta_0+\\beta_{1}x_{1}+\\ldots+\\beta_{p}x_{p}\\]\noù \\(x\\) est l’instance pour laquelle nous voulons calculer les contributions. Chaque \\(x_j\\) est une valeur de caractéristique, avec \\(j = 1, \\dots, p\\). Le \\(\\beta_j\\) est le poids correspondant à la caractéristique \\(j\\).\nLa contribution \\(\\phi_j\\) de la \\(j^{ème}\\) fonctionnalité sur la prédiction \\(\\hat{f}(x)\\) est:\n\\[\\phi_j(\\hat{f})=\\beta_{j}x_j-E(\\beta_{j}X_{j})=\\beta_{j}x_j-\\beta_{j}E(X_{j})\\]\noù \\(E(\\beta_jX_{j})\\) est l’estimation de l’effet moyen pour la caractéristique j. La contribution est la différence entre l’effet caractéristique moins l’effet moyen. Bon! Nous savons désormais dans quelle mesure chaque fonctionnalité a contribué à la prédiction. Si nous additionnons toutes les contributions aux fonctionnalités pour une instance, le résultat est le suivant :\n\\(\\begin{align*}\\sum_{j=1}^{p}\\phi_j(\\hat{f})=&\\sum_{j=1}^p(\\beta_{j}x_j-E(\\beta_{j}X_{j}))\\\\=&(\\beta_0+\\sum_{j=1}^p\\beta_{j}x_j)-(\\beta_0+\\sum_{j=1}^{p}E(\\beta_{j}X_{j}))\\\\=&\\hat{f}(x)-E(\\hat{f}(X))\\end{align*}\\)\nIl s’agit de la valeur prédite pour le point de données \\(x\\) moins la valeur prédite moyenne. Les contributions aux fonctionnalités peuvent être négatives.\nPouvons-nous faire la même chose pour n’importe quel type de modèle ? Ce serait formidable d’avoir cela comme un outil indépendant du modèle. Étant donné que nous n’avons généralement pas de poids similaires dans d’autres types de modèles, nous avons besoin d’une solution différente.\nL’aide vient d’endroits inattendus : la théorie des jeux coopératifs. La valeur Shapley est une solution permettant de calculer les contributions de fonctionnalités pour des prédictions uniques pour n’importe quel modèle d’apprentissage automatique.\n\n9.5.3.1 - La valeur Shapley\nLa valeur Shapley est définie via une fonction de valeur \\(val\\) des joueurs en \\(S\\).\nLa valeur Shapley d’une valeur de caractéristique est sa contribution au paiement, pondérée et additionnée sur toutes les combinaisons possibles de valeurs de caractéristique :\n\\[\\phi_j(val)=\\sum_{S\\subseteq\\{1,\\ldots,p\\} \\backslash \\{j\\}}\\frac{|S|!\\left(p-|S|-1\\right)!}{p!}\\left(val\\left(S\\cup\\{j\\}\\right)-val(S)\\right)\\]\noù \\(S\\) est un sous-ensemble des fonctionnalités utilisées dans le modèle, \\(x\\) est le vecteur des valeurs des fonctionnalités de l’instance à expliquer et \\(p\\) le nombre de fonctionnalités. \\(val_x(S)\\) est la prédiction des valeurs de caractéristiques de l’ensemble \\(S\\) qui sont marginalisées par rapport aux caractéristiques qui ne sont pas incluses dans l’ensemble \\(S\\) :\n\\[val_{x}(S)=\\int\\hat{f}(x_{1},\\ldots,x_{p})d\\mathbb{P}_{x\\notin{}S}-E_X(\\hat{f}(X))\\]\nVous effectuez en fait plusieurs intégrations pour chaque fonctionnalité qui n’est pas contenue \\(S\\). Un exemple concret : Le modèle d’apprentissage automatique fonctionne avec 4 fonctionnalités \\(x1\\), \\(x2\\), \\(x3\\) et \\(x4\\) et nous évaluons la prédiction pour la coalition \\(S\\) composée des valeurs de fonctionnalités \\(x1\\) et \\(x3\\) :\n\\[val_{x}(S)=val_{x}(\\{1,3\\})=\\int_{\\mathbb{R}}\\int_{\\mathbb{R}}\\hat{f}(x_{1},X_{2},x_{3},X_{4})d\\mathbb{P}_{X_2X_4}-E_X(\\hat{f}(X))\\]\nCela ressemble aux contributions de fonctionnalités dans le modèle linéaire !\nNe vous laissez pas tromper par les nombreuses utilisations du mot « valeur » : la valeur de caractéristique est la valeur numérique ou catégorielle d’une caractéristique et d’une instance ; la valeur de Shapley est la contribution des caractéristiques à la prédiction ; la fonction de valeur est la fonction de paiement pour les coalitions de joueurs (valeurs de caractéristiques).\nLa valeur Shapley est la seule méthode d’attribution qui satisfait aux propriétés Efficacité, Symétrie, Factice et Additivité, qui, ensemble, peuvent être considérées comme une définition d’un paiement équitable.\nEfficacité : Les contributions des fonctionnalités doivent correspondre à la différence de prédiction pour x et à la moyenne.\n\\[\\sum\\nolimits_{j=1}^p\\phi_j=\\hat{f}(x)-E_X(\\hat{f}(X))\\]\nSymétrie : Les contributions de deux valeurs de caractéristiques j et k devraient être les mêmes si elles contribuent également à toutes les coalitions possibles. Si\n\\[val(S \\cup \\{j\\})=val(S\\cup\\{k\\})\\]\npour tout\n\\[S\\subseteq\\{1,\\ldots, p\\} \\backslash \\{j,k\\}\\]\nalors\n\\[\\phi_j = \\phi_{k}\\]\nFactice : Une caractéristique j qui ne modifie pas la valeur prédite – quelle que soit la coalition de valeurs de caractéristiques à laquelle elle est ajoutée – devrait avoir une valeur Shapley de 0. Si\n\\[val(S\\cup\\{j\\})=val(S)\\]\npour tout\n\\[S\\subseteq\\{1,\\ldots,p\\}\\]\nalors\n\\[\\phi_j=0\\]\nAdditivité : Pour un jeu avec des gains combinés val+val + les valeurs Shapley respectives sont les suivantes :\n\\[\\phi_j + \\phi_j^{+}\\]\nSupposons que vous ayez entraîné une forêt aléatoire, ce qui signifie que la prédiction est une moyenne de plusieurs arbres de décision. La propriété Additivité garantit que pour une valeur de caractéristique, vous pouvez calculer la valeur Shapley pour chaque arbre individuellement, faire la moyenne et obtenir la valeur Shapley pour la valeur de caractéristique de la forêt aléatoire.\n\n\n9.5.3.2 - Intuitions\nUne manière intuitive de comprendre la valeur de Shapley est l’illustration suivante : Les valeurs de caractéristique entrent dans une pièce dans un ordre aléatoire. Toutes les valeurs caractéristiques de la salle participent au jeu (= contribuent à la prédiction). La valeur Shapley d’une valeur de caractéristique est le changement moyen de la prédiction que la coalition déjà présente dans la salle reçoit lorsque la valeur de caractéristique la rejoint.\n\n\n9.5.3.3 - Estimation de la valeur de Shapley\nToutes les coalitions (ensembles) possibles de valeurs de caractéristiques doivent être évaluées avec et sans la j-ième caractéristique pour calculer la valeur exacte de Shapley. Pour plusieurs fonctionnalités, la solution exacte à ce problème devient problématique à mesure que le nombre de coalitions possibles augmente de façon exponentielle à mesure que davantage de fonctionnalités sont ajoutées. Strumbelj et coll. (2014)2 proposent une approximation avec échantillonnage de Monte-Carlo :\n\\[\\hat{\\phi}_{j}=\\frac{1}{M}\\sum_{m=1}^M\\left(\\hat{f}(x^{m}_{+j})-\\hat{f}(x^{m}_{-j})\\right)\\]\noù \\(\\hat{f}(x^{m}_{+j})\\) est la prédiction pour \\(x\\), mais avec un nombre aléatoire de valeurs de caractéristiques remplacées par des valeurs de caractéristiques provenant d’un point de données aléatoire \\(z\\), à l’exception de la valeur respective de la caractéristique \\(j\\). Le vecteur \\(x^{m}_{-j}\\) est presque identique à \\(x^{m}_{+j}\\), mais la valeur \\(x_j^{m}\\) est également tiré du \\(z\\) échantillonné. Chacune de ces nouvelles instances \\(M\\) est une sorte de « Monstre de Frankenstein » assemblé à partir de deux instances. Notez que dans l’algorithme suivant, l’ordre des entités n’est pas réellement modifié : chaque entité reste à la même position vectorielle lorsqu’elle est transmise à la fonction de prédiction. L’ordre n’est utilisé ici que comme « astuce » : en donnant un nouvel ordre aux fonctionnalités, nous obtenons un mécanisme aléatoire qui nous aide à composer le « Monstre de Frankenstein ». Pour les fonctionnalités qui apparaissent à gauche de la fonctionnalité \\(x_j\\), nous prenons les valeurs des observations originales, et pour les caractéristiques de droite, nous prenons les valeurs d’une instance aléatoire.\nEstimation Shapley approximative pour une valeur de caractéristique unique : - Sortie : valeur Shapley pour la valeur de la \\(j^{ème}\\) fonctionnalité - Obligatoire : nombre d’itérations \\(M\\), instance d’intérêt \\(x\\), index de fonctionnalités \\(j\\), matrice de données \\(X\\) et modèle d’apprentissage automatique \\(f\\) - Pour tout \\(m = 1, \\dots, M\\) : - Dessinez une instance aléatoire \\(z\\) à partir de la matrice de données \\(X\\) - Choisissez une permutation aléatoire des valeurs des caractéristiques - Instance de commande \\(x\\) : \\(x_o=(x_{(1)},\\ldots,x_{(j)},\\ldots,x_{(p)})\\) - Instance de commande \\(z\\) : \\(z_o=(z_{(1)},\\ldots,z_{(j)},\\ldots,z_{(p)})\\) - Construire deux nouvelles instances - Avec \\(j\\) : \\(x_{+j}=(x_{(1)},\\ldots,x_{(j-1)},x_{(j)},z_{(j+1)},\\ldots,z_{(p)})\\) - Sans \\(j\\) : \\(x_{-j}=(x_{(1)},\\ldots,x_{(j-1)},z_{(j)},z_{(j+1)},\\ldots,z_{(p)})\\) - Calculer la contribution marginale : \\(\\phi_j^{m}=\\hat{f}(x_{+j})-\\hat{f}(x_{-j})\\) - Calculez la valeur de Shapley comme moyenne : \\(\\phi_j(x)=\\frac{1}{M}\\sum_{m=1}^M\\phi_j^{m}\\)\nTout d’abord, sélectionnez une instance d’intérêt \\(x\\), une fonctionnalité \\(j\\) et le nombre d’itérations \\(M\\). Pour chaque itération, une instance aléatoire \\(z\\) est sélectionnée à partir des données et un ordre aléatoire des fonctionnalités est généré. Deux nouvelles instances sont créées en combinant les valeurs de l’instance d’intérêt \\(x\\) et de l’échantillon \\(z\\). L’exemple \\(x_{+j}\\) est l’instance qui nous intéresse, mais toutes les valeurs dans l’ordre après la caractéristique \\(j\\) sont remplacées par les valeurs de caractéristiques de l’échantillon z. L’exemple \\(x_{-j}\\) est le même que \\(x_{+j}\\), mais en plus la fonctionnalité \\(j\\) est remplacée par la valeur de la fonctionnalité \\(j\\) de l’échantillon \\(z\\). La différence entre la prédiction et la boîte noire est calculée :\n\\[\\phi_j^{m}=\\hat{f}(x^m_{+j})-\\hat{f}(x^m_{-j})\\]\nToutes ces différences sont moyennées et donnent :\n\\[\\phi_j(x)=\\frac{1}{M}\\sum_{m=1}^M\\phi_j^{m}\\]\nLa moyenne pondère implicitement les échantillons selon la distribution de probabilité de X.\nLa procédure doit être répétée pour chacune des caractéristiques afin d’obtenir toutes les valeurs de Shapley.\n\n\n\n9.5.4 - Avantages\nLa différence entre la prédiction et la prédiction moyenne est équitablement répartie entre les valeurs de caractéristiques de l’instance – la propriété Efficacité des valeurs de Shapley. Cette propriété distingue la valeur Shapley des autres méthodes telles que LIME . LIME ne garantit pas que la prédiction soit équitablement répartie entre les fonctionnalités. La valeur de Shapley pourrait être la seule méthode permettant de fournir une explication complète. Dans les situations où la loi exige l’explicabilité – comme le « droit à des explications » de l’UE – la valeur Shapley pourrait être la seule méthode juridiquement conforme, car elle repose sur une théorie solide et répartit équitablement les effets. Je ne suis pas avocat, cela reflète donc uniquement mon intuition concernant les exigences.\nLa valeur de Shapley permet des explications contrastées. Au lieu de comparer une prédiction à la prédiction moyenne de l’ensemble de données, vous pouvez la comparer à un sous-ensemble ou même à un seul point de données. Cette contrastivité est également quelque chose que les modèles locaux comme LIME ne possèdent pas.\nLa valeur de Shapley est la seule méthode d’explication dotée d’une théorie solide. Les axiomes – efficacité, symétrie, muette, additivité – donnent à l’explication une base raisonnable. Des méthodes telles que LIME supposent un comportement linéaire du modèle d’apprentissage automatique localement, mais il n’existe aucune théorie expliquant pourquoi cela devrait fonctionner.\nIl est époustouflant d’expliquer une prédiction comme un jeu joué par les valeurs des caractéristiques.\n\n\n9.5.5 - Inconvénients\nLa valeur de Shapley nécessite beaucoup de temps de calcul. Dans \\(99,9%\\) des problèmes du monde réel, seule la solution approximative est réalisable. Un calcul exact de la valeur de Shapley est coûteux en termes de calcul car il existe 2 k coalitions possibles de valeurs de caractéristiques et « l’absence » d’une caractéristique doit être simulée en tirant des instances aléatoires, ce qui augmente la variance pour l’estimation des valeurs de Shapley. Le nombre exponentiel des coalitions est traité en échantillonnant les coalitions et en limitant le nombre d’itérations M. Diminuer M réduit le temps de calcul, mais augmente la variance de la valeur de Shapley. Il n’existe pas de bonne règle empirique concernant le nombre d’itérations M. M doit être suffisamment grand pour estimer avec précision les valeurs de Shapley, mais suffisamment petit pour terminer le calcul dans un temps raisonnable. Il devrait être possible de choisir M en fonction des limites de Chernoff, mais je n’ai vu aucun article sur ce faisant pour les valeurs de Shapley pour les prédictions d’apprentissage automatique.\nLa valeur de Shapley peut être mal interprétée. La valeur Shapley d’une valeur de fonctionnalité n’est pas la différence de la valeur prédite après la suppression de la fonctionnalité de la formation du modèle. L’interprétation de la valeur de Shapley est la suivante : étant donné l’ensemble actuel de valeurs de caractéristiques, la contribution d’une valeur de caractéristique à la différence entre la prédiction réelle et la prédiction moyenne est la valeur de Shapley estimée.\nLa valeur Shapley n’est pas la bonne méthode d’explication si vous recherchez des explications clairsemées (explications contenant peu de fonctionnalités). Les explications créées avec la méthode des valeurs de Shapley utilisent toujours toutes les fonctionnalités. Les humains préfèrent les explications sélectives, telles que celles produites par LIME. LIME pourrait être le meilleur choix pour les explications auxquelles les profanes doivent faire face. Une autre solution est SHAP introduite par Lundberg et Lee (2016)3, qui est basée sur la valeur de Shapley, mais peut également fournir des explications avec peu de fonctionnalités.\nLa valeur Shapley renvoie une valeur simple par fonctionnalité, mais aucun modèle de prédiction comme LIME. Cela signifie qu’il ne peut pas être utilisé pour faire des déclarations sur les changements de prédiction en fonction des changements dans les entrées, telles que : “Si je devais gagner 300 € de plus par an, ma cote de crédit augmenterait de 5 points.”\nUn autre inconvénient est que vous devez accéder aux données si vous souhaitez calculer la valeur Shapley pour une nouvelle instance de données. Il ne suffit pas d’accéder à la fonction de prédiction, car vous avez besoin que les données remplacent des parties de l’instance d’intérêt par des valeurs provenant d’instances de données tirées au hasard. Cela ne peut être évité que si vous pouvez créer des instances de données qui ressemblent à des instances de données réelles mais ne sont pas de véritables instances à partir des données d’entraînement.\nComme beaucoup d’autres méthodes d’interprétation basées sur la permutation, la méthode des valeurs de Shapley souffre de l’inclusion d’instances de données irréalistes lorsque les caractéristiques sont corrélées. Pour simuler l’absence d’une valeur de fonctionnalité dans une coalition, nous marginalisons la fonctionnalité. Ceci est réalisé en échantillonnant les valeurs de la distribution marginale de l’entité. C’est très bien tant que les fonctionnalités sont indépendantes. Lorsque les fonctionnalités sont dépendantes, nous pouvons alors échantillonner des valeurs de fonctionnalités qui n’ont pas de sens pour cette instance. Mais nous les utiliserions pour calculer la valeur Shapley de la fonctionnalité. Une solution pourrait consister à permuter les caractéristiques corrélées ensemble et à obtenir une valeur Shapley mutuelle pour elles. Une autre adaptation est l’échantillonnage conditionnel : les fonctionnalités sont échantillonnées en fonction des fonctionnalités qui font déjà partie de l’équipe. Alors que l’échantillonnage conditionnel résout le problème des points de données irréalistes, un nouveau problème est introduit : les valeurs résultantes ne sont plus les valeurs de Shapley dans notre jeu, car elles violent l’axiome de symétrie, comme l’ont découvert Sundararajan et al. (2019)4 et discuté plus en détail par Janzing et al. (2020)5.\n\n\n9.5.6 - Logiciels et alternatives\nLes valeurs Shapley sont implémentées dans les packages iml et fastshap pour R. Dans Julia, vous pouvez utiliser Shapley.jl.\nSHAP, une méthode alternative d’estimation des valeurs de Shapley, est présentée dans le chapitre suivant.\nUne autre approche est appelée breakDown, qui est implémentée dans le breakDown package R6. BreakDown affiche également les contributions de chaque fonctionnalité à la prédiction, mais les calcule étape par étape. Réutilisons l’analogie du jeu : nous commençons avec une équipe vide, ajoutons la valeur de fonctionnalité qui contribuerait le plus à la prédiction et itérons jusqu’à ce que toutes les valeurs de fonctionnalité soient ajoutées. La contribution de chaque valeur de fonctionnalité dépend des valeurs de fonctionnalité respectives qui sont déjà dans « l’équipe », ce qui est le gros inconvénient de la méthode breakDown. Elle est plus rapide que la méthode des valeurs de Shapley et pour les modèles sans interactions, les résultats sont les mêmes.",
    "crumbs": [
      "9 - Méthodes locales indépendantes du modèle",
      "9.5 - Valeurs de Shapley"
    ]
  },
  {
    "objectID": "09-local_model_agnostic_methods/09.5-shapley.html#footnotes",
    "href": "09-local_model_agnostic_methods/09.5-shapley.html#footnotes",
    "title": "Apprentissage automatique interprétable",
    "section": "Notes de bas de page",
    "text": "Notes de bas de page\n\n\nShapley, Lloyd S. “A value for n-person games.” Contributions to the Theory of Games 2.28 (1953): 307-317.↩︎\nŠtrumbelj, Erik, and Igor Kononenko. “Explaining prediction models and individual predictions with feature contributions.” Knowledge and information systems 41.3 (2014): 647-665.↩︎\nLundberg, Scott M., and Su-In Lee. “A unified approach to interpreting model predictions.” Advances in Neural Information Processing Systems (2017).↩︎\nSundararajan, Mukund, and Amir Najmi. “The many Shapley values for model explanation.” arXiv preprint arXiv:1908.08474 (2019).↩︎\nJanzing, Dominik, Lenon Minorics, and Patrick Blöbaum. “Feature relevance quantification in explainable AI: A causal problem.” International Conference on Artificial Intelligence and Statistics. PMLR (2020).↩︎\nStaniak, Mateusz, and Przemyslaw Biecek. “Explanations of model predictions with live and breakDown packages.” arXiv preprint arXiv:1804.01955 (2018).↩︎",
    "crumbs": [
      "9 - Méthodes locales indépendantes du modèle",
      "9.5 - Valeurs de Shapley"
    ]
  },
  {
    "objectID": "10-neuralnet/10.1-learned-features.html",
    "href": "10-neuralnet/10.1-learned-features.html",
    "title": "Apprentissage automatique interprétable",
    "section": "",
    "text": "Avertissement\n\n\n\nEn cours de traduction.",
    "crumbs": [
      "10 - Interprétation d'un réseau de neurone",
      "10.1 - Caractéristiques apprises"
    ]
  },
  {
    "objectID": "10-neuralnet/10.1-learned-features.html#caractéristiques-apprises",
    "href": "10-neuralnet/10.1-learned-features.html#caractéristiques-apprises",
    "title": "Apprentissage automatique interprétable",
    "section": "10.1 - Caractéristiques apprises",
    "text": "10.1 - Caractéristiques apprises\n\nLes réseaux neuronaux convolutionnels apprennent des caractéristiques et des concepts abstraits à partir de pixels d’images bruts. La visualisation de caractéristiques visualise les caractéristiques apprises par maximisation de l’activation. La dissection de réseau étiquette les unités de réseau neuronal (p. ex. les canaux) avec des concepts humains.\nLes réseaux neuronaux profonds apprennent des caractéristiques de haut niveau dans les couches cachées. C’est l’une de leurs plus grandes forces et réduit le besoin d’ingénierie des caractéristiques. Supposons que vous voulez construire un classificateur d’images avec une machine à vecteurs de support. Les matrices de pixels bruts ne constituent pas la meilleure entrée pour entraîner votre SVM, donc vous créez de nouvelles caractéristiques basées sur la couleur, le domaine fréquentiel, les détecteurs de bords, etc. Avec les réseaux neuronaux convolutionnels, l’image est introduite dans le réseau sous sa forme brute (i.e. les pixels). Le réseau transforme l’image à de nombreuses reprises. D’abord, l’image passe par de plusieurs couches convolutionnelles. Dans ces couches convolutionnelles, le réseau y apprend de nouvelles caractéristiques de plus en plus complexes. Ensuite, l’information de l’image transformée passe à travers les couches entièrement connectées et se transforme en une classification ou une prédiction.\n\n\n\n\nFeatures learned by a convolutional neural network (Inception V1) trained on the ImageNet data. The features range from simple features in the lower convolutional layers (left) to more abstract features in the higher convolutional layers (right). Figure from Olah, et al. (2017, CC-BY 4.0) https://distill.pub/2017/feature-visualization/appendix/.\n\n\n\nLa ou les premières couches convolutionnelles apprennent des caractéristiques telles que les bords et les textures simples.\nLes couches convolutionnelles suivantes apprennent des caractéristiques telles que des textures et des motifs plus complexes.\nLes dernières couches convolutionnelles apprennent des caractéristiques telles que des objets ou des parties d’objets.\nLes couches entièrement connectées apprennent à relier les activations des caractéristiques de haut niveau aux classes individuelles à prédire.\n\nGénial. Mais comment obtenons-nous réellement ces images hallucinatoires ?\n\n10.1.1 - Visualisation des caractéristiques\nL’approche consistant à rendre explicites les caractéristiques apprises est appelée visualisation de caractéristiques. La visualisation de caractéristiques pour une unité d’un réseau neuronal se fait en trouvant l’entrée qui maximise l’activation de cette “unité”.\n“Unité” fait référence soit à des neurones individuels, des canaux (également appelés cartes de caractéristiques), des couches entières ou à la probabilité de classe finale en classification (ou le neurone pré-softmax correspondant, ce qui est recommandé). Les neurones individuels sont des unités atomiques du réseau, donc nous obtiendrions le plus d’informations en créant des visualisations de caractéristiques pour chaque neurone. Mais il y a un problème : Les réseaux neuronaux contiennent souvent des millions de neurones. Examiner la visualisation de caractéristiques de chaque neurone prendrait trop de temps. Les canaux (parfois appelés cartes d’activation) en tant qu’unités sont un bon choix pour la visualisation de caractéristiques. Nous pouvons aller plus loin et visualiser une couche convolutionnelle entière. Les couches en tant qu’unité sont utilisées pour le DeepDream de Google, qui ajoute de manière répétée les caractéristiques visualisées d’une couche à l’image originale, résultant en une version onirique de l’entrée.\n\n\n\nFeature visualization can be done for different units. A) Convolution neuron, B) Convolution channel, C) Convolution layer, D) Neuron, E) Hidden layer, F) Class probability neuron (or corresponding pre-softmax neuron)\n\n\n\n\n10.1.1.1 Visualisation des caractéristiques par optimisation\nEn termes mathématiques, la visualisation de caractéristiques est un problème d’optimisation. Nous supposons que les poids du réseau neuronal sont fixes, ce qui signifie que le réseau est entraîné. Nous cherchons une nouvelle image qui maximise l’activation (moyenne) d’une unité, ici un seul neurone :\n\\[img^*=\\arg\\max_{img}h_{n,x,y,z}(img)\\]\nLa fonction \\(h\\) correspond à l’activation du neurone, \\(img\\) les données d’entrée (une image) du réseau, \\(x\\) et \\(y\\) décrive la position spatiale du neurone, \\(n\\) identifie la couche et \\(z\\) l’indice du canal.\n\\[img^*=\\arg\\max_{img}\\sum_{x,y}h_{n,x,y,z}(img)\\]\nDans cette formule, tous les neurones dans le canal \\(z\\) sont pondérés de manière égale. Alternativement, vous pouvez également maximiser des directions aléatoires, ce qui signifie que les neurones seraient multipliés par différents paramètres, y compris des directions négatives. De cette façon, nous étudions comment les neurones interagissent au sein du canal. Au lieu de maximiser l’activation, vous pouvez également la minimiser (ce qui correspond à maximiser la direction négative). De manière intéressante, lorsque vous maximisez la direction négative, vous obtenez des caractéristiques très différentes pour la même unité :\n\n\n\nPositive (left) and negative (right) activation of Inception V1 neuron 484 from layer mixed4d pre relu. While the neuron is maximally activated by wheels, something which seems to have eyes yields a negative activation.\n\n\nNous pouvons aborder ce problème d’optimisation de différentes manières. Par exemple, au lieu de générer de nouvelles images, nous pourrions parcourir nos images d’entraînement et sélectionner celles qui maximisent l’activation. C’est une approche valable, mais utiliser des données d’entraînement présente le problème que les éléments sur les images peuvent être corrélés et nous ne pouvons pas voir ce que le réseau neuronal recherche réellement. Si les images qui génèrent une haute activation d’un certain canal montrent un chien et une balle de tennis, nous ne savons pas si le réseau neuronal se concentre sur le chien, la balle de tennis ou peut-être sur les deux.\nUne autre approche consiste à générer de nouvelles images, en partant de bruit aléatoire. Pour obtenir des visualisations significatives, il y a généralement des contraintes sur l’image, par exemple que seuls de petits changements sont autorisés. Pour réduire le bruit dans la visualisation des caractéristiques, vous pouvez appliquer du jittering, de la rotation ou du redimensionnement à l’image avant l’étape d’optimisation. D’autres options de régularisation incluent la pénalisation de fréquence (par exemple, réduire la variance des pixels voisins) ou générer des images avec des prioris appris, par exemple avec des réseaux adverses génératifs (GANs)1 ou des autoencodeurs débruitants2.\n\n\n\nIterative optimization from random image to maximizing activation. Olah, et al. 2017 (CC-BY 4.0), https://distill.pub/2017/feature-visualization/.\n\n\nSi vous désirez approdonfir le sujet de visualisation des caractéristiques, jetez une oeil au journal en ligne distill.pub, particulièrement l’article posté par Olah et al (2017)3, dont j’ai utilisé de nombreuses images. Je recommande également l’article sur la construction de blocs d’interprétation4.\n\n\n10.1.1.2 Lien avec les exemples adverses\nIl existe un lien entre la visualisation de caractéristiques et les exemples adverses : les deux techniques maximisent l’activation d’une unité de réseau neuronal. Pour les exemples adverses, nous recherchons l’activation maximale du neurone pour la classe adverse (= incorrecte). Une différence réside dans l’image avec laquelle nous commençons : pour les exemples adverses, il s’agit de l’image pour laquelle nous voulons générer l’image adverse. Pour la visualisation de caractéristiques, c’est, selon l’approche, du bruit aléatoire.\n\n\n10.1.1.3 Données textuelles et tabulaires\nLa littérature se concentre sur la visualisation des caractéristiques pour les réseaux neuronaux convolutionnels destinés à la reconnaissance d’images. Techniquement, rien ne vous empêche de trouver l’entrée qui active au maximum un neurone d’un réseau neuronal entièrement connecté pour des données tabulaires ou d’un réseau neuronal récurrent pour des données textuelles. Vous pourriez ne plus appeler cela “visualisation de caractéristiques”, puisque le “caractéristique” serait une entrée de données tabulaires ou textuelle. Pour la prédiction de défaut de crédit, les entrées pourraient être le nombre de crédits antérieurs, le nombre de contrats mobiles, l’adresse et des dizaines d’autres caractéristiques. La caractéristique apprise d’un neurone serait alors une certaine combinaison des dizaines de caractéristiques. Pour les réseaux neuronaux récurrents, il est un peu plus agréable de visualiser ce que le réseau a appris : Karpathy et al. (2015)5 ont montré que les réseaux neuronaux récurrents ont effectivement des neurones qui apprennent des caractéristiques interprétables. Ils ont entraîné un modèle au niveau des caractères, qui prédit le prochain caractère dans la séquence à partir des caractères précédents. Une fois qu’une parenthèse ouvrante “(” survenait, l’un des neurones était fortement activé, et se désactivait lorsque la parenthèse fermante correspondante “)” survenait. D’autres neurones se déclenchaient à la fin d’une ligne. Certains neurones se déclenchaient dans les URL. La différence avec la visualisation des caractéristiques pour les CNN est que les exemples n’ont pas été trouvés par optimisation, mais en étudiant les activations des neurones dans les données d’entraînement.\nCertaines des images semblent montrer des concepts bien connus comme des museaux de chiens ou des bâtiments. Mais comment pouvons-nous en être sûrs ? La méthode de dissection de réseau relie les concepts humains avec des unités individuelles de réseau neuronal. Alerte de divulgâchage : la dissection de réseau nécessite des jeux de données supplémentaires que quelqu’un a étiquetés avec des concepts humains.\n\n\n\n10.1.2 Dissection d’un réseau\nL’approche de Dissection de Réseau par Bau & Zhou et al. (2017)6 quantifie l’interprétabilité d’une unité d’un réseau neuronal convolutionnel. Elle relie les zones fortement activées des canaux de CNN avec des concepts humains (objets, parties, textures, couleurs, …).\nLes canaux d’un réseau neuronal convolutionnel apprennent de nouvelles caractéristiques, comme nous l’avons vu dans le chapitre consacré à la visualisation de caractéristiques. Mais ces visualisations ne prouvent pas qu’une unité a appris un certain concept. Nous n’avons pas non plus de mesure pour évaluer à quel point une unité détecte, par exemple, les gratte-ciels. Avant d’entrer dans les détails de la Dissection de Réseau, nous devons parler de la grande hypothèse qui est derrière cette ligne de recherche. L’hypothèse est la suivante : Les unités d’un réseau neuronal (comme les canaux convolutionnels) apprennent des concepts désenchevêtrés.\nLa Question des caractéristiques désenchevêtrées\nLes réseaux neuronaux (convolutionnels) apprennent-ils des caractéristiques désenchevêtrées ? Les caractéristiques désenchevêtrées signifient que des unités individuelles du réseau détectent des concepts spécifiques du monde réel. Le canal convolutionnel 394 pourrait détecter des gratte-ciels, le canal 121 des museaux de chiens, le canal 12 des rayures à un angle de 30 degrés… L’opposé d’un réseau désenchevêtré est un réseau complètement enchevêtré. Dans un réseau complètement enchevêtré, par exemple, il n’y aurait pas d’unité individuelle pour les museaux de chiens. Tous les canaux contribueraient à la reconnaissance des museaux de chiens.\nLes caractéristiques désenchevêtrées impliquent que le réseau est hautement interprétable. Supposons que nous ayons un réseau avec des unités complètement désenchevêtrées qui sont étiquetées avec des concepts connus. Cela ouvrirait la possibilité de suivre le processus de prise de décision du réseau. Par exemple, nous pourrions analyser comment le réseau classe les loups par rapport aux huskies. D’abord, nous identifions l’unité “husky”. Nous pouvons vérifier si cette unité dépend des unités “museau de chien”, “fourrure duveteuse” et “neige” de la couche précédente. Si c’est le cas, nous savons qu’elle classera par erreur une image d’un husky avec un arrière-plan enneigé comme un loup. Dans un réseau désenchevêtré, nous pourrions identifier des corrélations non causales problématiques. Nous pourrions automatiquement lister toutes les unités fortement activées et leurs concepts pour expliquer une prédiction individuelle. Nous pourrions facilement détecter les biais dans le réseau neuronal. Par exemple, le réseau a-t-il appris une caractéristique de “peau blanche” pour prédire un salaire ?\nAlerte de divulgâchage : les réseaux neuronaux convolutionnels ne sont pas parfaitement désenchevêtrés. Nous allons maintenant examiner de plus près la Dissection de Réseau pour découvrir à quel point les réseaux neuronaux sont interprétables.\n\n10.1.2.1 - Algorithme de dissection de réseau\nLa Dissection de Réseau comporte trois étapes :\n\nObtenir des images avec des concepts visuels étiquetés par des humains, allant des rayures aux gratte-ciels.\nMesurer les activations des canaux CNN pour ces images.\nQuantifier l’alignement des activations et des concepts étiquetés.\n\nLa figure suivante visualise comment une image est transmise à un canal et associée aux concepts étiquetés.\n\n\n\nFor a given input image and a trained network (fixed weights), we propagate the image forward to the target layer, upscale the activations to match the original image size and compare the maximum activations with the ground truth pixel-wise segmentation. Figure originally from http://netdissect.csail.mit.edu/.\n\n\nÉtape 1 : Le jeu de données Broden\nLa première étape difficile mais cruciale est la collecte de données. La dissection de réseau nécessite des images étiquetées pixel par pixel avec des concepts de différents niveaux d’abstraction (des couleurs aux scènes de rue). Bau & Zhou et al. ont combiné plusieurs jeux de données avec des concepts pixel par pixel. Ils ont nommé ce nouveau jeu de données Broden, qui signifie que les données sont largement et densément étiquetées. Le jeu de données Broden est segmenté principalement au niveau du pixel, pour certains jeux de données, toute l’image est étiquetée. Broden contient 60 000 images avec plus de 1 000 concepts visuels à différents niveaux d’abstraction : 468 scènes, 585 objets, 234 parties, 32 matériaux, 47 textures et 11 couleurs.\nÉtape 2 : Récupération des activations du réseau\nEnsuite, nous créons les masques des zones les plus activées par canal et par image. À ce stade, les étiquettes de concept ne sont pas encore impliquées.\n\nPour chaque canal convolutionnel \\(k\\) :\n\nPour chaque image \\(x\\) dans le jeu de données Broden :\n\nPropager l’image \\(x\\) vers la couche cible contenant le canal \\(k\\).\nExtraire les activations de pixels du canal convolutionnel \\(k\\) : \\(A_k(x)\\).\n\nCalculer la distribution des activations de pixels \\(\\alpha_k\\) sur toutes les images.\nDéterminer le niveau du quantile à \\(0,995\\) de activations \\(\\alpha_k\\). Cela signifie que \\(0,5%\\) de toutes les activations du canal \\(k\\) pour l’image \\(x\\) sont supérieures à \\(T_k\\).\nPour chaque image x dans le jeu de données Broden :\n\nMettre à l’échelle la carte d’activation (éventuellement) de résolution inférieure \\(A_k(x)\\) à la résolution de l’image \\(x\\). Nous appelons le résultat \\(S_k(x)\\).\nBinariser la carte d’activation : Un pixel est soit activé soit désactivé, selon qu’il dépasse ou non le seuil d’activation \\(T_k\\). Le nouveau masque est \\(M_k(x) = S_k(x) \\geq T_k(x)\\).\n\n\n\nÉtape 3 : Alignement activation-concept\nAprès l’étape 2, nous avons un masque d’activation par canal et par image. Ces masques d’activation marquent les zones fortement activées. Pour chaque canal, nous voulons trouver le concept humain qui active ce canal. Nous trouvons le concept en comparant les masques d’activation avec tous les concepts étiquetés. Nous quantifions l’alignement entre le masque d’activation k et le masque de concept c avec le score d’Intersection sur Union (IoU) :\n\\[IoU_{k,c} = \\frac{\\sum |M_k(x) \\bigcap L_c(x)|}{\\sum |M_k(x) \\bigcup L_c(x)|}\\]\noù \\(|\\cdot|\\) est la cardinalité d’un ensemble. L’intersection sur union compare l’alignement entre deux zones. \\(IoU_{k,c}\\) peut être interprété comme la précision avec laquelle l’unité k détecte le concept \\(c\\). Nous appelons l’unité \\(k\\) un détecteur du concept c lorsque \\(IoU_{k,c} &gt; 0,04\\). Ce seuil a été choisi par Bau & Zhou et al (2017).\nLa figure suivante illustre l’intersection et l’union du masque d’activation et du masque de concept pour une seule image :\n\n\n\nThe Intersection over Union (IoU) is computed by comparing the human ground truth annotation and the top activated pixels.\n\n\n\n\n\nActivation mask for inception - 4e channel 750 which detects dogs with \\(IoU=0.203\\). Figure originally from http://netdissect.csail.mit.edu/\n\n\n\n\n10.1.2.2 - Expériences\nLes auteurs de la dissection de réseau ont entraîné différentes architectures de réseaux (AlexNet, VGG, GoogleNet, ResNet) à partir de zéro sur différents jeux de données (ImageNet, Places205, Places365). ImageNet contient 1,6 million d’images de 1000 classes axées sur les objets. Places205 et Places365 contiennent respectivement 2,4 millions / 1,6 million d’images de 205 / 365 scènes différentes. Les auteurs ont également entraîné AlexNet sur des tâches d’apprentissage auto-supervisées telles que la prédiction de l’ordre des images vidéo ou la colorisation des images. Pour bon nombre de ces différents paramètres, ils ont compté le nombre de détecteurs de concepts uniques comme mesure de l’interprétabilité. Voici quelques-unes des découvertes :\n\nLes réseaux détectent des concepts de niveau inférieur (couleurs, textures) dans les couches inférieures et des concepts de niveau supérieur (parties, objets) dans les couches supérieures. Nous avons déjà vu cela dans les Visualisations de Caractéristiques.\nLa normalisation par lots réduit le nombre de détecteurs de concepts uniques.\nDe nombreuses unités détectent le même concept. Par exemple, il y a 95 (!) canaux de chien dans VGG entraîné sur ImageNet en utilisant \\(IoU \\geq 0,04\\) comme seuil de détection (4 dans conv4_3, 91 dans conv5_3, voir site web du projet).\nAugmenter le nombre de canaux dans une couche augmente le nombre d’unités interprétables.\nLes initialisations aléatoires (entraînement avec différentes graines aléatoires) entraînent des nombres légèrement différents d’unités interprétables.\nResNet est l’architecture de réseau avec le plus grand nombre de détecteurs uniques, suivi de VGG, GoogleNet et AlexNet en dernier.\nLe plus grand nombre de détecteurs de concepts uniques est appris pour Places356, suivi de Places205 et ImageNet en dernier.\nLe nombre de détecteurs de concepts uniques augmente avec le nombre d’itérations d’entraînement.\nLes réseaux formés sur des tâches auto-supervisées ont moins de détecteurs uniques par rapport aux réseaux formés sur des tâches supervisées.\nEn apprentissage par transfert, le concept d’un canal peut changer. Par exemple, un détecteur de chien est devenu un détecteur de cascade. Cela s’est produit dans un modèle initialement formé pour classifier des objets et ensuite affiné pour classifier des scènes.\nDans l’une des expériences, les auteurs ont projeté les canaux sur une nouvelle base tournée. Cela a été fait pour le réseau VGG formé sur ImageNet. “Tourné” ne signifie pas que l’image a été tournée. “Tourné” signifie que nous prenons les 256 canaux de la couche conv5 et calculons 256 nouveaux canaux comme combinaisons linéaires des canaux originaux. Dans le processus, les canaux deviennent enchevêtrés. La rotation réduit l’interprétabilité, c’est-à-dire que le nombre de canaux alignés sur un concept diminue. La rotation a été conçue pour maintenir la performance du modèle identique. La première conclusion : l’interprétabilité des CNN est dépendante de l’axe. Cela signifie que des combinaisons aléatoires de canaux sont moins susceptibles de détecter des concepts uniques. La deuxième conclusion : l’interprétabilité est indépendante du pouvoir discriminant. Les canaux peuvent être transformés avec des transformations orthogonales tandis que le pouvoir discriminant reste le même, mais l’interprétabilité diminue.\n\nLes auteurs ont également utilisé la dissection de réseau pour les réseaux adverses génératifs (Generative Adversarial Networks : GANs). Vous pouvez trouver la dissection de réseau pour les GANs sur le site web du projet.\n\n\n\n10.1.3 Avantages\nLes visualisations de caractéristiques offrent un aperçu unique du fonctionnement des réseaux neuronaux, en particulier pour la reconnaissance d’images. Étant donné la complexité et l’opacité des réseaux neuronaux, la visualisation des caractéristiques est une étape importante dans l’analyse et la description des réseaux neuronaux. Grâce à la visualisation des caractéristiques, nous avons appris que les réseaux neuronaux apprennent d’abord des détecteurs de bords et de textures simples et des détecteurs de parties et d’objets plus abstraits dans les couches supérieures. La dissection de réseau élargit ces perspectives et rend mesurable l’interprétabilité des unités de réseau.\nLa dissection de réseau nous permet de lier automatiquement des unités à des concepts, ce qui est très pratique.\nLa visualisation de caractéristiques est un excellent outil pour communiquer de manière non technique le fonctionnement des réseaux neuronaux.\nAvec la dissection de réseau, nous pouvons également détecter des concepts au-delà des classes dans la tâche de classification. Mais nous avons besoin de jeux de données contenant des images avec des concepts étiquetés pixel par pixel.\nLa visualisation des caractéristiques peut être combinée avec des méthodes d’attribution de caractéristiques, qui expliquent quels pixels étaient importants pour la classification. La combinaison des deux méthodes permet d’expliquer une classification individuelle ainsi que la visualisation locale des caractéristiques apprises qui ont été impliquées dans la classification. Voir Les Blocs de Construction de l’Interprétabilité de distill.pub.\nEnfin, les visualisations de caractéristiques forment de magnifiques fonds d’écran et impressions sur T-shirt.\n\n\n10.1.4 Inconvénients\nDe nombreuses images de visualisation de caractéristiques ne sont pas interprétables du tout, mais contiennent certaines caractéristiques abstraites pour lesquelles nous n’avons ni mots ni concept mental. L’affichage des visualisations de caractéristiques avec des données d’entraînement peut aider. Les images pourraient toujours ne pas révéler ce à quoi le réseau neuronal a réagi et indiquer seulement quelque chose comme “peut-être doit-il y avoir du jaune dans les images”. Même avec la dissection de réseau, certains canaux ne sont pas liés à un concept humain. Par exemple, la couche conv5_3 de VGG formée sur ImageNet a 193 canaux (sur 512) qui n’ont pas pu être associés à un concept humain.\nIl y a trop d’unités à examiner, même en “visualisant seulement” les activations de canal. Pour l’architecture Inception V1, par exemple, il y a déjà plus de 5000 canaux issus de neuf couches convolutionnelles. Si vous voulez également montrer les activations négatives plus quelques images des données d’entraînement qui activent maximale ou minimalement le canal (disons quatre images positives, quatre images négatives), alors vous devez déjà afficher plus de 50 000 images. Au moins, nous savons – grâce à la dissection de réseau – que nous n’avons pas besoin d’étudier les directions aléatoires.\nIllusion d’interprétabilité ? Les visualisations de caractéristiques peuvent donner l’illusion que nous comprenons ce que fait le réseau neuronal. Mais comprenons-nous vraiment ce qui se passe dans le réseau neuronal ? Même si nous examinons des centaines ou des milliers de visualisations de caractéristiques, nous ne pouvons pas comprendre le réseau neuronal. Les canaux interagissent de manière complexe, les activations positives et négatives ne sont pas liées, plusieurs neurones peuvent apprendre des caractéristiques très similaires et pour beaucoup de ces caractéristiques, nous n’avons pas de concepts humains équivalents. Nous ne devons pas tomber dans le piège de croire que nous comprenons complètement les réseaux neuronaux simplement parce que nous croyons avoir vu que le neurone 349 de la couche 7 est activé par des marguerites. La Dissection de Réseau a montré que des architectures comme ResNet ou Inception ont des unités qui réagissent à certains concepts. Mais l’IoU n’est pas si élevé et souvent de nombreuses unités répondent au même concept et certaines à aucun concept du tout. Les canaux ne sont pas complètement désenchevêtrés et nous ne pouvons pas les interpréter isolément.\nPour la dissection de réseau, vous avez besoin de jeux de données étiquetés au niveau du pixel avec les concepts. Ces jeux de données nécessitent beaucoup d’efforts pour être collectés, car chaque pixel doit être étiqueté, ce qui se fait généralement en dessinant des segments autour des objets sur l’image.\nLa dissection de réseau n’aligne que les concepts humains avec les activations positives mais pas avec les activations négatives des canaux. Comme l’ont montré les visualisations de caractéristiques, les activations négatives semblent être liées à des concepts. Cela pourrait être corrigé en examinant également le quantile inférieur des activations.\n\n\n10.1.5 Logiciels et matériel complémentaire\nIl existe une implémentation open-source de la visualisation de caractéristiques appelée Lucid. Vous pouvez l’essayer facilement dans votre navigateur en utilisant les liens vers les notebooks fournis sur la page GitHub de Lucid. Aucun logiciel supplémentaire n’est nécessaire. D’autres implémentations incluent tf_cnnvis pour TensorFlow, Keras Filters pour Keras et DeepVis pour Caffe.\nLa dissection de réseau dispose d’un excellent site web associé à ce projet. En plus de la publication, il héberge du matériel supplémentaire tel que du code, des données et des visualisations de masques d’activation.",
    "crumbs": [
      "10 - Interprétation d'un réseau de neurone",
      "10.1 - Caractéristiques apprises"
    ]
  },
  {
    "objectID": "10-neuralnet/10.1-learned-features.html#footnotes",
    "href": "10-neuralnet/10.1-learned-features.html#footnotes",
    "title": "Apprentissage automatique interprétable",
    "section": "Notes de bas de page",
    "text": "Notes de bas de page\n\n\nNguyen, Anh, Alexey Dosovitskiy, Jason Yosinski, Thomas Brox, and Jeff Clune. “Synthesizing the preferred inputs for neurons in neural networks via deep generator networks.” Advances in neural information processing systems 29 (2016): 3387-3395.↩︎\nNguyen, Anh, Jeff Clune, Yoshua Bengio, Alexey Dosovitskiy, and Jason Yosinski. “Plug & play generative networks: Conditional iterative generation of images in latent space.” In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 4467-4477. 2017.↩︎\nOlah, Chris, Alexander Mordvintsev, and Ludwig Schubert. “Feature visualization.” Distill 2, no. 11 (2017): e7.↩︎\nOlah, Chris, Arvind Satyanarayan, Ian Johnson, Shan Carter, Ludwig Schubert, Katherine Ye, and Alexander Mordvintsev. “The building blocks of interpretability.” Distill 3, no. 3 (2018): e10.↩︎\nKarpathy, Andrej, Justin Johnson, and Li Fei-Fei. “Visualizing and understanding recurrent networks.” arXiv preprint arXiv:1506.02078 (2015).↩︎\nBau, David, Bolei Zhou, Aditya Khosla, Aude Oliva, and Antonio Torralba. “Network dissection: Quantifying interpretability of deep visual representations.” In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 6541-6549 (2017).↩︎",
    "crumbs": [
      "10 - Interprétation d'un réseau de neurone",
      "10.1 - Caractéristiques apprises"
    ]
  },
  {
    "objectID": "10-neuralnet/10.3-concepts.html",
    "href": "10-neuralnet/10.3-concepts.html",
    "title": "Apprentissage automatique interprétable",
    "section": "",
    "text": "Avertissement\n\n\n\nEn cours de traduction.",
    "crumbs": [
      "10 - Interprétation d'un réseau de neurone",
      "10.3 - Détecter les concepts"
    ]
  },
  {
    "objectID": "10-neuralnet/10.3-concepts.html#détecter-les-concepts",
    "href": "10-neuralnet/10.3-concepts.html#détecter-les-concepts",
    "title": "Apprentissage automatique interprétable",
    "section": "10.3 - Détecter les concepts",
    "text": "10.3 - Détecter les concepts\n\nAuteur: Fangzhou Li @ Université de Californie, Davis\nJusqu’à présent, nous avons rencontré de nombreuses méthodes pour expliquer les modèles “boîte noire” à travers l’attribution de caractéristiques. Cependant, il existe certaines limitations concernant l’approche basée sur les caractéristiques. Premièrement, les caractéristiques ne sont pas nécessairement conviviales en termes d’interprétabilité. Par exemple, l’importance d’un seul pixel dans une image ne transmet généralement pas beaucoup d’interprétation significative. Deuxièmement, l’expressivité d’une explication basée sur les caractéristiques est limitée par le nombre de caractéristiques.\nL’approche basée sur les concepts aborde les deux limitations mentionnées ci-dessus. Un concept peut être n’importe quelle abstraction, comme une couleur, un objet, ou même une idée. Étant donné n’importe quel concept défini par l’utilisateur, bien qu’un réseau neuronal puisse ne pas être explicitement entraîné avec le concept donné, l’approche basée sur les concepts détecte ce concept intégré dans l’espace latent appris par le réseau. En d’autres termes, l’approche basée sur les concepts peut générer des explications qui ne sont pas limitées par l’espace de caractéristiques d’un réseau neuronal.\nDans ce chapitre, nous nous concentrerons principalement sur l’article “Testing with Concept Activation Vectors (TCAV)” de Kim et al.1\n\n10.3.1 - Test avec Vecteurs d’Activation de Concept (TCAV : Testing with Concept Activation Vectors)\nTCAV est proposé pour générer des explications globales pour les réseaux neuronaux, mais, en théorie, cela devrait également fonctionner pour tout modèle où il est possible de prendre une dérivée directionnelle. Pour tout concept donné, TCAV mesure l’étendue de l’influence de ce concept sur la prédiction du modèle pour une certaine classe. Par exemple, TCAV peut répondre à des questions telles que comment le concept de “rayé” influence un modèle classifiant une image comme un “zèbre”. Puisque TCAV décrit la relation entre un concept et une classe, au lieu d’expliquer une seule prédiction, il fournit une interprétation globale utile pour le comportement général d’un modèle.\n\n10.3.1.1 - Vecteur d’Activation de Concept (CAV : Concept Activation Vector)\nUn CAV est simplement la représentation numérique qui généralise un concept dans l’espace d’activation d’une couche d’un réseau neuronal. Un CAV, noté \\(v_l^C\\), dépend d’un concept \\(C\\) et d’une couche d’un réseau neuronal \\(l\\), où \\(l\\) est également appelée un goulot d’étranglement du modèle. Pour calculer le CAV d’un concept \\(C\\), d’abord, nous devons préparer deux ensembles de données : un ensemble de données concept qui représente \\(C\\) et un ensemble de données aléatoires qui consiste en des données arbitraires. Par exemple, pour définir le concept de “rayé”, nous pouvons collecter des images d’objets rayés comme l’ensemble de données concept, tandis que l’ensemble de données aléatoires est un groupe d’images aléatoires sans rayures. Ensuite, nous ciblons une couche cachée \\(l\\) et entraînons un classificateur binaire qui sépare les activations générées par l’ensemble concept de celles générées par l’ensemble aléatoire. Le vecteur des coefficients de ce classificateur binaire entraîné est alors le CAV \\(v_l^C\\). En pratique, nous pouvons utiliser un modèle SVM ou de régression logistique comme classificateur binaire. Enfin, étant donné une entrée d’image \\(x\\), nous pouvons mesurer sa “sensibilité conceptuelle” en calculant la dérivée directionnelle de la prédiction dans la direction du CAV unitaire :\n\\[S_{C,k,l}(x)=\\nabla h_{l,k}(\\hat{f}_l(x))\\cdot v_l^C\\]\noù \\(\\hat{f}_l\\) mappe l’entrée \\(x\\) sur le vecteur d’activation de la couche \\(l\\) et \\(h_{l,k}\\) mappe le vecteur d’activation sur la sortie logit de la classe \\(k\\).\nMathématiquement, le signe de \\(S_{C,k,l}(x)\\) dépend uniquement de l’angle entre le gradient de \\(h_{l,k}(\\hat{f}_l(x))\\) et \\(v_l^C\\). Si l’angle est supérieur à \\(90\\) degrés, \\(S_{C,k,l}(x)\\) sera positif, et si l’angle est inférieur à 90 degrés, \\(S_{C,k,l}(x)\\) sera négatif. Étant donné que le gradient \\(\\nabla h_{l,k}\\) pointe vers la direction qui maximise la sortie le plus rapidement, la sensibilité conceptuelle \\(S_{C,k,l}\\), intuitivement, indique si \\(v_l^C\\) pointe vers la direction similaire qui maximise \\(h_{l,k}\\). Ainsi, \\(S_{C,k,l}(x)&gt;0\\) peut être interprété comme le concept \\(C\\) encourageant le modèle à classer \\(x\\) dans la classe \\(k\\).\n\n\nTest avec des CAVs (TCAV : Testing with CAVs)\nDans le dernier paragraphe, nous avons appris comment calculer la sensibilité conceptuelle d’un seul point de données. Cependant, notre objectif est de produire une explication globale qui indique une sensibilité conceptuelle globale d’une classe entière. Une approche très simple réalisée par TCAV consiste à calculer le ratio d’entrées avec des sensibilités conceptuelles positives par rapport au nombre d’entrées pour une classe :\n\\[TCAV_{Q,C,k,l}=\\frac{|{x\\in X_k:S_{C,k,l}(x)&gt;0}|}{|X_k|}\\]\nRevenant à notre exemple, nous sommes intéressés par la manière dont le concept de “rayé” influence le modèle lors de la classification d’images comme “zèbre”. Nous collectons des données étiquetées comme “zèbre” et calculons la sensibilité conceptuelle pour chaque image d’entrée. Puis, le score TCAV du concept “rayé” avec la classe prédite “zèbre” est le nombre d’images “zèbre” ayant des sensibilités conceptuelles positives divisé par le nombre total d’images “zèbre”. Autrement dit, un \\(TCAV\\) avec \\(C=\\)“rayé” et \\(k=\\)“zèbre” égal à \\(0,8\\) indique que \\(80%\\) des prédictions pour la classe “zèbre” sont positivement influencées par le concept de “rayé”.\nCela semble génial, mais comment savons-nous que notre score TCAV est significatif ? Après tout, un CAV est formé par des ensembles de données de concepts sélectionnés par l’utilisateur et de données aléatoires. Si les ensembles de données utilisés pour former le CAV sont mauvais, l’explication peut être trompeuse et inutile. Ainsi, nous effectuons un simple test de signification statistique pour aider TCAV à devenir plus fiable. C’est-à-dire, au lieu de former seulement un CAV, nous formons plusieurs CAVs en utilisant différents ensembles de données aléatoires tout en gardant l’ensemble de données concept le même. Un concept significatif devrait générer des CAVs avec des scores TCAV cohérents. La procédure de test plus détaillée est montrée ci-dessous :\n\n\n\n\n\n\nNote du traducteur\n\n\n\nPossible problème de traduction du sens dans le paragraphe ci-sessus : After all, a CAV is trained by user-selected concept and random datasets.\n\n\n\nCollecter \\(N\\) ensembles de données aléatoires, où il est recommandé que \\(N\\) soit au moins \\(10\\).\nFixer l’ensemble de données concept et calculer le score TCAV en utilisant chacun des \\(N\\) ensembles de données aléatoires.\nAppliquer un test \\(t\\) à deux queues aux \\(N\\) scores TCAV contre d’autres \\(N\\) scores TCAV générés par un CAV aléatoire. Un CAV aléatoire peut être obtenu en choisissant un ensemble de données aléatoire comme ensemble de données concept.\n\n\n\n\n\n\n\nNote du traducteur\n\n\n\nPossible problème de traduction du sens dans le paragraphe ci-sessus : a two-sided t-test to\n\n\nIl est également suggéré d’appliquer une méthode de correction de tests multiples à cette étape si vous avez plusieurs hypothèses. L’article original utilise la correction de Bonferroni, et ici le nombre d’hypothèses est égal au nombre de concepts que vous testez.\n\n\n\n10.3.2 - Exemple\nVoyons un exemple disponible sur le dépôt GitHub de TCAV. En suivant l’exemple de la classe “zèbre” que nous avons utilisé précédemment, on constate le résultat des scores TCAV des concepts “rayé”, “zigzag” et “à pois”. Le classificateur d’images que nous utilisons est InceptionV32, un réseau neuronal convolutif formé en utilisant les données ImageNet. Chaque ensemble de données concept ou aléatoire contient \\(50\\) images, et nous utilisons \\(10\\) ensembles de données aléatoires pour le test de signification statistique avec un niveau de signification de \\(0,05\\). Nous n’utilisons pas la correction de Bonferroni, car nous n’avons que quelques ensembles de données aléatoires, mais il est recommandé d’ajouter la correction en pratique pour éviter les fausses découvertes.\n\n\n\nL’exemple de mesure des scores TCAV de trois concepts pour le modèle prédisant “zèbre”. Le goulot d’étranglement ciblé est une couche appelée “mixed4c”. Un signe étoile au-dessus de “à pois” indique que “à pois” n’a pas passé le test de signification statistique, c’est-à-dire ayant une valeur p supérieure à 0,05. Tant “rayé” que “zigzag” ont passé le test, et les deux concepts sont utiles pour le modèle pour identifier les images de “zèbre” selon TCAV. Figure originalement du GitHub de TCAV.\n\n\nEn pratique, vous pouvez vouloir utiliser plus de 50 images dans chaque ensemble de données pour former de meilleurs VACs. Vous pouvez également vouloir utiliser plus de 10 ensembles de données aléatoires pour effectuer de meilleurs tests de signification statistique. Vous pouvez également appliquer TCAV à plusieurs goulots d’étranglement pour avoir une observation plus approfondie.\n\n\n10.3.3 - Avantages\nPuisque les utilisateurs sont uniquement tenus de collecter des données pour former les concepts qui les intéressent, TCAV ne nécessite pas qu’ils aient une expertise en apprentissage automatique. Cela permet à TCAV d’être extrêmement utile pour les experts de domaine afin d’évaluer leurs modèles de réseau neuronal complexes.\nUne autre caractéristique unique de TCAV est sa capacité de personnalisation activée par les explications de TCAV au-delà de l’attribution de caractéristiques. Les utilisateurs peuvent étudier n’importe quel concept tant que le concept peut être défini par son ensemble de données concept. En d’autres termes, un utilisateur peut contrôler l’équilibre entre la complexité et l’interprétabilité des explications en fonction de ses besoins : si un expert de domaine comprend bien le problème et le concept, il peut façonner l’ensemble de données concept en utilisant des données plus compliquées pour générer une explication plus détaillée.\nEnfin, TCAV génère des explications globales qui relient les concepts à n’importe quelle classe. Une explication globale vous donne une idée de savoir si votre modèle global se comporte correctement ou non, ce qui ne peut généralement pas être fait par des explications locales. Et ainsi, TCAV peut être utilisé pour identifier les “défauts” ou les “points aveugles” potentiels survenus pendant la formation du modèle : peut-être que votre modèle a appris à pondérer un concept de manière inappropriée. Si un utilisateur peut identifier ces concepts mal appris, il peut utiliser cette connaissance pour améliorer son modèle. Disons qu’il y a un classificateur qui prédit “zèbre” avec une grande précision. Cependant, TCAV montre que le classificateur est plus sensible au concept de “à pois” plutôt que de “rayé”. Cela pourrait indiquer que le classificateur est accidentellement formé par un ensemble de données déséquilibré, vous permettant d’améliorer le modèle en ajoutant soit plus d’images de “zèbres rayés” soit moins d’images de “zèbres à pois” à l’ensemble de données d’entraînement.\n\n\n10.3.4 - Inconvénients\nTCAV pourrait mal fonctionner sur des réseaux neuronaux moins profonds. Comme de nombreux articles l’ont suggéré3, les concepts dans les couches plus profondes sont plus séparables. Si un réseau est trop peu profond, ses couches peuvent ne pas être capables de séparer clairement les concepts de sorte que TCAV n’est pas applicable.\nPuisque TCAV nécessite des annotations supplémentaires pour les ensembles de données de concept, cela peut être très coûteux pour des tâches qui n’ont pas de données étiquetées prêtes à l’emploi. Une alternative possible à TCAV lorsque l’annotation est coûteuse est d’utiliser ACE, dont nous parlerons brièvement dans la section suivante.\nBien que TCAV soit salué pour sa capacité de personnalisation, il est difficile à appliquer à des concepts trop abstraits ou généraux. Cela est principalement dû au fait que TCAV décrit un concept par son ensemble de données de concept correspondant. Plus un concept est abstrait ou général, comme “le bonheur”, plus le volume de données nécessaires est important pour former un VAC relatif à ce concept.\nBien que TCAV gagne en popularité dans son application aux données de type image, il a des applications relativement limitées pour les données textuelles et les données tabulaires.\n\n\n\n\n\n\nNote du traducteur\n\n\n\nSource de cette affirmation ?\n\n\n\n\n10.3.5 - Autres approches basées sur les concepts\nL’approche basée sur les concepts a gagné en popularité ces derniers temps, et il existe de nombreuses nouvelles méthodes inspirées de l’utilisation de concepts. Nous aimerions ici mentionner brièvement ces méthodes, et nous vous recommandons de lire les travaux originaux si vous êtes intéressés.\nL’Explication Automatisée Basée sur les Concepts (ACE : Automated Concept-based Explanation)4 peut être vue comme la version automatisée de TCAV. ACE passe en revue un ensemble d’images d’une classe et génère automatiquement des concepts basés sur le regroupement de segments d’image.\nLes modèles à goulot d’étranglement conceptuel (CBM : Concept bottleneck models)5 sont des réseaux neuronaux intrinsèquement interprétables. Un CBM est similaire à un modèle encodeur-décodeur, où la première moitié du CBM mappe les entrées aux concepts, et la seconde moitié utilise les concepts mappés pour prédire les sorties du modèle. Chaque activation de neurone de la couche de goulot d’étranglement représente alors l’importance d’un concept. De plus, les utilisateurs peuvent manipuler les activations des neurones du goulot d’étranglement pour générer des explications contrefactuelles du modèle.\nLe blanchiment de concept (CW : Concept whitening)6 est une autre approche pour générer des classificateurs d’images intrinsèquement interprétables. Pour utiliser CW, on substitue une couche de normalisation, comme une couche de normalisation par lots, par une couche CW. Ainsi, CW est très utile lorsque les utilisateurs veulent transformer leurs classificateurs d’images pré-entraînés pour être intrinsèquement interprétables, tout en maintenant la performance du modèle. CW est fortement inspiré par la transformation de blanchiment, et nous vous recommandons vivement d’étudier les mathématiques derrière la transformation de blanchiment si vous êtes intéressés à en apprendre davantage sur CW.\n\n\n10.3.6 - Logiciels\nLa librairie officielle de TCAV respose sur Tensorflow, mais il existe d’autres versions disponibles en ligne. Les carnets Jupyter, faciles d’emploi, sont également accessibles sur ce dépôt GitHUb.",
    "crumbs": [
      "10 - Interprétation d'un réseau de neurone",
      "10.3 - Détecter les concepts"
    ]
  },
  {
    "objectID": "10-neuralnet/10.3-concepts.html#footnotes",
    "href": "10-neuralnet/10.3-concepts.html#footnotes",
    "title": "Apprentissage automatique interprétable",
    "section": "Notes de bas de page",
    "text": "Notes de bas de page\n\n\nKim, Been, Martin Wattenberg, Justin Gilmer, Carrie Cai, James Wexler, and Fernanda Viegas. “Interpretability beyond feature attribution: Quantitative testing with concept activation vectors (tcav).” In International conference on machine learning, pp. 2668-2677. PMLR (2018).↩︎\nSzegedy, Christian, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. “Rethinking the inception architecture for computer vision.” In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 2818-2826 (2016).↩︎\nAlain, Guillaume, and Yoshua Bengio. “Understanding intermediate layers using linear classifier probes.” arXiv preprint arXiv:1610.01644 (2016).↩︎\nGhorbani, Amirata, James Wexler, James Zou and Been Kim. “Towards automatic concept-based explanations.” Advances in Neural Information Processing Systems 32 (2019).↩︎\nKoh, Pang Wei, Thao Nguyen, Yew Siang Tang, Stephen Mussmann, Emma Pierson, Been Kim, and Percy Liang. “Concept bottleneck models.” In International Conference on Machine Learning, pp. 5338-5348. PMLR (2020).↩︎\nChen, Zhi, Yijie Bei, and Cynthia Rudin. “Concept whitening for interpretable image recognition.” Nature Machine Intelligence 2, no. 12 (2020): 772-782.↩︎",
    "crumbs": [
      "10 - Interprétation d'un réseau de neurone",
      "10.3 - Détecter les concepts"
    ]
  },
  {
    "objectID": "10-neuralnet/10.5-influential-instances.html",
    "href": "10-neuralnet/10.5-influential-instances.html",
    "title": "Apprentissage automatique interprétable",
    "section": "",
    "text": "Avertissement\n\n\n\nEn cours de traduction.",
    "crumbs": [
      "10 - Interprétation d'un réseau de neurone",
      "10.5 - Instances Influentes"
    ]
  },
  {
    "objectID": "10-neuralnet/10.5-influential-instances.html#instances-influentes",
    "href": "10-neuralnet/10.5-influential-instances.html#instances-influentes",
    "title": "Apprentissage automatique interprétable",
    "section": "10.5 - Instances Influentes",
    "text": "10.5 - Instances Influentes\nLes modèles d’apprentissage automatique sont finalement le produit des données d’entraînement et la suppression d’une des instances d’entraînement peut affecter le modèle résultant. Nous qualifierons une instance d’entraînement d’“influente” lorsque sa suppression des données d’entraînement change considérablement les paramètres ou les prédictions du modèle. En identifiant les instances d’entraînement influentes, nous pouvons “déboguer” les modèles d’apprentissage automatique et mieux expliquer leurs comportements et prédictions.\nCe chapitre vous montre deux approches pour identifier les instances influentes, à savoir les diagnostics de suppression et les fonctions d’influence. Les deux approches sont basées sur la statistique robuste, qui fournit des méthodes statistiques moins affectées par les valeurs aberrantes ou les violations des hypothèses du modèle. La statistique robuste fournit également des méthodes pour mesurer la robustesse des estimations à partir des données (comme une estimation moyenne ou les poids d’un modèle de prédiction).\nImaginez que vous voulez estimer le revenu moyen des habitants de votre ville et demandez à dix personnes choisies au hasard dans la rue combien elles gagnent. Outre le fait que votre échantillon est probablement vraiment mauvais, à quel point votre estimation du revenu moyen peut-elle être influencée par une seule personne ? Pour répondre à cette question, vous pouvez recalculer la valeur moyenne en omettant des réponses individuelles ou dériver mathématiquement via des “fonctions d’influence” comment la valeur moyenne peut être influencée. Avec l’approche de suppression, nous recalculons la valeur moyenne dix fois, en omettant une des déclarations de revenus à chaque fois, et mesurons à quel point l’estimation moyenne change. Un grand changement signifie qu’une instance était très influente. La deuxième approche surpondère l’une des personnes par un poids infinitésimalement petit, ce qui correspond au calcul de la première dérivée d’une statistique ou d’un modèle. Cette approche est également connue sous le nom d’“approche infinitésimale” ou “fonction d’influence”. La réponse est, en passant, que votre estimation moyenne peut être très fortement influencée par une seule réponse, puisque la moyenne évolue linéairement avec les valeurs individuelles. Un choix plus robuste est la médiane (la valeur pour laquelle la moitié des gens gagnent plus et l’autre moitié moins), car même si la personne avec le revenu le plus élevé de votre échantillon gagnait dix fois plus, la médiane résultante ne changerait pas.\nLes diagnostics de suppression et les fonctions d’influence peuvent également être appliqués aux paramètres ou prédictions des modèles d’apprentissage automatique pour mieux comprendre leur comportement ou pour expliquer des prédictions individuelles. Avant de regarder ces deux approches pour trouver des instances influentes, nous examinerons la différence entre une valeur aberrante et une instance influente.\nValeur extrême\nUne valeur aberrante est une instance qui est éloignée des autres instances dans l’ensemble de données. “Éloignée” signifie que la distance, par exemple la distance euclidienne, à toutes les autres instances est très grande. Dans un ensemble de données de nouveau-nés, un nouveau-né pesant 6 kg serait considéré comme une valeur aberrante. Dans un ensemble de données de comptes bancaires comprenant principalement des comptes courants, un compte de prêt dédié (solde négatif important, peu de transactions) serait considéré comme une valeur aberrante. La figure suivante montre une valeur aberrante pour une distribution unidimensionnelle.\n\n\n\nFeature x follows a Gaussian distribution with an outlier at x=8.\n\n\nLes valeurs extrêmes peuvent représenter des points intéressants (p. ex. critiques). Quand une valeur extrême influence le modèle, elle peut être une instance influente.\nInstance influente\nUne instance influente est une instance de données dont la suppression a un fort effet sur le modèle entraîné. Plus les paramètres ou les prédictions du modèle changent lorsque le modèle est réentraîné avec une instance particulière retirée des données d’entraînement, plus cette instance est influente. Le fait qu’une instance soit influente pour un modèle entraîné dépend également de sa valeur pour la cible y. La figure suivante montre une instance influente pour un modèle de régression linéaire.\n\n\n\nA linear model with one feature. Trained once on the full data and once without the influential instance. Removing the influential instance changes the fitted slope (weight/coefficient) drastically.\n\n\nPourquoi les instances influentes aident-elles à comprendre le modèle ?\nL’idée clé derrière les instances influentes pour l’interprétabilité est de remonter aux origines des paramètres et des prédictions du modèle : les données d’entraînement. Un apprenant, c’est-à-dire l’algorithme qui génère le modèle d’apprentissage automatique, est une fonction qui prend des données d’entraînement constituées de caractéristiques X et de la cible y et génère un modèle d’apprentissage automatique. Par exemple, l’apprenant d’un arbre de décision est un algorithme qui sélectionne les caractéristiques de division et les valeurs auxquelles diviser. Un apprenant pour un réseau neuronal utilise la rétropropagation pour trouver les meilleurs poids.\n\n\n\nA learner learns a model from training data (features plus target). The model makes predictions for new data.\n\n\nNous nous demandons comment les paramètres du modèle ou les prédictions changeraient si nous retirions des instances des données d’entraînement dans le processus de formation. Cela contraste avec d’autres approches d’interprétabilité qui analysent comment la prédiction change lorsque nous manipulons les caractéristiques des instances à prédire, telles que les graphiques de dépendance partielle ou l’importance des caractéristiques. Avec les instances influentes, nous ne traitons pas le modèle comme fixe, mais comme une fonction des données d’entraînement. Les instances influentes nous aident à répondre à des questions sur le comportement global du modèle et sur des prédictions individuelles. Quelles étaient les instances les plus influentes pour les paramètres du modèle ou les prédictions dans l’ensemble ? Quelles étaient les instances les plus influentes pour une prédiction particulière ? Les instances influentes nous indiquent pour quelles instances le modèle pourrait avoir des problèmes, quelles instances d’entraînement devraient être vérifiées pour des erreurs et donnent une impression de la robustesse du modèle. Nous pourrions ne pas faire confiance à un modèle si une seule instance a une forte influence sur les prédictions et les paramètres du modèle. Au moins, cela nous inciterait à enquêter davantage.\nComment pouvons-nous trouver des instances influentes ? Nous avons deux façons de mesurer l’influence : Notre première option est de supprimer l’instance des données d’entraînement, de réentraîner le modèle sur l’ensemble de données d’entraînement réduit et d’observer la différence dans les paramètres du modèle ou les prédictions (soit individuellement, soit sur l’ensemble complet des données). La deuxième option est de surpondérer une instance de données en approximant les changements de paramètres basés sur les gradients des paramètres du modèle. L’approche de suppression est plus facile à comprendre et motive l’approche de surpondération, donc nous commençons par la première.\n\n10.5.1 - Diagnostics par suppression\nLes statisticiens ont déjà beaucoup recherché dans le domaine des instances influentes, en particulier pour les modèles de régression linéaire (généralisée). Lorsque vous recherchez “observations influentes”, les premiers résultats de recherche concernent des mesures telles que DFBETA et la distance de Cook. DFBETA mesure l’effet de la suppression d’une instance sur les paramètres du modèle. La distance de Cook (Cook, 19771) mesure l’effet de la suppression d’une instance sur les prédictions du modèle. Pour les deux mesures, nous devons réentraîner le modèle à plusieurs reprises, en omettant des instances individuelles à chaque fois. Les paramètres ou prédictions du modèle avec toutes les instances sont comparés avec les paramètres ou prédictions du modèle avec l’une des instances supprimée des données d’entraînement.\nDFBETA est défini comme suit :\n\\[DFBETA_{i}=\\beta-\\beta^{(-i)}\\]\noù \\(\\beta\\) est le vecteur de poids lorsque le modèle est entraîné sur toutes les instances de données, et \\(\\beta^{(-i)}\\) le vecteur de poids lorsque le modèle est entraîné sans l’instance \\(i\\). Assez intuitif je dirais. DFBETA ne fonctionne que pour les modèles avec des paramètres de poids, tels que la régression logistique ou les réseaux neuronaux, mais pas pour des modèles tels que les arbres de décision, les ensembles d’arbres, certaines machines à vecteurs de support, etc.\nLa distance de Cook a été inventée pour les modèles de régression linéaire et des approximations pour les modèles de régression linéaire généralisée existent. La distance de Cook pour une instance d’entraînement est définie comme la somme (mise à l’échelle) des différences au carré dans le résultat prédit lorsque la \\(i^{ième}\\)instance est retirée de l’entraînement du modèle.\n\\[D_i=\\frac{\\sum_{j=1}^n(\\hat{y}_j-\\hat{y}_{j}^{(-i)})^2}{p\\cdot{}MSE}\\]\noù le numérateur est la différence au carré entre la prédiction du modèle avec et sans la \\(i^{ième}\\) instance, sommée sur l’ensemble de données. Le dénominateur est le nombre de caractéristiques \\(p\\) fois l’erreur quadratique moyenne (MSE : Mean Square Error). Le dénominateur est le même pour toutes les instances, peu importe quelle instance \\(i\\) est retirée. La distance de Cook nous indique à quel point le résultat prédit d’un modèle linéaire change lorsque nous retirons la \\(i^{ième}\\) instance de l’entraînement.\nPouvons-nous utiliser la distance de Cook et DFBETA pour tout modèle d’apprentissage automatique ? DFBETA nécessite des paramètres de modèle, donc cette mesure fonctionne uniquement pour les modèles paramétrés. La distance de Cook ne nécessite aucun paramètre de modèle. Fait intéressant, la distance de Cook n’est généralement pas vue en dehors du contexte des modèles linéaires et des modèles linéaires généralisés, mais l’idée de prendre la différence entre les prédictions du modèle avant et après la suppression d’une instance particulière est très générale. Un problème avec la définition de la distance de Cook est la MSE, qui n’est pas significative pour tous les types de modèles de prédiction (par exemple, la classification).\nLa mesure d’influence la plus simple pour l’effet sur les prédictions du modèle peut s’écrire comme suit :\n\\[\\text{Influence}^{(-i)}=\\frac{1}{n}\\sum_{j=1}^{n}\\left|\\hat{y}_j-\\hat{y}_{j}^{(-i)}\\right|\\]\nCette expression est essentiellement le numérateur de la distance de Cook, avec la différence que la différence absolue est ajoutée au lieu des différences au carré. C’était un choix que j’ai fait, car il a du sens pour les exemples plus tard. La forme générale des mesures de diagnostic de suppression consiste à choisir une mesure (comme le résultat prédit) et à calculer la différence de la mesure pour le modèle entraîné sur toutes les instances et lorsque l’instance est supprimée.\nNous pouvons facilement décomposer l’influence pour expliquer pour la prédiction de l’instance \\(j\\) quelle était l’influence de l’instance d’entraînement \\(i^{ième}\\) :\n\\[\\text{Influence}_{j}^{(-i)}=\\left|\\hat{y}_j-\\hat{y}_{j}^{(-i)}\\right|\\]\nCela fonctionnerait également pour la différence dans les paramètres du modèle ou la différence dans la perte. Dans l’exemple suivant, nous utiliserons ces mesures d’influence simples.\nExemple de diagnostic par suppression\nDans l’exemple suivant, nous entraînons une machine à vecteurs de support pour prédire le cancer du col de l’utérus en fonction des facteurs de risque et mesurons quelles instances d’entraînement ont été les plus influentes dans l’ensemble et pour une prédiction particulière. Étant donné que la prédiction du cancer est un problème de classification, nous mesurons l’influence comme la différence de probabilité prédite pour le cancer. Une instance est influente si la probabilité prédite augmente ou diminue fortement en moyenne dans l’ensemble de données lorsque l’instance est retirée de l’entraînement du modèle. La mesure de l’influence pour toutes les r nrow(cervical) instances d’entraînement nécessite d’entraîner le modèle une fois sur toutes les données et de le réentraîner r nrow(cervical) fois (= taille des données d’entraînement) avec une des instances retirée à chaque fois.\nL’instance la plus influente a une mesure d’influence d’environ r sprintf(\"%.2f\", abs(df[1,\"influence\"])). Une influence de r sprintf('%.2f', abs(df[1,\"influence\"])) signifie que si nous retirons l’instance r df$id[1], la probabilité prédite change en moyenne de r sprintf('%.0f', 100 * df[1,\"influence\"]) points de pourcentage. C’est assez considérable étant donné que la probabilité prédite moyenne pour le cancer est de r sprintf('%.1f', 100 *mean(predicted.orig))%. La valeur moyenne des mesures d’influence sur toutes les suppressions possibles est de r sprintf('%.1f', 100 * mean(abs(df$influence))) points de pourcentage. Maintenant, nous savons quelles sont les instances de données les plus influentes pour le modèle. Cela est déjà utile à savoir pour déboguer les données. Y a-t-il une instance problématique ? Y a-t-il des erreurs de mesure ? Les instances influentes sont les premières à vérifier pour les erreurs, car chaque erreur en elles influence fortement les prédictions du modèle.\nEn dehors du débogage du modèle, pouvons-nous apprendre quelque chose pour mieux comprendre le modèle ? Imprimer simplement les dix instances les plus influentes n’est pas très utile, car il s’agit juste d’une table d’instances avec de nombreuses caractéristiques. Toutes les méthodes qui retournent des instances en sortie n’ont de sens que si nous avons un bon moyen de les représenter. Mais nous pouvons mieux comprendre quel type d’instances est influent lorsque nous nous demandons : Qu’est-ce qui distingue une instance influente d’une instance non influente ? Nous pouvons transformer cette question en un problème de régression et modéliser l’influence d’une instance en fonction de ses valeurs de caractéristiques. Nous sommes libres de choisir n’importe quel modèle du chapitre sur les Modèles d’Apprentissage Automatique Interprétables. Pour cet exemple, j’ai choisi un arbre de décision (figure suivante) qui montre que les données des femmes de 35 ans et plus étaient les plus influentes pour la machine à vecteurs de support. Parmi toutes les femmes de l’ensemble de données r sum(cervical$Age &gt;= 35) sur r nrow(cervical) étaient âgées de plus de 35 ans. Dans le chapitre sur les Graphiques de Dépendance Partielle, nous avons vu qu’après 40 ans, il y a une augmentation marquée de la probabilité prédite de cancer et l’Importance des Caractéristiques a également détecté l’âge comme l’une des caractéristiques les plus importantes. L’analyse de l’influence nous dit que le modèle devient de plus en plus instable lors de la prédiction du cancer pour les âges plus élevés. Cela en soi est une information précieuse. Cela signifie que les erreurs dans ces instances peuvent avoir un fort effet sur le modèle.\n\n\n\nA decision tree that models the relationship between the influence of the instances and their features. The maximum depth of the tree is set to 2.\n\n\nCette première analyse d’influence a révélé l’instance la plus influente dans l’ensemble. Maintenant, nous sélectionnons l’une des instances, à savoir l’instance r i, pour laquelle nous voulons expliquer la prédiction en trouvant les instances de données d’entraînement les plus influentes. C’est comme une question contrefactuelle : Comment la prédiction pour l’instance r i changerait-elle si nous omettions l’instance i du processus d’entraînement ? Nous répétons cette suppression pour toutes les instances. Ensuite, nous sélectionnons les instances d’entraînement qui entraînent le plus grand changement dans la prédiction de l’instance r i lorsqu’elles sont omises de l’entraînement et les utilisons pour expliquer la prédiction du modèle pour cette instance. J’ai choisi d’expliquer la prédiction pour l’instance r i car c’est l’instance avec la probabilité prédite la plus élevée de cancer (r sprintf('%.2f', 100 * predicted.orig[i])%), que je pensais être un cas intéressant à analyser plus en profondeur. Nous pourrions retourner, disons, les 10 instances les plus influentes pour prédire l’instance r i imprimées sous forme de tableau. Pas très utile, car nous ne pourrions pas voir grand-chose. Encore une fois, il est plus logique de découvrir ce qui distingue les instances influentes des instances non influentes en analysant leurs caractéristiques. Nous utilisons un arbre de décision formé pour prédire l’influence étant donné les caractéristiques, mais en réalité, nous le détournons seulement pour trouver une structure et non pour prédire réellement quelque chose. L’arbre de décision suivant montre quel type d’instances d’entraînement était le plus influent pour prédire la r i instance.\n\n\n\nDecision tree that explains which instances were most influential for predicting the 7-th instance. Data from women who smoked for 18.5 years or longer had a large influence on the prediction of the 7-th instance, with an average change in absolute prediction by 11.7 percentage points of cancer probability.\n\n\nLes instances de données de femmes qui ont fumé ou fument depuis 18,5 ans ou plus ont une forte influence sur la prédiction de l’instance r i. La femme derrière l’instance r i a fumé pendant r cervical$Smokes..years.[i] ans. Dans les données, r sum(cervical$Smokes..years &gt;= 18.5) femmes (r sprintf('%.2f', 100 * mean(cervical$Smokes..years &gt;= 18.5))%) ont fumé pendant 18,5 ans ou plus. Toute erreur commise dans la collecte du nombre d’années de tabagisme de l’une de ces femmes aura un impact énorme sur le résultat prédit pour l’instance r i.\nLe changement le plus extrême dans la prédiction se produit lorsque nous supprimons l’instance numéro r worst.case.index. La patiente aurait fumé pendant r cervical$Smokes..years.[worst.case.index] ans, ce qui correspond aux résultats de l’arbre de décision. La probabilité prédite pour l’instance r i passe de r sprintf('%.2f', 100 * predicted.orig[i])% à r sprintf('%.2f', 100 * (predicted.orig[i]  - cervical.200$influence[worst.case.index]))% si nous retirons l’instance r worst.case.index !\nSi nous examinons de plus près les caractéristiques de l’instance la plus influente, nous pouvons voir un autre problème possible. Les données indiquent que la femme a 28 ans et fume depuis 22 ans. Soit c’est un cas vraiment extrême et elle a vraiment commencé à fumer à 6 ans, soit c’est une erreur de données. Je penche pour cette dernière hypothèse. C’est certainement une situation dans laquelle nous devons remettre en question l’exactitude des données.\nCes exemples ont montré à quel point il est utile d’identifier les instances influentes pour déboguer des modèles. Un problème avec l’approche proposée est que le modèle doit être réentraîné pour chaque instance d’entraînement. Le réentraînement complet peut être assez lent, car si vous avez des milliers d’instances d’entraînement, vous devrez réentraîner votre modèle des milliers de fois. En supposant que le modèle prend un jour pour s’entraîner et que vous avez 1000 instances d’entraînement, alors le calcul des instances influentes – sans parallélisation – prendra presque 3 ans. Personne n’a le temps pour cela. Dans le reste de ce chapitre, je vous montrerai une méthode qui ne nécessite pas de réentraîner le modèle.\n\n\n10.5.2 - Fonction d’influence\nVous : Je veux connaître l’influence qu’une instance d’entraînement a sur une prédiction particulière.\nRecherche : Vous pouvez supprimer l’instance d’entraînement, réentraîner le modèle et mesurer la différence dans la prédiction.\nVous : Génial ! Mais avez-vous une méthode pour moi qui fonctionne sans réentraînement ? Cela prend tellement de temps.\nRecherche : Avez-vous un modèle avec une fonction de perte qui est deux fois différentiable par rapport à ses paramètres ?\nVous : J’ai entraîné un réseau neuronal avec la perte logistique. Donc oui.\nRecherche : Alors vous pouvez approximer l’influence de l’instance sur les paramètres du modèle et sur la prédiction avec des fonctions d’influence. La fonction d’influence est une mesure de la force avec laquelle les paramètres du modèle ou les prédictions dépendent d’une instance d’entraînement. Au lieu de supprimer l’instance, la méthode surpondère l’instance dans la perte par un très petit pas. Cette méthode implique d’approximer la perte autour des paramètres actuels du modèle en utilisant le gradient et la matrice hessienne. La surpondération de la perte est similaire à la suppression de l’instance.\nVous : Super, c’est ce que je recherche !\nKoh et Liang (2017)2 ont suggéré d’utiliser les fonctions d’influence, une méthode de statistique robuste, pour mesurer comment une instance influence les paramètres du modèle ou les prédictions. Comme avec les diagnostics de suppression, les fonctions d’influence retracent les paramètres du modèle et les prédictions jusqu’à l’instance d’entraînement responsable. Cependant, au lieu de supprimer des instances d’entraînement, la méthode approxime à quel point le modèle change lorsque l’instance est surpondérée dans le risque empirique (somme de la perte sur les données d’entraînement).\nLa méthode des fonctions d’influence nécessite un accès au gradient de la perte par rapport aux paramètres du modèle, ce qui ne fonctionne que pour un sous-ensemble de modèles d’apprentissage automatique. La régression logistique, les réseaux neuronaux et les machines à vecteurs de support se qualifient, mais les méthodes basées sur des arbres comme les forêts aléatoires ne le font pas. Les fonctions d’influence aident à comprendre le comportement du modèle, à déboguer le modèle et à détecter des erreurs dans l’ensemble de données.\nLa section suivante explique l’intuition et les mathématiques derrière les fonctions d’influence.\nMathématiques derrière les fonctions d’influence\nL’idée clé derrière les fonctions d’influence est de surpondérer la perte d’une instance d’entraînement par un petit pas infinitésimal \\(\\epsilon\\), ce qui entraîne de nouveaux paramètres du modèle :\n\\[\\hat{\\theta}_{\\epsilon,z}=\\arg\\min_{\\theta{}\\in\\Theta}\\frac{1}{n}\\sum_{i=1}^n{}L(z_i,\\theta)+\\epsilon{}L(z,\\theta)\\]\noù \\(\\theta\\) est le vecteur de paramètres du modèle et \\(\\hat{\\theta}_{\\epsilon,z}\\) est le vecteur de paramètres après la surpondération de \\(z\\) par un très petit nombre \\(\\epsilon\\). \\(L\\) est la fonction de perte avec laquelle le modèle a été entraîné, \\(z_i\\) est les données d’entraînement et \\(z\\) est l’instance d’entraînement que nous voulons surpondérer pour simuler sa suppression. L’intuition derrière cette formule est : À quel point la perte changera-t-elle si nous surpondérons une instance particulière \\(z_i\\) des données d’entraînement d’un peu (\\(\\epsilon\\)) et sous-pondérons les autres instances de données en conséquence ? À quoi ressemblerait le vecteur de paramètres pour optimiser cette nouvelle perte combinée ? La fonction d’influence des paramètres, c’est-à-dire l’influence de la surpondération de l’instance d’entraînement \\(z\\) sur les paramètres, peut être calculée comme suit.\n\\[I_{\\text{up,params}}(z)=\\left.\\frac{d{}\\hat{\\theta}_{\\epsilon,z}}{d\\epsilon}\\right|_{\\epsilon=0}=-H_{\\hat{\\theta}}^{-1}\\nabla_{\\theta}L(z,\\hat{\\theta})\\]\nLa dernière expression \\(\\nabla_{\\theta}L(z,\\hat{\\theta})\\) est le gradient de la perte par rapport aux paramètres pour l’instance d’entraînement surpondérée. Le gradient est le taux de changement de la perte de l’instance d’entraînement. Il nous indique de combien la perte change lorsque nous changeons les paramètres du modèle \\(\\hat{\\theta}\\) d’un peu. Une entrée positive dans le vecteur de gradient signifie qu’une petite augmentation du paramètre du modèle correspondant augmente la perte, une entrée négative signifie que l’augmentation du paramètre réduit la perte. La première partie \\(H^{-1}_{\\hat{\\theta}}\\) est la matrice hessienne inverse (deuxième dérivée de la perte par rapport aux paramètres du modèle). La matrice hessienne est le taux de changement du gradient, ou exprimée en termes de perte, c’est le taux de changement du taux de changement de la perte. Elle peut être estimée en utilisant :\n\\[H_{\\theta}=\\frac{1}{n}\\sum_{i=1}^n\\nabla^2_{\\hat{\\theta}}L(z_i,\\hat{\\theta})\\]\nPlus informellement : La matrice hessienne enregistre la courbure de la perte à un certain point. La hessienne est une matrice et non juste un vecteur, car elle décrit la courbure de la perte et cette courbure dépend de la direction dans laquelle nous regardons. Le calcul réel de la matrice hessienne est chronophage si vous avez de nombreux paramètres. Koh et Liang ont suggéré quelques astuces pour la calculer efficacement, ce qui va au-delà du cadre de ce chapitre. Mettre à jour les paramètres du modèle, comme décrit par la formule ci-dessus, est équivalent à prendre une seule étape de Newton après avoir formé une expansion quadratique autour des paramètres estimés du modèle.\nQuelle intuition se cache derrière cette formule de fonction d’influence ? La formule vient de la formation d’une expansion quadratique autour des paramètres \\(\\hat{\\theta}\\). Cela signifie que nous ne savons pas réellement, ou il est trop complexe de calculer comment exactement la perte de l’instance z changera lorsqu’elle est supprimée/surpondérée. Nous approximons localement la fonction en utilisant des informations sur la pente (i.e gradient) et la courbure (i.e matrice hessienne) à la configuration actuelle des paramètres du modèle. Avec cette approximation de la perte, nous pouvons calculer à quoi ressembleraient approximativement les nouveaux paramètres si nous surpondérions l’instance z :\n\\[\\hat{\\theta}_{-z}\\approx\\hat{\\theta}-\\frac{1}{n}I_{\\text{up,params}}(z)\\]\nLe vecteur de paramètres approximatif est essentiellement le paramètre original moins le gradient de la perte de \\(z\\) (car nous voulons diminuer la perte) ajusté par la courbure (i.e multiplié par la matrice hessienne inverse) et ajusté par \\(1/n\\), car c’est le poids d’une seule instance d’entraînement.\nLa figure suivante montre comment fonctionne la surpondération. L’axe des \\(x\\) montre la valeur du paramètre \\(\\theta\\) et l’axe des \\(y\\) la valeur correspondante de la perte avec l’instance \\(z\\) surpondérée. Le paramètre du modèle est unidimensionnel à des fins de démonstration, mais en réalité, il est généralement de haute dimension. Nous ne nous déplaçons que de \\(1/n\\) dans la direction de l’amélioration de la perte pour l’instance \\(z\\). Nous ne savons pas comment la perte changerait réellement lorsque nous supprimons \\(z\\), mais avec la première et la deuxième dérivée de la perte, nous créons cette approximation quadratique autour de notre paramètre de modèle actuel et prétendons que c’est ainsi que la perte réelle se comporterait.\n\n\n\nUpdating the model parameter (x-axis) by forming a quadratic expansion of the loss around the current model parameter, and moving 1/n into the direction in which the loss with upweighted instance z (y-axis) improves most. This upweighting of instance z in the loss approximates the parameter changes if we delete z and train the model on the reduced data.\n\n\nNous n’avons pas réellement besoin de calculer les nouveaux paramètres, mais nous pouvons utiliser la fonction d’influence comme mesure de l’influence de \\(z\\) sur les paramètres.\nComment les prédictions changent-elles lorsque nous surpondérons l’instance d’entraînement \\(z\\) ? Nous pouvons soit calculer les nouveaux paramètres, puis faire des prédictions en utilisant le modèle nouvellement paramétré, soit nous pouvons également calculer directement l’influence de l’instance \\(z\\) sur les prédictions, car nous pouvons calculer l’influence en utilisant la règle de chaîne :\n\\[\\begin{align*}I_{up,loss}(z,z_{test})&=\\left.\\frac{d{}L(z_{test},\\hat{\\theta}_{\\epsilon,z})}{d\\epsilon}\\right|_{\\epsilon=0}\\\\&=\\left.\\nabla_{\\theta}L(z_{test},\\hat{\\theta})^T\\frac{d\\hat{\\theta}_{\\epsilon,z}}{d\\epsilon}\\right|_{\\epsilon=0}\\\\&=-\\nabla_{\\theta}L(z_{test},\\hat{\\theta})^T{}H^{-1}_{\\theta}\\nabla_{\\theta}L(z,\\hat{\\theta})\\end{align*}\\]\nLa première ligne de cette équation signifie que nous mesurons l’influence d’une instance d’entraînement sur une certaine prédiction \\(z_{test}\\) comme un changement de perte de l’instance de test lorsque nous surpondérons l’instance \\(z\\) et obtenons de nouveaux paramètres \\(\\hat{\\theta}_{\\epsilon,z}\\). Pour la deuxième ligne de l’équation, nous avons appliqué la règle de chaîne des dérivées et obtenons la dérivée de la perte de l’instance de test par rapport aux paramètres multipliée par l’influence de \\(z\\) sur les paramètres. Dans la troisième ligne, nous remplaçons l’expression par la fonction d’influence pour les paramètres. Le premier terme de la troisième ligne \\(\\nabla_{\\theta}L(z_{test},\\hat{\\theta})^T{}\\) est le gradient de l’instance de test par rapport aux paramètres du modèle.\nAvoir une formule est formidable et la manière scientifique et précise de montrer les choses. Mais je pense qu’il est très important de comprendre l’intuition derrière la formule. La formule pour \\(I_{\\text{up,loss}}\\) indique que la fonction d’influence de l’instance d’entraînement \\(z\\) sur la prédiction d’une instance \\(z_{test}\\) est “à quel point l’instance réagit à un changement des paramètres du modèle” multipliée par “à quel point les paramètres changent lorsque nous surpondérons l’instance \\(z\\)”. Une autre façon de lire la formule : L’influence est proportionnelle à la taille des gradients pour la perte d’entraînement et la perte de test. Plus le gradient de la perte d’entraînement est élevé, plus son influence sur les paramètres et plus l’influence sur la prédiction de test est élevée. Plus le gradient de la prédiction de test est élevé, plus l’instance de test est influençable. L’ensemble du construct peut également être vu comme une mesure de la similarité (telle qu’apprise par le modèle) entre l’instance d’entraînement et l’instance de test.\nC’est tout pour la théorie et l’intuition. La section suivante explique comment les fonctions d’influence peuvent être appliquées.\nApplication des Fonctions d’Influence\nLes fonctions d’influence ont de nombreuses applications, dont certaines ont déjà été présentées dans ce chapitre.\nComprendre le comportement du modèle\nDifférents modèles d’apprentissage automatique ont différentes manières de faire des prédictions. Même si deux modèles ont la même performance, la manière dont ils font des prédictions à partir des caractéristiques peut être très différente et donc échouer dans différents scénarios. Comprendre les faiblesses particulières d’un modèle en identifiant des instances influentes aide à former un “modèle mental” du comportement du modèle d’apprentissage automatique dans votre esprit.\nGérer les inadéquations de domaine / Déboguer les erreurs du modèle\nLa gestion de l’inadéquation de domaine est étroitement liée à une meilleure compréhension du comportement du modèle. L’inadéquation de domaine signifie que la distribution des données d’entraînement et de test est différente, ce qui peut amener le modèle à mal fonctionner sur les données de test. Les fonctions d’influence peuvent identifier des instances d’entraînement qui ont causé l’erreur. Supposons que vous avez formé un modèle de prédiction pour le résultat de patients ayant subi une chirurgie. Tous ces patients viennent du même hôpital. Maintenant, vous utilisez le modèle dans un autre hôpital et constatez qu’il ne fonctionne pas bien pour de nombreux patients. Bien sûr, vous supposez que les deux hôpitaux ont des patients différents, et si vous regardez leurs données, vous pouvez voir qu’ils diffèrent dans de nombreuses caractéristiques. Mais quelles sont les caractéristiques ou les instances qui ont “cassé” le modèle ? Ici aussi, les instances influentes sont un bon moyen de répondre à cette question. Vous prenez l’un des nouveaux patients, pour lequel le modèle a fait une fausse prédiction, trouvez et analysez les instances les plus influentes. Par exemple, cela pourrait montrer que le deuxième hôpital a en moyenne des patients plus âgés et les instances les plus influentes des données d’entraînement sont les quelques patients plus âgés du premier hôpital et le modèle manquait simplement de données pour apprendre à bien prédire ce sous-groupe. La conclusion serait que le modèle doit être entraîné sur plus de patients plus âgés pour bien fonctionner dans le deuxième hôpital.\nCorriger les données d’entraînement\nSi vous avez une limite sur le nombre d’instances d’entraînement que vous pouvez vérifier pour leur exactitude, comment faire une sélection efficace ? La meilleure façon est de sélectionner les instances les plus influentes, car – par définition – elles ont le plus d’influence sur le modèle. Même si vous aviez une instance avec des valeurs manifestement incorrectes, si l’instance n’est pas influente et que vous avez besoin des données uniquement pour le modèle de prédiction, il est préférable de vérifier les instances influentes. Par exemple, vous entraînez un modèle pour prédire si un patient doit rester à l’hôpital ou être libéré prématurément. Vous voulez vraiment vous assurer que le modèle est robuste et fait des prédictions correctes, car une libération erronée d’un patient peut avoir de mauvaises conséquences. Les dossiers des patients peuvent être très désordonnés, donc vous n’avez pas une confiance parfaite dans la qualité des données. Mais vérifier les informations des patients et les corriger peut prendre beaucoup de temps, car une fois que vous avez signalé quels patients vous devez vérifier, l’hôpital doit réellement envoyer quelqu’un pour examiner de plus près les dossiers des patients sélectionnés, qui pourraient être manuscrits et se trouver dans une archive. Vérifier les données d’un patient pourrait prendre une heure ou plus. Compte tenu de ces coûts, il est logique de vérifier seulement quelques instances de données importantes. La meilleure façon est de sélectionner des patients qui ont eu une forte influence sur le modèle de prédiction. Koh et Liang (2017) ont montré que ce type de sélection fonctionne beaucoup mieux qu’une sélection aléatoire ou la sélection de ceux avec la perte la plus élevée ou une classification incorrecte.\n\n\n10.5.3 - Avantages d’identifier les instances influentes\nLes approches des diagnostics de suppression et des fonctions d’influence sont très différentes des approches principalement basées sur la perturbation des caractéristiques présentées dans le chapitre Modèle-Agnostique. Examiner les instances influentes met en évidence le rôle des données d’entraînement dans le processus d’apprentissage. Cela fait des fonctions d’influence et des diagnostics de suppression l’un des meilleurs outils de débogage pour les modèles d’apprentissage automatique. Parmi les techniques présentées dans ce livre, ce sont les seules qui aident directement à identifier les instances qui devraient être vérifiées pour des erreurs.\nLes diagnostics de suppression sont agnostiques au modèle, ce qui signifie que l’approche peut être appliquée à n’importe quel modèle. De même, les fonctions d’influence basées sur les dérivées peuvent être appliquées à une large classe de modèles.\nNous pouvons utiliser ces méthodes pour comparer différents modèles d’apprentissage automatique et mieux comprendre leurs comportements différents, allant au-delà de la comparaison de la seule performance prédictive.\nNous n’avons pas parlé de ce sujet dans ce chapitre, mais les fonctions d’influence via des dérivées peuvent également être utilisées pour créer des données d’entraînement adverses. Ce sont des instances qui sont manipulées de telle manière que le modèle ne peut pas prédire correctement certaines instances de test lorsque le modèle est entraîné sur ces instances manipulées. La différence avec les méthodes du chapitre Exemples Adverses est que l’attaque a lieu pendant le temps d’entraînement, également connue sous le nom d’attaques de contamination. Si cela vous intéresse, lisez l’article de Koh et Liang (2017).\nPour les diagnostics de suppression et les fonctions d’influence, nous avons considéré la différence dans la prédiction et pour la fonction d’influence l’augmentation de la perte. Mais, en réalité, l’approche est généralisable à toute question de la forme : “Que se passe-t-il pour … lorsque nous supprimons ou surpondérons l’instance \\(z\\) ?”, où vous pouvez remplir “…” avec toute fonction de votre modèle de votre choix. Vous pouvez analyser à quel point une instance d’entraînement influence la perte globale du modèle. Vous pouvez analyser à quel point une instance d’entraînement influence l’importance des caractéristiques. Vous pouvez analyser à quel point une instance d’entraînement influence la caractéristique sélectionnée pour la première division lors de l’entraînement d’un arbre de décision.\nIl est également possible d’identifier des groupes d’instances influentes3.\n\n\n10.5.4 - Inconvénients d’identifier les instances influentes\nLes diagnostics de suppression sont très coûteux à calculer car ils nécessitent un réentraînement. Mais l’histoire a montré que les ressources informatiques augmentent constamment. Un calcul qui, il y a 20 ans, était impensable en termes de ressources peut facilement être effectué avec votre smartphone. Vous pouvez entraîner des modèles avec des milliers d’instances d’entraînement et des centaines de paramètres sur un ordinateur portable en quelques secondes ou minutes. Il n’est donc pas difficile d’imaginer que les diagnostics de suppression fonctionneront sans problème même avec de grands réseaux neuronaux dans 10 ans.\nLes fonctions d’influence sont une bonne alternative aux diagnostics de suppression, mais uniquement pour les modèles avec une fonction de perte différentiable au 2ème ordre par rapport à ses paramètres, tels que les réseaux neuronaux. Elles ne fonctionnent pas pour les méthodes basées sur les arbres comme les forêts aléatoires, les arbres boostés ou les arbres de décision. Même si vous avez des modèles avec des paramètres et une fonction de perte, la perte peut ne pas être différentiable. Mais pour le dernier problème, il y a une astuce : Utilisez une perte différentiable comme substitut pour calculer l’influence lorsque, par exemple, le modèle sous-jacent utilise la perte Hinge au lieu d’une perte différentiable. La perte est remplacée par une version lissée de la perte problématique pour les fonctions d’influence, mais le modèle peut toujours être entraîné avec la perte non lisse.\nLes fonctions d’influence ne sont qu’approximatives, car l’approche forme une expansion quadratique autour des paramètres. L’approximation peut être erronée et l’influence d’une instance peut en réalité être plus élevée ou plus faible lorsqu’elle est supprimée. Koh et Liang (2017) ont montré pour certains exemples que l’influence calculée par la fonction d’influence était proche de la mesure d’influence obtenue lorsque le modèle était réellement réentraîné après la suppression de l’instance. Mais il n’y a aucune garantie que l’approximation sera toujours aussi proche.\nIl n’y a pas de seuil clair de la mesure d’influence à partir duquel nous appelons une instance influente ou non influente. Il est utile de classer les instances par influence, mais il serait formidable de disposer des moyens non seulement de trier les instances, mais aussi de distinguer réellement entre influentes et non influentes. Par exemple, si vous identifiez les 10 instances d’entraînement les plus influentes pour une instance de test, certaines d’entre elles peuvent ne pas être influentes car, par exemple, seules les 3 premières étaient réellement influentes.\n\n\n10.5.5 - Logiciel et alternatives\nLes diagnostics de suppression sont très simples à mettre en œuvre. Consultez le code que j’ai écrit pour les exemples de ce chapitre.\n\n\n\n\n\n\nNote du traducteur\n\n\n\nAccès au code de cette page dans Github -&gt; python + FR + allégée ?\n\n\nPour les modèles linéaires et les modèles linéaires généralisés, de nombreuses mesures d’influence comme la distance de Cook sont implémentées dans R dans le package stats.\n\n\n\n\n\n\nNote du traducteur\n\n\n\nEquivalent(s) Python ?\n\n\nKoh et Liang ont publié le code Python pour les fonctions d’influence de leur article dans un dépôt. C’est génial ! Malheureusement, il s’agit “seulement” du code de l’article et non d’un module Python maintenu et documenté. Le code est axé sur la bibliothèque Tensorflow, donc vous ne pouvez pas l’utiliser directement pour des modèles boîte noire utilisant d’autres frameworks, comme sci-kit learn.\n\n\n\n\n\n\nNote du traducteur\n\n\n\nEt depuis ?",
    "crumbs": [
      "10 - Interprétation d'un réseau de neurone",
      "10.5 - Instances Influentes"
    ]
  },
  {
    "objectID": "10-neuralnet/10.5-influential-instances.html#footnotes",
    "href": "10-neuralnet/10.5-influential-instances.html#footnotes",
    "title": "Apprentissage automatique interprétable",
    "section": "Notes de bas de page",
    "text": "Notes de bas de page\n\n\nCook, R. Dennis. “Detection of influential observation in linear regression.” Technometrics 19.1 (1977): 15-18.↩︎\nKoh, Pang Wei, and Percy Liang. “Understanding black-box predictions via influence functions.” arXiv preprint arXiv:1703.04730 (2017).↩︎\nKoh, Pang Wei, Kai-Siang Ang, Hubert HK Teo, and Percy Liang. “On the accuracy of influence functions for measuring group effects.” arXiv preprint arXiv:1905.13289 (2019).↩︎",
    "crumbs": [
      "10 - Interprétation d'un réseau de neurone",
      "10.5 - Instances Influentes"
    ]
  }
]